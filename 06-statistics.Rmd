# Statistics

## Sample Data
This chapter uses the sample data generate with below code. The idea is to simulate two categorical-alike feature, and two numeric value feature:  

- **dept** is random character between 'D1', 'D2', 'D3', 'D4' and 'D5'  
- **grp** is random character with randomly generated 'G1', 'G2'  
- **value1** represents numeric value, normally distributed at mean 50  
- **value2** is numeric value, normally distributed at mean 25  
```{r}

```

```{r}
is.matrix(state.x77)

str(state.x77)

dimnames(state.x77)[1]

dimnames(state.x77)[2]

head(state.x77)
```


```{r}
set.seed(1234)
my.df = data.frame(
  dept = paste('D',sample(1:5, 100, replace = T),sep=''),
  grp  = paste('G',sample(1:2, 100, replace = T),sep=''),
  value1 = rnorm(1:100, mean = 50, sd = 5),
  value2 = rnorm(1:100, mean = 20, sd = 3),
  stringsAsFactors = T
)
head(my.df)
```

## Descriptive Summary

### Single Vector

- **summary** provides min, max, quantiles, mean for numerical vector. But it doesn't provide standard deviation.
- Other functions below take single vector as input, and output a single value
```{r, echo=TRUE, results='hold'}
summary(my.df$value1)
mean   (my.df$value1)
max    (my.df$value1)
median (my.df$value1)
sd     (my.df$value1)    # standard deviation
var    (my.df$value1)    # variance
length (my.df$value1)    # number of elements
```

### Multiple Vectors

- **summary** can be used for multiple columns in a data frame, which each columns is evaluated
- For factor data, **summary** provides frequency count
- For individual functions (**mean, max, min, sd, var**) that take only single vecotr and output single value, use **sapply** to provide calculation for multiple columns of a dataframe
```{r, echo=TRUE, results='hold'}
summary (my.df)
sapply (my.df[,3:4], min)
sapply (my.df[,3:4], max)
sapply (my.df[,3:4], median)
sapply (my.df[,3:4], sd)
sapply (my.df[,3:4], var)
```


### Custom Function

- Custom function can be built to accept single vector and return single vector
- Use **sapply** with the custom function to sweep through multiple columns in a dataframe and return a matrix (with row and col names) as a result
```{r}
mystats = function(x, na.omit=FALSE){
  if (na.omit)
    x =x[!is.na(x)]
  m = mean(x)
  med = median(x)
  v = var(x)
  s = sd(x)
  n = length(x)
  skew = sum((x-m)^3/s^3)/n
  kurt = sum((x-m)^4/s^4)/n - 3
  return(c(length=n, mean=m, median=med, stdev=s, skew=skew, kurtosis=kurt))
}

sapply(my.df[,3:4], mystats)
```
## T-Test

## Covariance / Correlation

If two variables are independent, their covariance/correlation  is 0.
**But**, having a covariance/correlation  of 0 does not imply the variables are independent. 

### Covariance

$$Pearson - Cov(X,Y)= \frac{\sum_{i=1}^n (X_i-\bar{X})*(Y_i-\bar{Y})}{n-1}$$  

- Covariance doesn't really tell you about the strength of the relationship between the two variables. - A large covariance can simply means the variables are made of large numbers, **doesn't means that the relation are strong**.  
- Hence **correlation** (scaled covariance) is a better indicator of the relation strenght.  

### Correlation

$$Pearson-Cor(X,Y)= \frac{Cov(X,Y)}{sd(X)sd(Y)}$$  

- Correlation is a scaled version of covariance that takes on values between -1 and 1. 0 indicates no correlation. +1 and -1 indicates perfect correlation.  
- Correlation  are used to measure the **strength** of relationship among linearly related quntitative variables (numerical).  

>**`cor(x, y, use= , method= )`**  
>**`x`**`      = matrix or dataframe`  
>**`y`**`      = matrix or dataframe, default = x`  
>**`method`**` = pearson, spearman, kendall, default is pearson`  
>**`use`**`    = everthing:missing value will set to missing, complete.obs:listwise deletion, pairwise.complete.obs:pairwise deletion`  

- If y is not specified, you get cross matrices by default (all variables crossed with all other variables).  
```{r, collapse=FALSE}
states = state.x77[,1:5]
cor(states) 
cor(states, method = 'kendall') 
```

- If x and y are specified, you can produce non squared correlation matrices with only the variables specified for both x and y axis 
```{r, collapse=FALSE}
cor(states[,1:5], states[,3:5])
```


### Correlation Test for significance

From the `cor` function, we know that Murder rate and Illiteracy are highly correlated (>0.7). However, is this merely by chance, or it is statistically significant that there are indeedc correlated ?   

To answer this question, we need to perform hypotesis testing:  

- $H_0$ : (population) correlation betwen Murder rate and Illiteracy = zero  
- $H_1$ : (sample) correlation between Murder rate and Illiteracy <> zero  

We then test our sample data using `cor.test` to find out the p-value. If the p-value < 0.025 (two sided), then we reject the null hypothesis.

>**`cor.test(x, y, method = , alternative= , conf.level= )`**  
>**`x`**`      = vector 1`  
>**`y`**`      = vector 2`  
>**`method`**` = pearson (default), spearman, kendall`  
>**`alternative`**`    = two.sided (default), less, more`  
>**`conf.level`** `= 0.95(default), any value between 0 to 1`  

```{r}
cor.test (states[,'Murder'], states[,'Illiteracy'])
```

If $H_0$ is true, then chance of observing the sample data (with correlation of 0.7) is 1.258e-08
(too low to be true). Hence we reject $H_0$, and accept $H_1$ that there is indeed a significant correlation between the variables.
