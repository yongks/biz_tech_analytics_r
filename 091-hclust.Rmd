```{r 091-init}
## global chunk option ##
knitr::opts_chunk$set(echo=TRUE, message=FALSE, fig.width=9, fig.height=2.5, fig.show='hold') 
```

## Hierarchical Clustering 

+ Hierarchical clustering is a widely used data analysis tool  
+ The idea is to build a binary tree of the data that successively merges similar groups of points  
+ Number of clusters (K) is not required initially  
+ It is an unsupervised learning  

### Clustering Process

This is how Hierarchical Clustering works:  
    1. Initially, put each data point in **its own cluster**  
    2. **Calucate the distances** between each cluster and all other clusters  
    3. **Combine the two clusters** with the smallest distance - This reduce cluster number by one  
    4. Repeat step (2) and (3) until all clusters have been **merged into single cluster**  <br><br>


### Cluster Distance  

Once distance for all data points has been measured, decide which of the five (5) methods below to measure distance between clusters:

1. **Single Linkage**:  
   Shortest distance between points belonging to two clusters    
2. **Complete Linkage (common)**:  
   Longest distance between points belonging to two clusters  
3. **Average Linkage (common)**:  
   Average distance between all points in one cluster with all points the another cluster  
4. **Centroid**:  
   Find the centroid of each cluster and calculate the distance between centroids between both  

> `hclust (d, method ='complete`)  
> $\quad$ `method = 'single', 'complete', 'average', 'centroid'  

Visualize the hierachical cluster using dendrogram, which displays how items are combined into clusters and is read from the bottom up.  

As you can see, if the number of observations are very large, the dendrogram will be cluttered.  

```{r fig.height=3 }
fit.complete = hclust (d1)  # default method: 'complete'
fit.average  = hclust (d1, method='average')
fit.single   = hclust (d1, method='single')
fit.centroid = hclust (d1, method='centroid')
```

### Visualiasing H-Cluster

```{r fig.height=9}
par(mfrow=c(2,2))
plot(fit.complete, main='HCluster, Complete-Linkage')
plot(fit.average,  main='HCluster, Average-Linkage')
plot(fit.single,   main='HCluster, Single-Linkage')
plot(fit.centroid, main='HCluster, Centroid-Linkage')
```

### Evaluate Optimum Number of Clusters (K) ?**  

+ `NbClust` package offers numerous 26 indices for determining the best number of clusters in a cluster analysis  
+ There is no guarantee that they will agree with each other. In fact, they probably wonâ€™t  
+ However, use this as a guidine and test few highest criteria score to determinee final number of cluster

```{r, warning=FALSE, results='hide'}
library(NbClust)
nbc = NbClust(data.scaled, distance="euclidean", min.nc=2, max.nc=8, method="average")
```

Clearly NMbClust suggest three (3) clusters, it most criteria suggested so.  

```{r, warning=FALSE, fig.height=3.5}
table( nc$Best.n[1,] )
barplot( table(nc$Best.n[1,] ),
  xlab="Numer of Clusters", ylab="Number of Criteria",
  main="Number of Clusters Chosen by 26 Criteria")
```

### Finalize Number of Cluster, K

Once K has been finalized, use `cutree` to produce a **vector** of cluster group number (in this example: 1,2,3 of total 3 groups) for all observations.  

```{r fig.height=3.5, results='markdown'}
clusters = cutree (fit.average, k=3)
clusters          # is a vector, marking cluster number for each observation
str(clusters)     # is a vector

# Frequency table to show number of observations per cluster group
barplot( table(clusters), xlab='Cluster Group Number', ylab='No. of Observations' ) 
```

Combination of `plot` and `rect.hclust` function plots dendrogram with clusters group marked in red.  

```{r fig.height=5}
plot(fit.average, main='HCluster, Average-Linkage\n5 Cluster Solution')
rect.hclust(fit.average, k=3) # superimpose cluster group in previous plot
```
