---
output:
  pdf_document: default
  html_document: default
---
```{r regression, echo=FALSE}
## global chunk option ##
knitr::opts_chunk$set(echo=TRUE, message=FALSE, warning=FALSE, fig.width=2.6736, fig.height=2.5, fig.show='hold', results = 'hold') 
```

# Regression Analysis







## Introduction

Regression is statistical process for estimating the relationships among variables:  

- It includes many techniques for modeling and analyzing  the relationship between a dependent variable and one or more independent variables  
- More specifically, regression analysis helps one understand how the typical value of the dependent variable changes when any one of the independent variables is varied, while the other independent variables are held fixed  
- Regression analysis estimates the conditional expectation of the dependent variable given the independent variables – that is, the average value of the dependent variable when the independent variables are fixed  
- Regression analysis is a form of predictive modelling technique which investigates the relationship between a dependent (target) and independent variable  

### General Equation and Terminilogy

A regression model relates Y to a function of $X$ and $\beta$:

$\quad{Y\approx f(\mathbf {X} ,{\boldsymbol {\beta }})}$  
$\quad \quad E(Y)$ = Predicted Value, Expected outcome or response, Expectation  
$\quad \quad Y$ = Dependent variable, criterion, outocme, response    
$\quad \quad X$ = Independent variable, features, predictors  

The goal of regression analaysis is to derive a model that estimate the **paramter ($\beta$) which has the least cost**.  

### R Formula Object

#### Notation Symbols

|  Symbol  | Usage                                                                                                                       | Example            |
|:--------:|:----------------------------------------------------------------------------------------------------------------------------|--------------------|
| ~        | Seperate response variables on the left, predictor variables on the right                                                   | y ~ x              |
| +        | Seperate predictor variables                                                                                                | y ~ x + z          |
| :        | Denotes interaction between predictors                                                                                      | y ~ x : z          |
| \*       | Shortcut denoting all possible interactions.  x \* z equivalent to x + z + x:z                                              | y ~ x * z          |
| ^        | ^3 measn include these variables and all interactions up to three way                                                       | y ~ (x + z + w)^2  |
| -1       | Suppress the intercept (make intercept to be zero) y ~ x - 1 fits the regression of y on x, forcing the line y = 0 at x = 0 | y ~ x -1           |
| .        | A shortcut placeholder to include all other variables except the dependent variable y ~ . will expand to y ~ x + z + w      | y ~ .              |
| -        | Exclude specific variable                                                                                                   | y ~.-z , y~.-w:z   |
| I ()     | Elements in parentheses are interpreted arithmetically                                                                      |                    |
| function | Math function can be used in formula, log(y) ~ x + z + w would predict log(y) from x, z and w                               | log(y) ~ x + z + w |

#### Example Equation and Notations

|   | Example Equation                                                    | R Formula Notation       |
|:-:|:--------------------------------------------------------------------|:-------------------------|
| 1 | $y=\beta_0 + \beta_1 x + \beta_2 w + \beta_3 z + e$                 | y ~ x + w + z            |
| 2 | $y=\beta_0 + \beta_1 x + \beta_2 wz + e$                            | y ~ x + w:z              |
| 3 | $y=\beta_0 + \beta_1 x + \beta_2 wz + \beta_3 w + \beta_4 z + e$    | y ~ x + w*z              |
|   |                                                                     | y ~ x + w:z + w + z      |
| 4 | $y = 0 + \beta_1 x + \beta_2 w$                                     | y ~ -1 + x + w           |
| 5 | $y=\beta_0 + \beta_1 x + \beta_2 w + e$                             | y ~ .-      z            |
| 6 | $y=\beta_0 + \beta_1 x + \beta_2 x^2 + \beta_3 x^3 + e$             | y ~ x + I(x^2) + I(x^3)  |

In R, there are **more than one way** to write a notation, consider below two examples:  

$y=\beta_0 + \beta_1 x + \beta_2 w + \beta_3 z + \beta_4 xw + \beta_5 xz + \beta_6 wz + \beta_7 xwz + e$  
can be represented by three formulas as below:  

```
y ~ (x + w + z)^3  
y ~ x * w * z
y ~ x + w + z + x:w + x:z + w:z + x:w:z
```

$y=\beta_0 + \beta_1 x + \beta_2 w + \beta_3 z + \beta_4 xw + \beta_5 xz + \beta_6 wz + e$  
can be represented by three formula as below:  
```
y ~ (x + w + z)^2
y ~ x * w * z - x:w:z
y ~ x + w + z + x:w + x:z + w:z
```

## Application

 
## Types of Regression

### Linear and Non Lienar

1. **Linear Regression**  

    a. Simple an d Multiple Regression  
    b. General Linear Model  
    c. Heteroscedastic Model  
    d. Hierarchical Linear Model  
    e. Erros-in-variables  
    f. Others  

2. **Non Linear Regression**  

    a. Logistic Regression  
    b. Polynomial Regression   
    c. Stepwise Regression  
    d. Ridge Regression  
    e. Lasso Regression  
    f. ElasticNet Regression  
    g. Others  

### Choosing the Regression Algorithm

1. Number of independent variables  
2. Shape of the regression line  
    
    
3. Type of dependent variable  
    - Binary outocme : Logistic Regression  




lm() fits models following the form Y = Xb + e, where e is Normal (0 , s^2).

glm() fits models following the form f(Y) = Xb + e. However, in glm both the function f(Y) (the 'link function') and the distribution of the error term e can be specified. Hence the name - 'generalised linear model'.

If you are getting the same results using both lm() and glm(), it is because for glm(), f(Y) - the 'link function' defaults to Y, and e defaults to Normal (0, s^2). i.e. if you don't specify the link function and error distribution, the parameters that glm() uses produce the same effect as running lm().

Glm uses normal distribution, lm uses t-distribution, hence the degree of freedom used are different.

lm models equation of $Y = \beta_0 X  + E$, where e = Normal(0,s^2)  
glm models equation of $g(Y) = \beta_0 X  + E$, where distribution of e can be specified  
function g(Y) is called 'link function'  .  By default parameters, glm fits the same model as lm, with exception that it uses normal distribution instead of t-distribution. 



## Linear Regression (OLS)

This section discussed **Ordinary Least Square** Linear Regression, this includes single and multiple variable OLS Linear Regression.  

There are other types of linear regression, which are not covered.  

- General Linear Regression  
- Generalized Linear Regression  
- Hierachical Linear Regression  
- Erros-in-variable Linear Regression  
- Others  

### The Concept

Linear Regression establishes a relationship between **dependent variable (Y)** and one or more **independent variables (X)** using a best fit straight line (also known as **regression line**).  

- The objective of linear regression modeling is to find the most **optimum equation** that **best explain** the data
- **Optimum** equation is defined as the one that has the least cost (error)

Once we had derived the **optimum equation**, we can use the model to predict target $Y'$ base on new variables $X$. 

### Assumptions

Below are conditions for the **least-squares estimator - used by linear regression** to possess desirable properties; in particular, these assumptions imply that the **parameter estimates** will be **unbiased, consistent, and efficient** in the class of linear unbiased estimators.  

#### Classical Assumptions

- The sample is representative of the population for the inference prediction  
    Question how is the data being gathered, is it convincing that it represents the population  ?

- Number of observations **must be larger** than number of independent variables  
    Check the length of observations >= column length of data  
    
#### Assumptions On Dependent Variable

- Must not be a categorical data type  
    
#### Assumptions On Independent Variable

- The independent variables are measured with **no error**, that is observations must be a set of known constants. (Note: If this is not so, modeling may be done instead using errors-in-variables model techniques)  

- **Each** independent variable are **linearly correclated with outcome**, when other independent variables are held constant. **Matrix scatter plot and correlation calculation** can validate this. Generally correlation of 0.7 and above are considered good.  

- **NO Multicollinearity** amont predictors - Meaning little or not linear correlationamong the predictors, i.e. it is not possible to express any predictor as a linear combination of the others, if so, we wouldn't know which predictor actually influene the outcome. 

#### Assumptions On Errors (residuals)
  
- The **errors are random numbers**, with means of **zero**  
    - There should not be a pattern in the residuals distribution  
    - If the residuals are **normally distributed** with **mean of zero**, then it is considered a bonus which we can perform statistical significant testing. $e = N(0,\sigma^2)$  
    - Normality on redisuals implies that the dependent variable are also normally distributed (if and only if **dependent variable is not stochastic**)  
    
- The **errors are uncorrelated** - that is, the variance–covariance matrix of the errors is diagonal and each non-zero element is the variance of the error  

- **Homoscedasticity** - The variance of the error is constant across observations. If heteroscedasticity exist, scatter plot of response and predictor will look like below  ![Heteroscedasticity](./images/heteroscedastic.jpg)  
    - The Goldfeld-Quandt Test can test for heteroscedasticity  
    - If homoscedasticity is present, a non-linear correction might fix the problem  
    - Otherwise, weighted least squares or other methods might instead be used.

#### Are These Assumptions to be followed strictly ?

In real life, actual data **rarely satisfies** the assumptions, that is:  

- Method is used even though the assumptions are not true  
- Variation from the assumptions can sometimes be used as a measure of how far the model is from being useful  
- Many of these assumptions may be relaxed in more advanced treatments  

Reports of statistical analyses usually include analyses of tests on the sample data and methodology for the fit and usefulness of the model. 
    
#### Additional Notes On Independent variables  
  
- Adding more variables to a regression procedure may **overfit** the model and make things worse. The idea is to pick the **best** variables  
- Some independent variable(s) are better at predicting the outocme, some contribute little or nothing  

  Because of **multicollinearity** and **overfitting**, there is a fair amount of **prep-work** to be performed BEFORE conducting multiple regression analysis - if one is to do it properly.  


### Equations

#### Terminology

**Simple** Linear Regression (classical) consists of just on predictor.  aka Single Variable Linear Regression.  
**Multiple** Linear Regression (classical)    consists of multiple predictors.  aka. Multiple Variable Linear Regression.  

**Multivariate** Regression (aka. **General Linear Regression**) is linear regression where the outocme is a vector (not scalar).   Not the same as multiple variable linear regression.  


#### Ordinary Least Square Estimatation

**Regression Model - Actual Outcome**

$\quad y_i = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + ...  \beta_k x_k + e_i$  
$\quad$where:  
$\quad \quad y_i$ = actual outcome value  
$\quad \quad \beta_0$ = intercept, when all independent variables are 0  
$\quad \quad \beta_k$ = parameter for independent variable k
$\quad \quad e_i$ = error for observation i  

**Regression Equation - Predicted Outcome**

$\quad E(y_i) = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + ...  \beta_k x_k$  

$\quad h_\theta(X)~=~\theta_0~+~\theta_1 \cdot X$  , error terms assumed to be zero  
$\quad$where:  
$\quad \quad h_\theta(x)$ = hypothesis target (dependant variable)  
$\quad \quad \theta_0$ = intercept  
$\quad \quad \theta_1$ = slopes or coefficients  
$\quad \quad X$ = independant variables (predictors)  

$\quad$Take note that each $\theta_0$ and $\theta_1$ represents multi-variate data in matrix form.  

#### Cost Function

- The goal is to find some values of θ (known as coefficients), so we can minimize the difference between real values $y$  and predicted values ($h(x)$)  
- Mathematically, this means finding the minimum value of cost function $J$ and derive the  optimum value of $\theta_0$ and $\theta_1$   
- Linear regression uses Total Sum Of Square calculation on Error as Cost Function, denoted by $J$ below:  

$\quad \quad J(\theta_0,\theta_1) = \frac{1}{2m}\sum_{i=1}^m ((h_\theta(x^i)-y^i)^2$

### OLS Performance

#### Fundamental

OLS performance is mainly on error analysis.  

**SST** (**total** sample variability) = **SSR** (**explained** variability) + **SSE** (**unexplained** variability) :    

![SST Explained](./images/explain_sst.jpg)

#### Root Mean Square Error (RMSE)

- RMSE = The square root of the average of the total sum of square error  
$RMSE = \sqrt{\frac{SSE}{n}} = \sqrt \frac{\sum^n_{i=1}{(y_i - \hat y_i)^2}}{n}$  
- It measure **how close** observed data points are to the model’s predicted values  
- It has a unit of Y, therefore cannot used for comparison models with different outcome  
- It can be used to compare different model with the similar outcome but different predictors, however, adjusted $R^2$ is better in this  
- Low RMSE value indicates better fit  
- Compared to the similar Mean Absolute Error, RMSE amplifies and severely punishes large errors  
- SSE is not usable for performance measurement becuase it increases with number of datapoints.  RMSE does not suffer this as it is divided by number of datapoints  
- **Residual Standard Errror (RSE)**  is very similar to **RMSE**, except that RSE use division by degree of freedom  

> **RMSE** is excellent general measurement to assess **the accuracy** of a model  


#### $r$, $R^2$ and $R^2_{adj}$

**r - Correlation Coeeficient**  

- Correlation, often measured as a correlation coefficient - indicates the **strength** and direction of a linear relationship between two variables (for example model output and observed values)  
- The best known is the Pearson product-moment correlation coefficient (also called Pearson correlation coefficient or the sample correlation coefficient)  
- It is a ratio (has no unit)  
$Pearson Correlation, r = \frac{{}\sum_{i=1}^{n} (x_i - \overline{x})(y_i - \overline{y})}
{\sqrt{\sum_{i=1}^{n} (x_i - \overline{x})^2(y_i - \overline{y})^2}} \quad, \quad 0=<r<=1$  

> **Scatter plot** predicted and actual outcome reveal visually the good-fit of the model, good correlation also means tigther the scatter plots with less variability    

**R-Squared - Coefficient Determination**  

$R^2 = r^2 = \frac{SSR}{SST} = 1-\frac{SSE}{SST}, \quad 0 <= R^2 <= 1$  

- $R^2$ is a ratio (unit-less) indicating how much **variations are explained** by regression model  
- $R^2$ compares the fit model to a 'baseline' model (SST)  
- High $R^2$ value indicates high SSR and low SSE, which means the model is **more precise**  
    - **Perfect Case**  - no errors (SSE=0), $R^2$ will be 1.   
    - **Worst Case** - no improvement over baseline,  (coefficient=0, SSR=0, aka horizontally flat line), $R^2$ will be 0.  

- One pitfall of $R^2$ is that it **always increases** when additional variables are added to the model. The increase can be artificial as it doesn't improve the model fit - which is called **over-fitting**.  A **remediation** to this is $R^2_{adj}$  


**Adjusted R-Squared**  

- Adjusted $R^2$ incorporates the number of coefficients and observations into the calculation  
$R_{adj}^2 = 1- \bigg( \frac{n-1}{n-p}\bigg) \frac{SSE}{SST}$  
$\quad$ p = number of coefficients (including intercept)  
$\quad$ n = number of observations  

- Adjusted $R^2$ will **decrease** when adding predictors that doesn't increase the model fit that make up for the loss of degrees of freedom  
- Likewise, it will **increase** as predictors are added if the increase in model fit is **worthwhile**  

> **$R^2_{adj}$** is useful to **compare models with a different number of predictors**, hence good for feature selection  

**Training Data and Test Data (out of sample)**  

- A built model based on **training data** always has $R^2$ between 0 <= $R^2$ <= 1  
- However, if the model is **underfit**, **test data** may reveal $R^2$ <= 0  

### Feature Selection

1. The **strength and importance** of an independent variable **is not measured** by its correlation or coefficient with the dependent variable - It only hold true if there is only single independent variable  
2. **Multicolinearity** has below independent variable diversion and therefore must be removed:  
    - It increases the p-value to make it insignificant 
    - It divert a coef direction (eg. positive becomes negative)  
3. Perform **transformation** (such as log, quadradric) if the plot of independet vs dependent variables shows Heteroscedasticity  


### Sample Data

```{r fig.width=9}
set.seed(1234)
n=100
my.df = data.frame(
  id = paste('ID', 1:n),
  x1 = 10:(10 + n - 1) * runif(n, min = 0.5, max = 1.1),
  x2 = 20:(20 + n - 1) * runif(n, min = 0.5, max = 1.1),
  x3 = 30:(30 + n - 1) * runif(n, min = 0.5, max = 1.1)
)
#my.df = gen_slinear(n=100,start=-1, intercept = 7, coef=3, visual=FALSE)
my.df$y = 88 + 0.1 * my.df$x1 + 0.2 * my.df$x2 + 0.3*my.df$x3

```


### Run The Code

#### Build The Model

**`lm` build a linear regresson model** for this equation: $h(x_1,x_2,x_3) = \theta_0+ \theta_1x_1 + \theta_2x_2 + \theta_3X_3$  

> **`lm ( formula , data )`**  
> $\quad$ `formula : y~x1 + x2 + x3`  
> $\quad$ `data: matrix or dataframe`  

```{r}
fit = lm(formula = y ~ . -id, data = my.df)
```

#### Evaluate The Model

**`summary`** provides useful infomration about the model, such as:  

  - List of coefficients, and their t-value, p-value  
  - R-square  
  - Adjusted R-square  
  - Residual Standard Error, similar to RMSE except that it divides by degree of freedom, instead of number of observations  

```{r results='markup'}
summary(fit)
```

However, `summary` **doesn't show** RMSE/SSE/SST/SSR measurement, which requried additional coding as in following section. 

#### Additional Evaluation (custom code)

The built model (**lm object**) contain good information as below for us to build additional evaluation:  

  - Original training data (\$model)   
  - Residuals (\$residuals)  
  - Predicted outocme (\$fitted.values)  
  - R2, Adjusted R2 (summary\$)  

From these information, we can calculate **SSE, SST, SSR, RMSE**. Note that R2 and R2-Adj can be derived from **`summary`** function, no calculation required.  

```{r}
eval_regression = function ( fit ) {
  # fit: lm object
  SSE = sum(fit$residuals^2)
  SSR = sum((fit$fitted.values - mean(fit$model[[1]]))^2)
  SST = sum((fit$model[1] - mean(fit$model[[1]]))^2)
  RMSE = sqrt(SSE/nrow(fit$model))
  R2   = 1-SSE/SST
  R2.ADJ = summary(fit)$adj.r.squared
  result = 
  c( sse=SSE, ssr=SSR, sst=SST, rmse=RMSE, r2=R2, r2.adj = R2.ADJ)
}
```

This evaluation function return key statistics in a **named vector**.  

```{r, collapse=T}
eval_regression(fit)
```

## Regression Diagnostic

```{r}
library('car')
```


### Outlier Test


### Linearity Test


### Homoscedasticity Test



### Multicollinearity Test

Validation method:  
    a. Scatter Plot Matrix for all independent variables  
    b. Correlation Matrix calculation for all independent varaibels. Correlation value 0.2 and below consider week correlation and therefore acceptable  
    c. **Tolerance (T)** - measures the influence of one independent variable on all other independent variables  
        **T <0.01** --> **confirm** correlation  
        **T <0.1**  --> **might have** correlation  
    d. **Variance Inflation Factor (VIF)**  - VIF = 1/T  
        **T >100**  --> **confirm** correlation  
        **T >10** --> **might have** correlation  
    e. **Condition Index (CI)** - calculated using a factor analysis on the independent variables  
        ** Values of 10-30** -->  indicate a mediocre multicollinearity  
        ** Values > 30** --> strong multicollinearity  

### Normality Test



