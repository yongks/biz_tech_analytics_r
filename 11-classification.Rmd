```{r classification, echo=FALSE}
## global chunk option ##
library('knitr')
knitr::opts_chunk$set(echo=TRUE, message=FALSE, warning=FALSE, fig.width=2.6736, fig.height=2.5, fig.show='hold') 
```

# Classification

## Introduction

## Application

## Comparing Algorithm


## Performance Measurement

There are many performance measurement used for binary classification.  Here are the rules of thumb which one to use:  

- **Recall**: If you don't mind getting some inaccurate result, as long as you get as much correct ones 
- **Precision**: If you demand rate of correctness and willing to reject some correct results  
- **F1 Score**: For a more balanced measurement, taking into consideraton both recall and precision  

### Confusion Matrix

![Confusion Matrix and Performance Measurement](./images/confusion_table.jpg)

#### Accuracy

- Accuracy answers the question: **From the total samples, how many had been correctly predicted by the model ?**  
- $Accuracy = \frac{TP+TN}{TP+TN+FP+FN}$
- This measurement is useful when the both **classes are balanced** (that is, the number of TP and TN cases are almost balanced)  
- In practice, it seems that the best accuracy is usually achieved when the cutpoint is **near the Probability(actual TRUE)**  
- Accuracy is completely usessless in highly skewed class.  For example, with a disease that only affects 1 in a million people a completely bogus screening test that always reports “negative” will be 99.9999% accurate  

#### Recall

- Recall answers the question: **Out of all actual positive samples, how many were correctly predicted by classifiers ?**  
- $Recall = \frac{TP}{TP+FN}$

#### Precision

- Precison answers the question:  **Out of all the samples classifier predicted as positive, what fraction were correct ?**  
- $Precision = \frac{TP}{TP+FN}$

#### F1 Score

- F1 score is the **harmonic mean of Precision and Recall**. Intuitively, F1 Score is the **weighted average of Precision and Recall**. It takes into account all three measures: TP, FP and FN  
- $Precision = 2*\frac{Recall * Precision}{Recall + Precision}$  
- F1 is usually more useful than accuracy, especially if you have an unbalanced class distribution  
  
#### Area Under ROC (AUROC)

- ROC curve is basically a graph of **TPR vs FPR** (some refer to as Recall vs (1-Sensitivity), plotted for **different thresholds**  
- Comparing two different models, the model with **higher AUROC is considered to have higher overall Accuracy**  
- AUROC (Area Under ROC) measures :  
    - AUC of **0.5**: means the model is as good as tossing a coin, worthless  
    - AUC of **1.0**: means for all cutoff points, TPR=1 and FPR=0. Intuitively it means, all samples had been correctly classified into TP (TPR=1) and TN(FPR=0), and there is no FP and FN. Ultiamtely it means Accuracy is 100%  
    
    
![AUROC and Thresholds](./images/roc_curve.gif)  
  
### Cutoff Threshold Optimization

#### Understanding Cutoff Impacts

Cutoff threshold **direclty influence** the value of TP, FP, TN, FN.  
If **cutoff threshold is lowered** (lower probability to classify as Postive), the results are:  

- **More linient** and hence **more samples will be classified as Positive**  
- More predicted Positives means more TP and FP, hence **TPR and FPR increases**  
- However, **TPR and FPR increases at different rate**:  
    - If TPR increases faster than FPR -> this is **good**, as the lowered threshold generated more TP than FP  
    - If FPR increases faster then TPR -> this is **not good**, as the lowered threhsold generated more FP than TP  

Different threshold produces different performance metrics (Accuracy, Recall, Precision and Specificity and F1-score). As an example, picture below shows how threshold influences the ROC curve.  

![Threshold and ROC](./images/roc_threshold_demo.gif)  

The only way to estimate the **optimum threshold** for each of the performance measurement will be to measure them for a **wide range of threshold**.  

#### Cutoff Impact Visualization

Cutoff and Performance is best visualized using the graphs below:  

1. Threshold vs Accuracy  
2. Threshold vs Recall / TPR  
3. Threshold vs Precision  
4. ROC Curve (TPR vs FPR) 

### Run The Code

#### Measurement Function

```{r}
#######################################################################################
#### Function Return Various Performance by Cutoff Threshold
##### Input : fit: glm model object that uses probability classifier
#######################################################################################
eval.binclass = function(fit = NULL, cutoff.min = 0, cutoff.max = 1, cutoff.by = 0.1) {

    ## Calculate Key Measurements For One Cutoff
    calc.metrics = function(x) {
        actual = factor(as.logical(fit$model[[1]]), levels = c(TRUE, FALSE))
        predicted = factor(fit$fitted.values > x, levels = c(TRUE, FALSE))
        ct = table(actual, predicted, useNA = 'no', exclude = c(NA)) #confusion table
        accuracy = (ct[1] + ct[4]) / (sum(ct))
        recall = ct[1] / (ct[1] + ct[3])
        precision = ct[1] / (ct[1] + ct[2])
        specificity = ct[4] / (ct[2] + ct[4])
        data.frame(cutoff = x, accuracy = accuracy, recall = recall, precision = precision,
            specificity = specificity, fscore = 2 * (precision * recall / (precision + recall)), fpr = 1 - specificity,
            tpr_fpr = recall / (1 - specificity), tp = ct[1], fp = ct[2], fn=ct[3], tn=ct[4])
    }

    ## Derive cutoff breakpoints, and loop to calculate measures for each breakpoint
    cutoffs = seq(cutoff.min, cutoff.max, by = cutoff.by)
    perf.tab = do.call(rbind, lapply(cutoffs, FUN = calc.metrics))

    ## Calculate AUC
    h = (perf.tab$recall[-1] + perf.tab$recall[-length(perf.tab$recall)]) / 2
    w = abs(diff(perf.tab$fpr))
    auc = sum(h*w)

    ## Summarize optimal cutoff for max performance on various metrics
    best.metrics = rbind(
        best.accuracy = c(max = max(perf.tab$accuracy, na.rm = TRUE),   cutoff = perf.tab$cutoff[which.max(perf.tab$accuracy)]),
        best.precision = c(max = max(perf.tab$precision, na.rm = TRUE), cutoff = perf.tab$cutoff[which.max(perf.tab$precision)]),
        best.recall = c(max = max(perf.tab$recall, na.rm = TRUE),       cutoff = perf.tab$cutoff[which.max(perf.tab$recall)]),
        best.fscore = c(max = max(perf.tab$fscore, na.rm = TRUE),       cutoff = perf.tab$cutoff[which.max(perf.tab$fscore)])
    )

    ## Plot ROC graphs
    plot(perf.tab$fpr, perf.tab$recall, type = 'b', main = paste('ROC Curve, auc=',auc),
        xlab='False Positive Rate, or (1-Specificity)', ylab='Recall or True Positive Rate')
    grid()
    abline(0, 1, col = "red", lty = 2)

    ## Plot Accuracy graph
    plot(perf.tab$cutoff, perf.tab$accuracy, type = 'b', main = 'Accuracy over Cutoffs',
        xlab = 'Cutoff Thresholds', ylab = 'Accuracy')
    grid()
    text(best.metrics['best.accuracy', 'cutoff'], min(perf.tab$accuracy, na.rm = T) + diff(range(perf.tab$accuracy, na.rm = T)) / 10, best.metrics['best.accuracy', 'cutoff'], col = "red")
    abline(v = best.metrics['best.accuracy', 'cutoff'], col = "red", lty = 2)
    abline(h = best.metrics['best.accuracy', 'max'], col = "red", lty = 2)

    ## Plot Precision graph
    plot(perf.tab$cutoff, perf.tab$precision, type = 'b', main = 'Precision over Cutoffs',
        xlab = 'Cutoff Thresholds', ylab = 'Precision')
    grid()
    text(best.metrics['best.precision', 'cutoff'], min(perf.tab$precision,na.rm = T)+diff(range(perf.tab$precision, na.rm=T))/10, best.metrics['best.precision', 'cutoff'], col = "red")
    abline(v = best.metrics['best.precision', 'cutoff'], col = "red", lty = 2)
    abline(h = best.metrics['best.precision', 'max'], col = "red", lty = 2)

    ## Plot F1 Scoregraph
    plot(perf.tab$cutoff, perf.tab$fscore, type = 'b', main = 'F1-Score over Cutoffs',
        xlab = 'Cutoff Thresholds', ylab = 'F1-Score')
    grid()
    text(best.metrics['best.fscore', 'cutoff'], min(perf.tab$fscore, na.rm = T) + diff(range(perf.tab$fscore, na.rm = T)) / 10, best.metrics['best.fscore', 'cutoff'], col = "red")
    abline(v = best.metrics['best.fscore', 'cutoff'], col = "red", lty = 2)
    abline(h = best.metrics['best.fscore', 'max'], col = "red", lty = 2)

    # return performance table, and optimal values per metric
    return(list(perf.tab = perf.tab, auroc = auc, best.metrics = best.metrics))
}
```

#### Train The Model

```{r collapse=T}
### Load The Data and Build The Model
train.data = read.csv('./datasets/hr.train.csv')

str(train.data)

## Trian The Model Using Logistic Regression
my.fit = glm(left ~ ., family = binomial(link = logit), data = train.data)
```

#### Run the Performance Eval Function

**List The Performance Table**

```{r collapse=T,fig.width=9, fig.height=9}
par(mfrow=c(2,2))
r = eval.binclass(my.fit, cutoff.by = 0.1)

kable(r$perf.tab)
```

**`$auroc`** is the **area under ROC**, calculated for each simulated threshold.  

```{r collapse=T}
r$auroc
```

**`$best.metrics`** provides **optimal threshold** that maximize individual metrics, based on the simulated thresholds. 

```{r collapse=T}
r$best.metrics
```

## Logistic Regression

### The Concept

Logistic Regression is a actually a **classification** algorithm. It is used to predict: 

- Binary outcome (1=Yes/Sucess, 0=No/Failure), given a set of independent variables. 
- Multinomial outcome (more than two categories) - however, reference category for comparison must be specified, otehrwise, must run multiple regressions with different refence categories  

Logistic Regression as a **special case** of linear regression where:  

- The outcome variable is categorical   
- **Ln of odds** as dependent variable  

Linear regression cannot be used for classification because:  

- Binary data does not have a normal distribution, which is a condition for many types of regressions  
- Predicted values can go beyond 0 and 1, which violates the definition of probability  
- Probabilities are often not linear  

### Assumptions

Since logistic regression is related to linear combination of IVs, it share some common asssumptions regarding IVs and error terms:  

1. Dependent variable must be 1/0 type eg. 'sucess/failure', 'male/female', 'yes/no'. Must not be ordinal and continous  
2. Observations must be independent  
3. Like OLS, Linearity between logit with all independent variables  
4. Like OLS, NO multicollinearity - if found, create interaction term, or drop one of the IVs  
5. Like OLS, error terms are assumed uncorrelated  

Although **logit**  is a linear relation with independent variables, logistic regression (which use MLE) is **different from OLS** Linear Regression as below, due to the fact that DV is categorical and not continuuous:  

- Can handle categorical independent variables  
- Does not assume normality of DV and IVs: becauae $p$ follow Bernoulli distribution  
- Does not assume linearity between DV and IVs: because DV is categorical  
- Does not assume homoscedasticity  
- Does not assume normal errors  

### Equations

- The goal of logistic regression is to estimate $p$ (the probability of 'Success') for a **linear combination** of the independent variables  
- This is done by 'linking' the linear combination of independent variables to Bernoulli probability distribution (with domain from 0 to 1), to predict the probability of success  

- The **link function** is called **logit**, which is the natural log of odds ratio. It is a linear function against independent variables:  
$logit(p) = ln(odds) = ln\bigg(\frac{p}{1-p}\bigg) = \beta_0 + \beta_1 x_1 + ... + \beta_n x_n$  

- Derive **Odd ratio** by anti-log. It measures the 'strength' of IV in affecting the outcome, p:  
$odds = \frac{p}{1-p} = e^{\beta_0 + \beta_1 x_1 + ... + \beta_n x_n}$  

- $p$ can be further derived as below sigmoid function. $p$ is **non-linear** against independent varibales :  
$p = \frac{1}{1+e^{-\beta_0 + \beta_1x_1 + ... +  \beta_nx_n}}$  


### Sample Data

### Run The Code

> **`glm`** `(formula, family=gaussian(link=identity), data)`  
> $\quad$ `formula : example y ~ x1 + x2 + x3`  
> $\quad$ `family  : binomial, gaussian, poisson, quasi etc  
> $\quad$ `link    : logit-default for binomial(), identity-default for gaussian, log-default for poisson`  

#### Binomial



**Binomial Example**

```
glm (y ~ x1 + x2 + x3 , family=binomial(logit), data=my.df)
```


