<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Business and Technical Analytics with R</title>
  <meta name="description" content="This is a study and reference notes to solving various business analytics problems using R.">
  <meta name="generator" content="bookdown 0.4 and GitBook 2.6.7">

  <meta property="og:title" content="Business and Technical Analytics with R" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is a study and reference notes to solving various business analytics problems using R." />
  <meta name="github-repo" content="yongks/biz_tech_analytics_r" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Business and Technical Analytics with R" />
  
  <meta name="twitter:description" content="This is a study and reference notes to solving various business analytics problems using R." />
  

<meta name="author" content="Yong Keh Soon">


<meta name="date" content="2017-06-10">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="11-3-types-of-regression.html">
<link rel="next" href="11-5-regression-diagnostic.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; background-color: #f8f8f8; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
pre, code { background-color: #f8f8f8; }
code > span.kw { color: #204a87; font-weight: bold; } /* Keyword */
code > span.dt { color: #204a87; } /* DataType */
code > span.dv { color: #0000cf; } /* DecVal */
code > span.bn { color: #0000cf; } /* BaseN */
code > span.fl { color: #0000cf; } /* Float */
code > span.ch { color: #4e9a06; } /* Char */
code > span.st { color: #4e9a06; } /* String */
code > span.co { color: #8f5902; font-style: italic; } /* Comment */
code > span.ot { color: #8f5902; } /* Other */
code > span.al { color: #ef2929; } /* Alert */
code > span.fu { color: #000000; } /* Function */
code > span.er { color: #a40000; font-weight: bold; } /* Error */
code > span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #000000; } /* Constant */
code > span.sc { color: #000000; } /* SpecialChar */
code > span.vs { color: #4e9a06; } /* VerbatimString */
code > span.ss { color: #4e9a06; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #000000; } /* Variable */
code > span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */
code > span.op { color: #ce5c00; font-weight: bold; } /* Operator */
code > span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */
code > span.ex { } /* Extension */
code > span.at { color: #c4a000; } /* Attribute */
code > span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */
code > span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Business and Technical Analytics /R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="1-fundamentals.html"><a href="1-fundamentals.html"><i class="fa fa-check"></i><b>1</b> R Fundamentals</a></li>
<li class="chapter" data-level="2" data-path="2-data-generation.html"><a href="2-data-generation.html"><i class="fa fa-check"></i><b>2</b> Data Generation</a></li>
<li class="chapter" data-level="3" data-path="3-data-simulation.html"><a href="3-data-simulation.html"><i class="fa fa-check"></i><b>3</b> Data Simulation</a></li>
<li class="chapter" data-level="4" data-path="4-data-summarization.html"><a href="4-data-summarization.html"><i class="fa fa-check"></i><b>4</b> Data Summarization</a></li>
<li class="chapter" data-level="5" data-path="5-data-preprocessing.html"><a href="5-data-preprocessing.html"><i class="fa fa-check"></i><b>5</b> Data Preprocessing</a></li>
<li class="chapter" data-level="6" data-path="6-ordering-and-filter-data.html"><a href="6-ordering-and-filter-data.html"><i class="fa fa-check"></i><b>6</b> Ordering and Filter Data</a></li>
<li class="chapter" data-level="7" data-path="7-graphic-visualization.html"><a href="7-graphic-visualization.html"><i class="fa fa-check"></i><b>7</b> Graphic Visualization</a></li>
<li class="chapter" data-level="8" data-path="8-statistics.html"><a href="8-statistics.html"><i class="fa fa-check"></i><b>8</b> Statistics</a></li>
<li class="chapter" data-level="9" data-path="9-clustering-analysis.html"><a href="9-clustering-analysis.html"><i class="fa fa-check"></i><b>9</b> Clustering Analysis</a></li>
<li class="chapter" data-level="10" data-path="10-classification.html"><a href="10-classification.html"><i class="fa fa-check"></i><b>10</b> Classification</a></li>
<li class="chapter" data-level="11" data-path="11-regression-analysis.html"><a href="11-regression-analysis.html"><i class="fa fa-check"></i><b>11</b> Regression Analysis</a><ul>
<li class="chapter" data-level="11.1" data-path="11-1-introduction.html"><a href="11-1-introduction.html"><i class="fa fa-check"></i><b>11.1</b> Introduction</a><ul>
<li class="chapter" data-level="11.1.1" data-path="11-1-introduction.html"><a href="11-1-introduction.html#general-equation-and-terminilogy"><i class="fa fa-check"></i><b>11.1.1</b> General Equation and Terminilogy</a></li>
<li class="chapter" data-level="11.1.2" data-path="11-1-introduction.html"><a href="11-1-introduction.html#r-formula-object"><i class="fa fa-check"></i><b>11.1.2</b> R Formula Object</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="11-2-application.html"><a href="11-2-application.html"><i class="fa fa-check"></i><b>11.2</b> Application</a></li>
<li class="chapter" data-level="11.3" data-path="11-3-types-of-regression.html"><a href="11-3-types-of-regression.html"><i class="fa fa-check"></i><b>11.3</b> Types of Regression</a><ul>
<li class="chapter" data-level="11.3.1" data-path="11-3-types-of-regression.html"><a href="11-3-types-of-regression.html#linear-and-non-lienar"><i class="fa fa-check"></i><b>11.3.1</b> Linear and Non Lienar</a></li>
<li class="chapter" data-level="11.3.2" data-path="11-3-types-of-regression.html"><a href="11-3-types-of-regression.html#choosing-the-regression-algorithm"><i class="fa fa-check"></i><b>11.3.2</b> Choosing the Regression Algorithm</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="11-4-linear-regression-ols.html"><a href="11-4-linear-regression-ols.html"><i class="fa fa-check"></i><b>11.4</b> Linear Regression (OLS)</a><ul>
<li class="chapter" data-level="11.4.1" data-path="11-4-linear-regression-ols.html"><a href="11-4-linear-regression-ols.html#the-concept"><i class="fa fa-check"></i><b>11.4.1</b> The Concept</a></li>
<li class="chapter" data-level="11.4.2" data-path="11-4-linear-regression-ols.html"><a href="11-4-linear-regression-ols.html#assumptions"><i class="fa fa-check"></i><b>11.4.2</b> Assumptions</a></li>
<li class="chapter" data-level="11.4.3" data-path="11-4-linear-regression-ols.html"><a href="11-4-linear-regression-ols.html#equations"><i class="fa fa-check"></i><b>11.4.3</b> Equations</a></li>
<li class="chapter" data-level="11.4.4" data-path="11-4-linear-regression-ols.html"><a href="11-4-linear-regression-ols.html#ols-performance"><i class="fa fa-check"></i><b>11.4.4</b> OLS Performance</a></li>
<li class="chapter" data-level="11.4.5" data-path="11-4-linear-regression-ols.html"><a href="11-4-linear-regression-ols.html#sample-data"><i class="fa fa-check"></i><b>11.4.5</b> Sample Data</a></li>
<li class="chapter" data-level="11.4.6" data-path="11-4-linear-regression-ols.html"><a href="11-4-linear-regression-ols.html#run-the-code"><i class="fa fa-check"></i><b>11.4.6</b> Run The Code</a></li>
</ul></li>
<li class="chapter" data-level="11.5" data-path="11-5-regression-diagnostic.html"><a href="11-5-regression-diagnostic.html"><i class="fa fa-check"></i><b>11.5</b> Regression Diagnostic</a><ul>
<li class="chapter" data-level="11.5.1" data-path="11-5-regression-diagnostic.html"><a href="11-5-regression-diagnostic.html#outlier-test"><i class="fa fa-check"></i><b>11.5.1</b> Outlier Test</a></li>
<li class="chapter" data-level="11.5.2" data-path="11-5-regression-diagnostic.html"><a href="11-5-regression-diagnostic.html#linearity-test"><i class="fa fa-check"></i><b>11.5.2</b> Linearity Test</a></li>
<li class="chapter" data-level="11.5.3" data-path="11-5-regression-diagnostic.html"><a href="11-5-regression-diagnostic.html#homoscedasticity-test"><i class="fa fa-check"></i><b>11.5.3</b> Homoscedasticity Test</a></li>
<li class="chapter" data-level="11.5.4" data-path="11-5-regression-diagnostic.html"><a href="11-5-regression-diagnostic.html#multicollinearity-test"><i class="fa fa-check"></i><b>11.5.4</b> Multicollinearity Test</a></li>
<li class="chapter" data-level="11.5.5" data-path="11-5-regression-diagnostic.html"><a href="11-5-regression-diagnostic.html#normality-test"><i class="fa fa-check"></i><b>11.5.5</b> Normality Test</a></li>
</ul></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Business and Technical Analytics with R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="linear-regression-ols" class="section level2">
<h2><span class="header-section-number">11.4</span> Linear Regression (OLS)</h2>
<p>This section discussed <strong>Ordinary Least Square</strong> Linear Regression, this includes single and multiple variable OLS Linear Regression.</p>
<p>There are other types of linear regression, which are not covered.</p>
<ul>
<li>General Linear Regression<br />
</li>
<li>Generalized Linear Regression<br />
</li>
<li>Hierachical Linear Regression<br />
</li>
<li>Erros-in-variable Linear Regression<br />
</li>
<li>Others</li>
</ul>
<div id="the-concept" class="section level3">
<h3><span class="header-section-number">11.4.1</span> The Concept</h3>
<p>Linear Regression establishes a relationship between <strong>dependent variable (Y)</strong> and one or more <strong>independent variables (X)</strong> using a best fit straight line (also known as <strong>regression line</strong>).</p>
<ul>
<li>The objective of linear regression modeling is to find the most <strong>optimum equation</strong> that <strong>best explain</strong> the data</li>
<li><strong>Optimum</strong> equation is defined as the one that has the least cost (error)</li>
</ul>
<p>Once we had derived the <strong>optimum equation</strong>, we can use the model to predict target <span class="math inline">\(Y&#39;\)</span> base on new variables <span class="math inline">\(X\)</span>.</p>
</div>
<div id="assumptions" class="section level3">
<h3><span class="header-section-number">11.4.2</span> Assumptions</h3>
<p>Below are conditions for the <strong>least-squares estimator - used by linear regression</strong> to possess desirable properties; in particular, these assumptions imply that the <strong>parameter estimates</strong> will be <strong>unbiased, consistent, and efficient</strong> in the class of linear unbiased estimators.</p>
<p><strong>Classical assumptions</strong> for linear regression analysis include:</p>
<ul>
<li><p>The sample is representative of the population for the inference prediction<br />
Question how is the data being gathered, is it convincing that it represents the population ?</p></li>
<li><p>Number of observations <strong>must be larger</strong> than number of independent variables<br />
Check the length of observations &gt;= column length of data</p></li>
</ul>
<p><strong>Assumptions On Dependent Variable</strong></p>
<ul>
<li>Must not be a categorical data type</li>
</ul>
<p><strong>Assumptions On Independent Variable</strong></p>
<ul>
<li><p>The independent variables are measured with <strong>no error</strong>, that is observations must be a set of known constants. (Note: If this is not so, modeling may be done instead using errors-in-variables model techniques)<br />
Think about if the ‘predictor’ has erors, what makes the outcome ?</p></li>
<li><strong>Each</strong> independent variable are <strong>linearly correclated with outcome</strong>, when other independent variables are held constant. Validation methods:
<ol style="list-style-type: lower-alpha">
<li>Run Matrix Scatter plot outcome against individual independent variables. Methods to verify are:<br />
</li>
<li>Run matrix correlation calculation for outcome againsts all independent varaibels. Correlation value 0.5 and above consider good correlation and therefore acceptable <br><br />
</li>
</ol></li>
<li><strong>NO Multicollinearity</strong> amont predictors - Meaning little or not linear correlationamong the predictors, i.e. it is not possible to express any predictor as a linear combination of the others, if so, we wouldn’t know which predictor actually influene the outcome. Validation method:
<ol style="list-style-type: lower-alpha">
<li>Scatter Plot Matrix for all independent variables<br />
</li>
<li>Correlation Matrix calculation for all independent varaibels. Correlation value 0.2 and below consider week correlation and therefore acceptable<br />
</li>
<li>Tolerance (<strong>T</strong>) - measures the influence of one independent variable on all other independent variables<br />
<strong>T &lt;0.01</strong> –&gt; <strong>confirm</strong> correlation<br />
<strong>T &lt;0.1</strong> –&gt; <strong>might have</strong> correlation<br />
</li>
<li>Variance Inflation Factor (<strong>VIF</strong>) - VIF = 1/T<br />
<strong>T &gt;100</strong> –&gt; <strong>confirm</strong> correlation<br />
<strong>T &gt;10</strong> –&gt; <strong>might have</strong> correlation<br />
</li>
<li>Condition Index (<strong>CI</strong>) - calculated using a factor analysis on the independent variables<br />
** Values of 10-30** –&gt; indicate a mediocre multicollinearity<br />
** Values &gt; 30** –&gt; strong multicollinearity</li>
</ol></li>
</ul>
<p><strong>Assumptions On Errors (residuals)</strong></p>
<ul>
<li>The <strong>errors are random numbers</strong>, with means of <strong>zero</strong>
<ul>
<li>There should not be a pattern in the residuals distribution<br />
</li>
<li>If the residuals are <strong>normally distributed</strong> with <strong>mean of zero</strong>, then it is considered a bonus which we can perform statistical significant testing. <span class="math inline">\(e = N(0,\sigma^2)\)</span><br />
</li>
<li>Normality on redisuals implies that the dependent variable are also normally distributed (if and only if <strong>dependent variable is not stochastic</strong>)</li>
</ul></li>
<li><p>The <strong>errors are uncorrelated</strong> - that is, the variance–covariance matrix of the errors is diagonal and each non-zero element is the variance of the error</p></li>
<li><p><strong>Homoscedasticity</strong> - The variance of the error is constant across observations. If heteroscedasticity exist, scatter plot of response and predictor will look like below</p></li>
</ul>
<p>If not, weighted least squares or other methods might instead be used. - The Goldfeld-Quandt Test can test for heteroscedasticity<br />
- If homoscedasticity is present, a non-linear correction might fix the problem</p>
<div id="are-the-assumptions-to-be-followed-strictly" class="section level4">
<h4><span class="header-section-number">11.4.2.1</span> Are The Assumptions to be followed strictly ?</h4>
<p>In real life, actual data <strong>rarely satisfies</strong> the assumptions, that is:</p>
<ul>
<li>Method is used even though the assumptions are not true<br />
</li>
<li>Variation from the assumptions can sometimes be used as a measure of how far the model is from being useful<br />
</li>
<li>Many of these assumptions may be relaxed in more advanced treatments</li>
</ul>
<p>Reports of statistical analyses usually include analyses of tests on the sample data and methodology for the fit and usefulness of the model.</p>
</div>
<div id="additional-notes-of-independent-variables" class="section level4">
<h4><span class="header-section-number">11.4.2.2</span> Additional notes of independent variables</h4>
<ul>
<li>Adding more variables to a regression procedure may <strong>overfit</strong> the model and make things worse. The idea is to pick the <strong>best</strong> variables<br />
</li>
<li>Some independent variable(s) are better at predicting the outocme, some contribute little or nothing</li>
</ul>
<p>Because of <strong>multicollinearity</strong> and <strong>overfitting</strong>, there is a fair amount of <strong>prep-work</strong> to be performed BEFORE conducting multiple regression analysis - if one is to do it properly.</p>
</div>
</div>
<div id="equations" class="section level3">
<h3><span class="header-section-number">11.4.3</span> Equations</h3>
<div id="terminology" class="section level4">
<h4><span class="header-section-number">11.4.3.1</span> Terminology</h4>
<p><strong>Simple</strong> Linear Regression (classical) consists of just on predictor. aka Single Variable Linear Regression.<br />
<strong>Multiple</strong> Linear Regression (classical) consists of multiple predictors. aka. Multiple Variable Linear Regression.</p>
<p><strong>Multivariate</strong> Regression (aka. <strong>General Linear Regression</strong>) is linear regression where the outocme is a vector (not scalar). Not the same as multiple variable linear regression.</p>
</div>
<div id="ordinary-least-square-estimatation" class="section level4">
<h4><span class="header-section-number">11.4.3.2</span> Ordinary Least Square Estimatation</h4>
<p><strong>Regression Model - Actual Outcome</strong></p>
<p><span class="math inline">\(\quad y_i = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + ... \beta_k x_k + e_i\)</span><br />
<span class="math inline">\(\quad\)</span>where:<br />
<span class="math inline">\(\quad \quad y_i\)</span> = actual outcome value<br />
<span class="math inline">\(\quad \quad \beta_0\)</span> = intercept, when all independent variables are 0<br />
<span class="math inline">\(\quad \quad \beta_k\)</span> = parameter for independent variable k <span class="math inline">\(\quad \quad e_i\)</span> = error for observation i</p>
<p><strong>Regression Equation - Predicted Outcome</strong></p>
<p><span class="math inline">\(\quad E(y_i) = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + ... \beta_k x_k\)</span></p>
<p><span class="math inline">\(\quad h_\theta(X)~=~\theta_0~+~\theta_1 \cdot X\)</span> , error terms assumed to be zero<br />
<span class="math inline">\(\quad\)</span>where:<br />
<span class="math inline">\(\quad \quad h_\theta(x)\)</span> = hypothesis target (dependant variable)<br />
<span class="math inline">\(\quad \quad \theta_0\)</span> = intercept<br />
<span class="math inline">\(\quad \quad \theta_1\)</span> = slopes or coefficients<br />
<span class="math inline">\(\quad \quad X\)</span> = independant variables (predictors)</p>
<p><span class="math inline">\(\quad\)</span>Take note that each <span class="math inline">\(\theta_0\)</span> and <span class="math inline">\(\theta_1\)</span> represents multi-variate data in matrix form.</p>
</div>
<div id="cost-function" class="section level4">
<h4><span class="header-section-number">11.4.3.3</span> Cost Function</h4>
<ul>
<li>The goal is to find some values of θ (known as coefficients), so we can minimize the difference between real values <span class="math inline">\(y\)</span> and predicted values (<span class="math inline">\(h(x)\)</span>)<br />
</li>
<li>Mathematically, this means finding the minimum value of cost function <span class="math inline">\(J\)</span> and derive the optimum value of <span class="math inline">\(\theta_0\)</span> and <span class="math inline">\(\theta_1\)</span><br />
</li>
<li>Linear regression uses Total Sum Of Square calculation on Error as Cost Function, denoted by <span class="math inline">\(J\)</span> below:</li>
</ul>
<p><span class="math inline">\(\quad \quad J(\theta_0,\theta_1) = \frac{1}{2m}\sum_{i=1}^m ((h_\theta(x^i)-y^i)^2\)</span></p>
</div>
</div>
<div id="ols-performance" class="section level3">
<h3><span class="header-section-number">11.4.4</span> OLS Performance</h3>
<div id="fundamental" class="section level4">
<h4><span class="header-section-number">11.4.4.1</span> Fundamental</h4>
<p>OLS performance is mainly on error analysis.</p>
<p><strong>SST</strong> (<strong>total</strong> sample variability) = <strong>SSR</strong> (<strong>explained</strong> variability) + <strong>SSE</strong> (<strong>unexplained</strong> variability) :</p>
<div class="figure">
<img src="images/explain_sst.jpg" alt="SST Explained" />
<p class="caption">SST Explained</p>
</div>
</div>
<div id="root-mean-square-error-rmse" class="section level4">
<h4><span class="header-section-number">11.4.4.2</span> Root Mean Square Error (RMSE)</h4>
<ul>
<li>RMSE = The square root of the average of the total sum of square error<br />
<span class="math inline">\(RMSE = \sqrt{\frac{SSE}{n}} = \sqrt \frac{\sum^n_{i=1}{(y_i - \hat y_i)^2}}{n}\)</span><br />
</li>
<li>It measure <strong>how close</strong> observed data points are to the model’s predicted values<br />
</li>
<li>Low RMSE value indicates better fit<br />
</li>
<li>Compared to the similar Mean Absolute Error, RMSE amplifies and severely punishes large errors</li>
</ul>
<blockquote>
<p><strong>RMSE</strong> is excellent general measurement to assess <strong>the accuracy</strong> of a model</p>
</blockquote>
</div>
<div id="r2-coefficient-of-determination-and-r2_adj" class="section level4">
<h4><span class="header-section-number">11.4.4.3</span> <span class="math inline">\(R^2\)</span> (Coefficient Of Determination) and <span class="math inline">\(R^2_{adj}\)</span></h4>
<p><strong>R-Squared</strong></p>
<ul>
<li><p><span class="math inline">\(R^2\)</span> is a ratio indicating how much <strong>variations are explained</strong> by regression model<br />
<span class="math inline">\(R^2 = \frac{SSR}{SST} = 1-\frac{SSE}{SST}, \quad 0 &lt;= R^2 &lt;= 1\)</span></p></li>
<li>High <span class="math inline">\(R^2\)</span> value indicates high SSR and low SSE, which means the model is <strong>more precise</strong>
<ul>
<li>In a <strong>perfect fit</strong> situation (SSE=0), <span class="math inline">\(R^2\)</span> will be 1.<br />
</li>
<li>In a worst fit situation (coefficient=0, SSR=0, aka horizontally flat line), <span class="math inline">\(R^2\)</span> will be 0.</li>
</ul></li>
<li><p>One pitfall of <span class="math inline">\(R^2\)</span> is that it <strong>always increases</strong> when additional variables are added to the model. The increase can be artificial as it doesn’t improve the model fit - which is called <strong>over-fitting</strong>. A <strong>remediation</strong> to this is <span class="math inline">\(R^2_{adj}\)</span></p></li>
</ul>
<p><strong>Adjusted R-Squared</strong></p>
<ul>
<li><p>Adjusted <span class="math inline">\(R^2\)</span> incorporates the number of coefficients and observations into the calculation<br />
<span class="math inline">\(R_{adj}^2 = 1- \bigg( \frac{n-1}{n-p}\bigg) \frac{SSE}{SST}\)</span><br />
<span class="math inline">\(\quad\)</span> p = number of coefficients (including intercept)<br />
<span class="math inline">\(\quad\)</span> n = number of observations</p></li>
<li>Adjusted <span class="math inline">\(R^2\)</span> will <strong>decrease</strong> when adding predictors that doesn’t increase the model fit that make up for the loss of degrees of freedom<br />
</li>
<li><p>Likewise, it will <strong>increase</strong> as predictors are added if the increase in model fit is <strong>worthwhile</strong></p></li>
</ul>
<blockquote>
<p><strong><span class="math inline">\(R^2_{adj}\)</span></strong> is useful for <strong>comparing models with a different number of predictors</strong>, hence can be use for features selection</p>
</blockquote>
</div>
</div>
<div id="sample-data" class="section level3">
<h3><span class="header-section-number">11.4.5</span> Sample Data</h3>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">1234</span>)
n=<span class="dv">100</span>
my.df =<span class="st"> </span><span class="kw">data.frame</span>(
  <span class="dt">id =</span> <span class="kw">paste</span>(<span class="st">&#39;ID&#39;</span>, <span class="dv">1</span><span class="op">:</span>n),
  <span class="dt">x1 =</span> <span class="dv">10</span><span class="op">:</span>(<span class="dv">10</span> <span class="op">+</span><span class="st"> </span>n <span class="op">-</span><span class="st"> </span><span class="dv">1</span>) <span class="op">*</span><span class="st"> </span><span class="kw">runif</span>(n, <span class="dt">min =</span> <span class="fl">0.5</span>, <span class="dt">max =</span> <span class="fl">1.1</span>),
  <span class="dt">x2 =</span> <span class="dv">20</span><span class="op">:</span>(<span class="dv">20</span> <span class="op">+</span><span class="st"> </span>n <span class="op">-</span><span class="st"> </span><span class="dv">1</span>) <span class="op">*</span><span class="st"> </span><span class="kw">runif</span>(n, <span class="dt">min =</span> <span class="fl">0.5</span>, <span class="dt">max =</span> <span class="fl">1.1</span>),
  <span class="dt">x3 =</span> <span class="dv">30</span><span class="op">:</span>(<span class="dv">30</span> <span class="op">+</span><span class="st"> </span>n <span class="op">-</span><span class="st"> </span><span class="dv">1</span>) <span class="op">*</span><span class="st"> </span><span class="kw">runif</span>(n, <span class="dt">min =</span> <span class="fl">0.5</span>, <span class="dt">max =</span> <span class="fl">1.1</span>)
)
my.df<span class="op">$</span>y =<span class="st"> </span><span class="dv">88</span> <span class="op">+</span><span class="st"> </span><span class="fl">0.1</span> <span class="op">*</span><span class="st"> </span>my.df<span class="op">$</span>x1 <span class="op">+</span><span class="st"> </span><span class="fl">0.2</span> <span class="op">*</span><span class="st"> </span>my.df<span class="op">$</span>x2 <span class="op">+</span><span class="st"> </span><span class="fl">0.3</span><span class="op">*</span>my.df<span class="op">$</span>x3
<span class="kw">cor</span>(my.df[, <span class="dv">2</span><span class="op">:</span><span class="dv">5</span>])
<span class="kw">lm</span>(<span class="dt">formula =</span> y <span class="op">~</span><span class="st"> </span>x1 <span class="op">+</span><span class="st"> </span>x2 <span class="op">+</span><span class="st"> </span>x3, <span class="dt">data =</span> my.df)</code></pre></div>
<pre><code>##           x1        x2        x3         y
## x1 1.0000000 0.7798930 0.7848857 0.8732915
## x2 0.7798930 1.0000000 0.7879751 0.9235238
## x3 0.7848857 0.7879751 1.0000000 0.9574600
## y  0.8732915 0.9235238 0.9574600 1.0000000
## 
## Call:
## lm(formula = y ~ x1 + x2 + x3, data = my.df)
## 
## Coefficients:
## (Intercept)           x1           x2           x3  
##        88.0          0.1          0.2          0.3</code></pre>
<p><span class="math inline">\(h(x_1,x_2,x_3) = \theta_0+ \theta_1x_1 + \theta_2x_2 + \theta_3X_3\)</span></p>
</div>
<div id="run-the-code" class="section level3">
<h3><span class="header-section-number">11.4.6</span> Run The Code</h3>
<blockquote>
<p><strong><code>fit = lm ( data = , formula = )</code></strong><br />
<strong><code>fit = lm ( data , formula )</code></strong><br />
<strong><code>fit = lm ( formula , data )</code></strong><br />
formula : y~x1 + x2 + x3<br />
data: matrix or dataframe</p>
</blockquote>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">cor</span>(my.df[, <span class="dv">2</span><span class="op">:</span><span class="dv">5</span>])
<span class="kw">lm</span>(<span class="dt">formula =</span> y <span class="op">~</span><span class="st"> </span>x1 <span class="op">+</span><span class="st"> </span>x2 <span class="op">+</span><span class="st"> </span>x3, <span class="dt">data =</span> my.df)</code></pre></div>
<pre><code>##           x1        x2        x3         y
## x1 1.0000000 0.7798930 0.7848857 0.8732915
## x2 0.7798930 1.0000000 0.7879751 0.9235238
## x3 0.7848857 0.7879751 1.0000000 0.9574600
## y  0.8732915 0.9235238 0.9574600 1.0000000
## 
## Call:
## lm(formula = y ~ x1 + x2 + x3, data = my.df)
## 
## Coefficients:
## (Intercept)           x1           x2           x3  
##        88.0          0.1          0.2          0.3</code></pre>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="11-3-types-of-regression.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="11-5-regression-diagnostic.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/yongks/biz_tech_analytics_r/edit/master/11-regression.Rmd",
"text": "Edit"
},
"download": ["biz_tech_analytics_r.pdf", "biz_tech_analytics_r.epub", "biz_tech_analytics_r.mobi"],
"toc": {
"collapse": "section",
"scroll_highlight": true
},
"toolbar": {
"position": "fixed"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
