[
["13-text-analysis.html", "Chapter 13 Text Analysis ", " Chapter 13 Text Analysis "],
["13-1-library.html", "13.1 Library", " 13.1 Library library(tm) library(SnowballC) "],
["13-2-concept.html", "13.2 Concept", " 13.2 Concept 13.2.1 Bag Of Words Count the number of times each words appears Each word is one feature Used as a baseline in text analysis and natural language processing Rquire further ‘preprocessing’ to dramatically improve performance ! 13.2.2 Text Pre-Processing 13.2.2.1 Convert to Lowercase Computers are very literal by default, eg. Apple and APPLE will be treated as different variable Hence, changing all words to lower-case or upper-case is essential 13.2.2.2 Cleaning Up Irregularities Punctuations causes problems Basic approach is to remove anything that is not alphabet For example, @Apple APPLE! --apple-- should be treated as just apple 13.2.2.3 Removing Unhelpful Terms Many words are frequently used but only meaningful in a sentence These are called: stop words They are unlikely to improve machine learning prediction quality, hence should be removed For example, the, is, at, which … 13.2.2.4 Stemming Stemming is an automated process which produces a base string in an attempt to represent related words For example, argu : can be represents: argue, argued, argues, arguing There are two ways to achive this: Database Of Stems Pro: handles exception well as all are predefined Cons: won’t handle new words, bad for internet Rule-Base Stemming This method is widely used totay ! eg. Porter Stemmer (Martin Porter, 1980) Pro: handles new/unknown words well, eg: dog: dog, dogs Cons: require many exception; such as prural handling, child and children (there is no childs) original: I cannot describe completely to you how important is children to our nation survivability after stemmer: I cannot describ complet to you how import is children to our nation surviv 13.2.3 Text Analytic Process Load data into dataframe of two columns: text message scoring (numeric) Preprocess The Data convert to lower case remove punctuation remove stop words Create Document Term Matrix (Bag of Words with frequency) Optimize Document Term Matrix (reduce sparseness) Merge term matrix table to original data.frame Each observation represents a document Each term is a new feature with its frequency as data "],
["13-3-sample-data.html", "13.3 Sample Data", " 13.3 Sample Data Sample data from tweets on Apple, it contains: Tweet: text messages Avg: Average score for potistive or negative feedback on Apple (manually tagged) tweets = read.csv(&#39;datasets/tweets.csv&#39;, stringsAsFactors = FALSE) str(tweets) ## &#39;data.frame&#39;: 1181 obs. of 2 variables: ## $ Tweet: chr &quot;I have to say, Apple has by far the best customer care service I have ever received! @Apple @AppStore&quot; &quot;iOS 7 is so fricking smooth &amp; beautiful!! #ThanxApple @Apple&quot; &quot;LOVE U @APPLE&quot; &quot;Thank you @apple, loving my new iPhone 5S!!!!! #apple #iphone5S pic.twitter.com/XmHJCU4pcb&quot; ... ## $ Avg : num 2 2 1.8 1.8 1.8 1.8 1.8 1.6 1.6 1.6 ... kable(head(tweets)) Tweet Avg I have to say, Apple has by far the best customer care service I have ever received! (???) (???) 2.0 iOS 7 is so fricking smooth &amp; beautiful!! #ThanxApple (???) 2.0 LOVE U (???) 1.8 Thank you (???), loving my new iPhone 5S!!!!! #apple #iphone5S pic.twitter.com/XmHJCU4pcb 1.8 .(???) has the best customer service. In and out with a new phone in under 10min! 1.8 (???) ear pods are AMAZING! Best sound from in-ear headphones I’ve ever had! 1.8 "],
["13-4-run-the-code.html", "13.4 Run The Code", " 13.4 Run The Code tm library is single library is able to handle corpus for text data analysis and pre-processing. 13.4.1 Convert Text To Corpus Corpus is a collection of documents containing (natural language) text It provide us a neat way to manipulate and work with text data for prediction 13.4.1.1 Convert Vector to Corpus Use tm::Corpus to convert text data into corpus. Corpus(x, control = list(language = &quot;en&quot;)) \\(\\quad\\) x : VectorSoruce(vector) or DirSource() corpus = Corpus(VectorSource(tweets$Tweet)) 13.4.1.2 Understand Corpus Structure Corpus treats every element in the vector as a document It returns a list object, which contain multiple lists (one list for each document) Each document list contain two items: meta data and content str(corpus[1]) ## List of 1 ## $ 1:List of 2 ## ..$ content: chr &quot;I have to say, Apple has by far the best customer care service I have ever received! @Apple @AppStore&quot; ## ..$ meta :List of 7 ## .. ..$ author : chr(0) ## .. ..$ datetimestamp: POSIXlt[1:1], format: &quot;2017-07-11 15:25:30&quot; ## .. ..$ description : chr(0) ## .. ..$ heading : chr(0) ## .. ..$ id : chr &quot;1&quot; ## .. ..$ language : chr &quot;en&quot; ## .. ..$ origin : chr(0) ## .. ..- attr(*, &quot;class&quot;)= chr &quot;TextDocumentMeta&quot; ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;PlainTextDocument&quot; &quot;TextDocument&quot; ## - attr(*, &quot;class&quot;)= chr [1:2] &quot;SimpleCorpus&quot; &quot;Corpus&quot; corpus[1:3]$content ## [1] &quot;I have to say, Apple has by far the best customer care service I have ever received! @Apple @AppStore&quot; ## [2] &quot;iOS 7 is so fricking smooth &amp; beautiful!! #ThanxApple @Apple&quot; ## [3] &quot;LOVE U @APPLE&quot; corpus[1:3]$meta ## $language ## [1] &quot;en&quot; ## ## attr(,&quot;class&quot;) ## [1] &quot;CorpusMeta&quot; 13.4.1.3 Viewing Corpus Data tm provides a better way to view the content of corpus using inspect() inspect(corpus[1:3]) ## &lt;&lt;SimpleCorpus&gt;&gt; ## Metadata: corpus specific: 1, document level (indexed): 0 ## Content: documents: 3 ## ## [1] I have to say, Apple has by far the best customer care service I have ever received! @Apple @AppStore ## [2] iOS 7 is so fricking smooth &amp; beautiful!! #ThanxApple @Apple ## [3] LOVE U @APPLE 13.4.2 Preprocess Corpus 13.4.2.1 Converting to lowercase Convert entire corpus to lowercase, using base R function tolower(). corpus = tm_map(corpus,tolower) corpus[1]$content ## [1] &quot;i have to say, apple has by far the best customer care service i have ever received! @apple @appstore&quot; 13.4.2.2 Remove Punctuation Remove punctuation for entire corpus, using tm::removePunctuation. corpus = tm_map(corpus,removePunctuation) corpus[1]$content ## [1] &quot;i have to say apple has by far the best customer care service i have ever received apple appstore&quot; 13.4.2.3 Remove Stopwords tm::stopwords contain 174 pre-defined stop words. Default is english. stopwords() ## [1] &quot;i&quot; &quot;me&quot; &quot;my&quot; &quot;myself&quot; &quot;we&quot; ## [6] &quot;our&quot; &quot;ours&quot; &quot;ourselves&quot; &quot;you&quot; &quot;your&quot; ## [11] &quot;yours&quot; &quot;yourself&quot; &quot;yourselves&quot; &quot;he&quot; &quot;him&quot; ## [16] &quot;his&quot; &quot;himself&quot; &quot;she&quot; &quot;her&quot; &quot;hers&quot; ## [21] &quot;herself&quot; &quot;it&quot; &quot;its&quot; &quot;itself&quot; &quot;they&quot; ## [26] &quot;them&quot; &quot;their&quot; &quot;theirs&quot; &quot;themselves&quot; &quot;what&quot; ## [31] &quot;which&quot; &quot;who&quot; &quot;whom&quot; &quot;this&quot; &quot;that&quot; ## [36] &quot;these&quot; &quot;those&quot; &quot;am&quot; &quot;is&quot; &quot;are&quot; ## [41] &quot;was&quot; &quot;were&quot; &quot;be&quot; &quot;been&quot; &quot;being&quot; ## [46] &quot;have&quot; &quot;has&quot; &quot;had&quot; &quot;having&quot; &quot;do&quot; ## [51] &quot;does&quot; &quot;did&quot; &quot;doing&quot; &quot;would&quot; &quot;should&quot; ## [56] &quot;could&quot; &quot;ought&quot; &quot;i&#39;m&quot; &quot;you&#39;re&quot; &quot;he&#39;s&quot; ## [61] &quot;she&#39;s&quot; &quot;it&#39;s&quot; &quot;we&#39;re&quot; &quot;they&#39;re&quot; &quot;i&#39;ve&quot; ## [66] &quot;you&#39;ve&quot; &quot;we&#39;ve&quot; &quot;they&#39;ve&quot; &quot;i&#39;d&quot; &quot;you&#39;d&quot; ## [71] &quot;he&#39;d&quot; &quot;she&#39;d&quot; &quot;we&#39;d&quot; &quot;they&#39;d&quot; &quot;i&#39;ll&quot; ## [76] &quot;you&#39;ll&quot; &quot;he&#39;ll&quot; &quot;she&#39;ll&quot; &quot;we&#39;ll&quot; &quot;they&#39;ll&quot; ## [81] &quot;isn&#39;t&quot; &quot;aren&#39;t&quot; &quot;wasn&#39;t&quot; &quot;weren&#39;t&quot; &quot;hasn&#39;t&quot; ## [86] &quot;haven&#39;t&quot; &quot;hadn&#39;t&quot; &quot;doesn&#39;t&quot; &quot;don&#39;t&quot; &quot;didn&#39;t&quot; ## [91] &quot;won&#39;t&quot; &quot;wouldn&#39;t&quot; &quot;shan&#39;t&quot; &quot;shouldn&#39;t&quot; &quot;can&#39;t&quot; ## [96] &quot;cannot&quot; &quot;couldn&#39;t&quot; &quot;mustn&#39;t&quot; &quot;let&#39;s&quot; &quot;that&#39;s&quot; ## [101] &quot;who&#39;s&quot; &quot;what&#39;s&quot; &quot;here&#39;s&quot; &quot;there&#39;s&quot; &quot;when&#39;s&quot; ## [106] &quot;where&#39;s&quot; &quot;why&#39;s&quot; &quot;how&#39;s&quot; &quot;a&quot; &quot;an&quot; ## [111] &quot;the&quot; &quot;and&quot; &quot;but&quot; &quot;if&quot; &quot;or&quot; ## [116] &quot;because&quot; &quot;as&quot; &quot;until&quot; &quot;while&quot; &quot;of&quot; ## [121] &quot;at&quot; &quot;by&quot; &quot;for&quot; &quot;with&quot; &quot;about&quot; ## [126] &quot;against&quot; &quot;between&quot; &quot;into&quot; &quot;through&quot; &quot;during&quot; ## [131] &quot;before&quot; &quot;after&quot; &quot;above&quot; &quot;below&quot; &quot;to&quot; ## [136] &quot;from&quot; &quot;up&quot; &quot;down&quot; &quot;in&quot; &quot;out&quot; ## [141] &quot;on&quot; &quot;off&quot; &quot;over&quot; &quot;under&quot; &quot;again&quot; ## [146] &quot;further&quot; &quot;then&quot; &quot;once&quot; &quot;here&quot; &quot;there&quot; ## [151] &quot;when&quot; &quot;where&quot; &quot;why&quot; &quot;how&quot; &quot;all&quot; ## [156] &quot;any&quot; &quot;both&quot; &quot;each&quot; &quot;few&quot; &quot;more&quot; ## [161] &quot;most&quot; &quot;other&quot; &quot;some&quot; &quot;such&quot; &quot;no&quot; ## [166] &quot;nor&quot; &quot;not&quot; &quot;only&quot; &quot;own&quot; &quot;same&quot; ## [171] &quot;so&quot; &quot;than&quot; &quot;too&quot; &quot;very&quot; Remove all stopwords from the entire corpus, with additional ‘apple’ stopword. Notice that, the remmoved character was replaced with empty spaces. corpus = tm_map(corpus, removeWords, c(&#39;apple&#39;, stopwords())) corpus[1]$content ## [1] &quot; say far best customer care service ever received appstore&quot; 13.4.3 Extract Bag Of Words tm:DocumentTermMatrix creates a bag of words from the corpus, and derive a frequency table for each words used in the corpus. freq = DocumentTermMatrix(corpus) freq ## &lt;&lt;DocumentTermMatrix (documents: 1181, terms: 3779)&gt;&gt; ## Non-/sparse entries: 9121/4453878 ## Sparsity : 100% ## Maximal term length: 115 ## Weighting : term frequency (tf) 13.4.3.1 View Term Matrix Table tm::DocumentTermMatrix is an complex object. Viewing with [,] notation won’t be possible Use tm::inspect() to look at the table content You will find that most likely the documents are sparsed (most words has 0 frequency) str(freq) ## List of 6 ## $ i : int [1:9121] 1 1 1 1 1 1 1 1 1 2 ... ## $ j : int [1:9121] 1 2 3 4 5 6 7 8 9 10 ... ## $ v : num [1:9121] 1 1 1 1 1 1 1 1 1 1 ... ## $ nrow : int 1181 ## $ ncol : int 3779 ## $ dimnames:List of 2 ## ..$ Docs : chr [1:1181] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... ## ..$ Terms: chr [1:3779] &quot;appstore&quot; &quot;best&quot; &quot;care&quot; &quot;customer&quot; ... ## - attr(*, &quot;class&quot;)= chr [1:2] &quot;DocumentTermMatrix&quot; &quot;simple_triplet_matrix&quot; ## - attr(*, &quot;weighting&quot;)= chr [1:2] &quot;term frequency&quot; &quot;tf&quot; inspect(freq[1000:1003, 505:511]) ## &lt;&lt;DocumentTermMatrix (documents: 4, terms: 7)&gt;&gt; ## Non-/sparse entries: 1/27 ## Sparsity : 96% ## Maximal term length: 11 ## Weighting : term frequency (tf) ## Sample : ## Terms ## Docs home itunes itunesmusic musicians office pen scaleboxapp ## 1000 0 1 0 0 0 0 0 ## 1001 0 0 0 0 0 0 0 ## 1002 0 0 0 0 0 0 0 ## 1003 0 0 0 0 0 0 0 13.4.3.2 Most Frequent Terms Since the terms are sparsed within the corpus, we should be interested to know what are the most frequent words in the corpus (that appeared more than 100 times). findFreqTerms(freq, lowfreq=100) ## [1] &quot;iphone&quot; &quot;new&quot; 13.4.3.3 Optimize The Term Matrix Table There are obviously too many words in the bag (too many terms) to be useful: We want to reduce the number words by selecting just the most frequent ones Specifying 0.97 means to select only words that appear in 3% of the documents Notice the number of terms in the new term matrix table has been reduced freq.sparse = removeSparseTerms(freq, 0.97) freq.sparse ## &lt;&lt;DocumentTermMatrix (documents: 1181, terms: 18)&gt;&gt; ## Non-/sparse entries: 1134/20124 ## Sparsity : 95% ## Maximal term length: 20 ## Weighting : term frequency (tf) 13.4.3.4 Merge Term Matrix To DataFrame We can now convert the term matrix table into data.frame: Each row represents an obervation as in original data New columns has been created with terms as the new variable, frequency as its data tweets.sparse = as.data.frame(as.matrix(freq.sparse)) -->"]
]
