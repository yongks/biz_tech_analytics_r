[
["index.html", "Business and Technical Analytics with R Preface", " Business and Technical Analytics with R Yong Keh Soon 2017-05-30 Preface "],
["1-fundamentals.html", "Chapter 1 R Fundamentals ", " Chapter 1 R Fundamentals "],
["1-1-package-management.html", "1.1 Package Management", " 1.1 Package Management Use installed.packages() : return matrix to list all installed package 1.1.1 Packages Listing Below are the column names and number for filtering purpose dimnames( installed.packages() ) [2] ## [[1]] ## [1] &quot;Package&quot; &quot;LibPath&quot; ## [3] &quot;Version&quot; &quot;Priority&quot; ## [5] &quot;Depends&quot; &quot;Imports&quot; ## [7] &quot;LinkingTo&quot; &quot;Suggests&quot; ## [9] &quot;Enhances&quot; &quot;License&quot; ## [11] &quot;License_is_FOSS&quot; &quot;License_restricts_use&quot; ## [13] &quot;OS_type&quot; &quot;MD5sum&quot; ## [15] &quot;NeedsCompilation&quot; &quot;Built&quot; Perform filter based on dimension names when necessary. Set priority = ‘NA’ will filter out all base R packages installed.packages( priority=&#39;NA&#39; ) [,c(3,2)] ## Version LibPath ## assertthat &quot;0.2.0&quot; &quot;C:/Users/YKS-NIC/Documents/R/win-library/3.3&quot; ## backports &quot;1.1.0&quot; &quot;C:/Users/YKS-NIC/Documents/R/win-library/3.3&quot; ## base64enc &quot;0.1-3&quot; &quot;C:/Users/YKS-NIC/Documents/R/win-library/3.3&quot; ## BH &quot;1.62.0-1&quot; &quot;C:/Users/YKS-NIC/Documents/R/win-library/3.3&quot; ## bitops &quot;1.0-6&quot; &quot;C:/Users/YKS-NIC/Documents/R/win-library/3.3&quot; ## bookdown &quot;0.4&quot; &quot;C:/Users/YKS-NIC/Documents/R/win-library/3.3&quot; ## broom &quot;0.4.2&quot; &quot;C:/Users/YKS-NIC/Documents/R/win-library/3.3&quot; ## car &quot;2.1-4&quot; &quot;C:/Users/YKS-NIC/Documents/R/win-library/3.3&quot; ## caret &quot;6.0-76&quot; &quot;C:/Users/YKS-NIC/Documents/R/win-library/3.3&quot; ## caTools &quot;1.17.1&quot; &quot;C:/Users/YKS-NIC/Documents/R/win-library/3.3&quot; ## cellranger &quot;1.1.0&quot; &quot;C:/Users/YKS-NIC/Documents/R/win-library/3.3&quot; ## colorspace &quot;1.3-2&quot; &quot;C:/Users/YKS-NIC/Documents/R/win-library/3.3&quot; ## colourpicker &quot;0.3&quot; &quot;C:/Users/YKS-NIC/Documents/R/win-library/3.3&quot; ## curl &quot;2.6&quot; &quot;C:/Users/YKS-NIC/Documents/R/win-library/3.3&quot; ## data.table &quot;1.10.4&quot; &quot;C:/Users/YKS-NIC/Documents/R/win-library/3.3&quot; ## DBI &quot;0.6-1&quot; &quot;C:/Users/YKS-NIC/Documents/R/win-library/3.3&quot; ## dichromat &quot;2.0-0&quot; &quot;C:/Users/YKS-NIC/Documents/R/win-library/3.3&quot; ## digest &quot;0.6.12&quot; &quot;C:/Users/YKS-NIC/Documents/R/win-library/3.3&quot; ## dplyr &quot;0.5.0&quot; &quot;C:/Users/YKS-NIC/Documents/R/win-library/3.3&quot; ## evaluate &quot;0.10&quot; &quot;C:/Users/YKS-NIC/Documents/R/win-library/3.3&quot; ## flexclust &quot;1.3-4&quot; &quot;C:/Users/YKS-NIC/Documents/R/win-library/3.3&quot; ## forcats &quot;0.2.0&quot; &quot;C:/Users/YKS-NIC/Documents/R/win-library/3.3&quot; ## foreach &quot;1.4.3&quot; &quot;C:/Users/YKS-NIC/Documents/R/win-library/3.3&quot; ## formatR &quot;1.5&quot; &quot;C:/Users/YKS-NIC/Documents/R/win-library/3.3&quot; ## gcookbook &quot;1.0&quot; &quot;C:/Users/YKS-NIC/Documents/R/win-library/3.3&quot; ## GGally &quot;1.3.0&quot; &quot;C:/Users/YKS-NIC/Documents/R/win-library/3.3&quot; ## ggExtra &quot;0.6&quot; &quot;C:/Users/YKS-NIC/Documents/R/win-library/3.3&quot; ## ggplot2 &quot;2.2.1&quot; &quot;C:/Users/YKS-NIC/Documents/R/win-library/3.3&quot; ## gridExtra &quot;2.2.1&quot; &quot;C:/Users/YKS-NIC/Documents/R/win-library/3.3&quot; ## gtable &quot;0.2.0&quot; &quot;C:/Users/YKS-NIC/Documents/R/win-library/3.3&quot; ## haven &quot;1.0.0&quot; &quot;C:/Users/YKS-NIC/Documents/R/win-library/3.3&quot; ## highr &quot;0.6&quot; &quot;C:/Users/YKS-NIC/Documents/R/win-library/3.3&quot; ## hms &quot;0.3&quot; &quot;C:/Users/YKS-NIC/Documents/R/win-library/3.3&quot; ## htmltools &quot;0.3.6&quot; &quot;C:/Users/YKS-NIC/Documents/R/win-library/3.3&quot; ## htmlwidgets &quot;0.8&quot; &quot;C:/Users/YKS-NIC/Documents/R/win-library/3.3&quot; ## httpuv &quot;1.3.3&quot; &quot;C:/Users/YKS-NIC/Documents/R/win-library/3.3&quot; ## httr &quot;1.2.1&quot; &quot;C:/Users/YKS-NIC/Documents/R/win-library/3.3&quot; ## iterators &quot;1.0.8&quot; &quot;C:/Users/YKS-NIC/Documents/R/win-library/3.3&quot; ## jsonlite &quot;1.4&quot; &quot;C:/Users/YKS-NIC/Documents/R/win-library/3.3&quot; ## knitr &quot;1.16&quot; &quot;C:/Users/YKS-NIC/Documents/R/win-library/3.3&quot; ## labeling &quot;0.3&quot; &quot;C:/Users/YKS-NIC/Documents/R/win-library/3.3&quot; ## lazyeval &quot;0.2.0&quot; &quot;C:/Users/YKS-NIC/Documents/R/win-library/3.3&quot; ## lme4 &quot;1.1-13&quot; &quot;C:/Users/YKS-NIC/Documents/R/win-library/3.3&quot; ## lubridate &quot;1.6.0&quot; &quot;C:/Users/YKS-NIC/Documents/R/win-library/3.3&quot; ## magrittr &quot;1.5&quot; &quot;C:/Users/YKS-NIC/Documents/R/win-library/3.3&quot; ## markdown &quot;0.8&quot; &quot;C:/Users/YKS-NIC/Documents/R/win-library/3.3&quot; ## MatrixModels &quot;0.4-1&quot; &quot;C:/Users/YKS-NIC/Documents/R/win-library/3.3&quot; ## mime &quot;0.5&quot; &quot;C:/Users/YKS-NIC/Documents/R/win-library/3.3&quot; ## miniUI &quot;0.1.1&quot; &quot;C:/Users/YKS-NIC/Documents/R/win-library/3.3&quot; ## minqa &quot;1.2.4&quot; &quot;C:/Users/YKS-NIC/Documents/R/win-library/3.3&quot; ## mnormt &quot;1.5-5&quot; &quot;C:/Users/YKS-NIC/Documents/R/win-library/3.3&quot; ## ModelMetrics &quot;1.1.0&quot; &quot;C:/Users/YKS-NIC/Documents/R/win-library/3.3&quot; ## modelr &quot;0.1.0&quot; &quot;C:/Users/YKS-NIC/Documents/R/win-library/3.3&quot; ## modeltools &quot;0.2-21&quot; &quot;C:/Users/YKS-NIC/Documents/R/win-library/3.3&quot; ## munsell &quot;0.4.3&quot; &quot;C:/Users/YKS-NIC/Documents/R/win-library/3.3&quot; ## nloptr &quot;1.0.4&quot; &quot;C:/Users/YKS-NIC/Documents/R/win-library/3.3&quot; ## openssl &quot;0.9.6&quot; &quot;C:/Users/YKS-NIC/Documents/R/win-library/3.3&quot; ## pbkrtest &quot;0.4-7&quot; &quot;C:/Users/YKS-NIC/Documents/R/win-library/3.3&quot; ## plyr &quot;1.8.4&quot; &quot;C:/Users/YKS-NIC/Documents/R/win-library/3.3&quot; ## prettyunits &quot;1.0.2&quot; &quot;C:/Users/YKS-NIC/Documents/R/win-library/3.3&quot; ## progress &quot;1.1.2&quot; &quot;C:/Users/YKS-NIC/Documents/R/win-library/3.3&quot; ## psych &quot;1.7.5&quot; &quot;C:/Users/YKS-NIC/Documents/R/win-library/3.3&quot; ## purrr &quot;0.2.2.2&quot; &quot;C:/Users/YKS-NIC/Documents/R/win-library/3.3&quot; ## quantreg &quot;5.33&quot; &quot;C:/Users/YKS-NIC/Documents/R/win-library/3.3&quot; ## R6 &quot;2.2.1&quot; &quot;C:/Users/YKS-NIC/Documents/R/win-library/3.3&quot; ## RColorBrewer &quot;1.1-2&quot; &quot;C:/Users/YKS-NIC/Documents/R/win-library/3.3&quot; ## Rcpp &quot;0.12.11&quot; &quot;C:/Users/YKS-NIC/Documents/R/win-library/3.3&quot; ## RcppEigen &quot;0.3.3.3.0&quot; &quot;C:/Users/YKS-NIC/Documents/R/win-library/3.3&quot; ## readr &quot;1.1.1&quot; &quot;C:/Users/YKS-NIC/Documents/R/win-library/3.3&quot; ## readxl &quot;1.0.0&quot; &quot;C:/Users/YKS-NIC/Documents/R/win-library/3.3&quot; ## rematch &quot;1.0.1&quot; &quot;C:/Users/YKS-NIC/Documents/R/win-library/3.3&quot; ## reshape &quot;0.8.6&quot; &quot;C:/Users/YKS-NIC/Documents/R/win-library/3.3&quot; ## reshape2 &quot;1.4.2&quot; &quot;C:/Users/YKS-NIC/Documents/R/win-library/3.3&quot; ## rlang &quot;0.1.1&quot; &quot;C:/Users/YKS-NIC/Documents/R/win-library/3.3&quot; ## rmarkdown &quot;1.5&quot; &quot;C:/Users/YKS-NIC/Documents/R/win-library/3.3&quot; ## rprojroot &quot;1.2&quot; &quot;C:/Users/YKS-NIC/Documents/R/win-library/3.3&quot; ## rstudioapi &quot;0.6&quot; &quot;C:/Users/YKS-NIC/Documents/R/win-library/3.3&quot; ## rvest &quot;0.3.2&quot; &quot;C:/Users/YKS-NIC/Documents/R/win-library/3.3&quot; ## scales &quot;0.4.1&quot; &quot;C:/Users/YKS-NIC/Documents/R/win-library/3.3&quot; ## selectr &quot;0.3-1&quot; &quot;C:/Users/YKS-NIC/Documents/R/win-library/3.3&quot; ## servr &quot;0.6&quot; &quot;C:/Users/YKS-NIC/Documents/R/win-library/3.3&quot; ## shiny &quot;1.0.3&quot; &quot;C:/Users/YKS-NIC/Documents/R/win-library/3.3&quot; ## shinyjs &quot;0.9&quot; &quot;C:/Users/YKS-NIC/Documents/R/win-library/3.3&quot; ## sourcetools &quot;0.1.6&quot; &quot;C:/Users/YKS-NIC/Documents/R/win-library/3.3&quot; ## SparseM &quot;1.77&quot; &quot;C:/Users/YKS-NIC/Documents/R/win-library/3.3&quot; ## stringi &quot;1.1.5&quot; &quot;C:/Users/YKS-NIC/Documents/R/win-library/3.3&quot; ## stringr &quot;1.2.0&quot; &quot;C:/Users/YKS-NIC/Documents/R/win-library/3.3&quot; ## tibble &quot;1.3.3&quot; &quot;C:/Users/YKS-NIC/Documents/R/win-library/3.3&quot; ## tidyr &quot;0.6.3&quot; &quot;C:/Users/YKS-NIC/Documents/R/win-library/3.3&quot; ## tidyverse &quot;1.1.1&quot; &quot;C:/Users/YKS-NIC/Documents/R/win-library/3.3&quot; ## xml2 &quot;1.1.1&quot; &quot;C:/Users/YKS-NIC/Documents/R/win-library/3.3&quot; ## xtable &quot;1.8-2&quot; &quot;C:/Users/YKS-NIC/Documents/R/win-library/3.3&quot; ## yaml &quot;2.1.14&quot; &quot;C:/Users/YKS-NIC/Documents/R/win-library/3.3&quot; ## translations &quot;3.3.3&quot; &quot;C:/Program Files/R/R-3.3.3/library&quot; Set priority = ‘high’ will display all base R packages installed.packages( priority=&#39;high&#39; ) [,c(3,2)] ## Version LibPath ## boot &quot;1.3-19&quot; &quot;C:/Users/YKS-NIC/Documents/R/win-library/3.3&quot; ## cluster &quot;2.0.6&quot; &quot;C:/Users/YKS-NIC/Documents/R/win-library/3.3&quot; ## foreign &quot;0.8-68&quot; &quot;C:/Users/YKS-NIC/Documents/R/win-library/3.3&quot; ## lattice &quot;0.20-35&quot; &quot;C:/Users/YKS-NIC/Documents/R/win-library/3.3&quot; ## MASS &quot;7.3-47&quot; &quot;C:/Users/YKS-NIC/Documents/R/win-library/3.3&quot; ## Matrix &quot;1.2-10&quot; &quot;C:/Users/YKS-NIC/Documents/R/win-library/3.3&quot; ## rpart &quot;4.1-11&quot; &quot;C:/Users/YKS-NIC/Documents/R/win-library/3.3&quot; ## survival &quot;2.41-3&quot; &quot;C:/Users/YKS-NIC/Documents/R/win-library/3.3&quot; ## base &quot;3.3.3&quot; &quot;C:/Program Files/R/R-3.3.3/library&quot; ## boot &quot;1.3-18&quot; &quot;C:/Program Files/R/R-3.3.3/library&quot; ## class &quot;7.3-14&quot; &quot;C:/Program Files/R/R-3.3.3/library&quot; ## cluster &quot;2.0.5&quot; &quot;C:/Program Files/R/R-3.3.3/library&quot; ## codetools &quot;0.2-15&quot; &quot;C:/Program Files/R/R-3.3.3/library&quot; ## compiler &quot;3.3.3&quot; &quot;C:/Program Files/R/R-3.3.3/library&quot; ## datasets &quot;3.3.3&quot; &quot;C:/Program Files/R/R-3.3.3/library&quot; ## foreign &quot;0.8-67&quot; &quot;C:/Program Files/R/R-3.3.3/library&quot; ## graphics &quot;3.3.3&quot; &quot;C:/Program Files/R/R-3.3.3/library&quot; ## grDevices &quot;3.3.3&quot; &quot;C:/Program Files/R/R-3.3.3/library&quot; ## grid &quot;3.3.3&quot; &quot;C:/Program Files/R/R-3.3.3/library&quot; ## KernSmooth &quot;2.23-15&quot; &quot;C:/Program Files/R/R-3.3.3/library&quot; ## lattice &quot;0.20-34&quot; &quot;C:/Program Files/R/R-3.3.3/library&quot; ## MASS &quot;7.3-45&quot; &quot;C:/Program Files/R/R-3.3.3/library&quot; ## Matrix &quot;1.2-8&quot; &quot;C:/Program Files/R/R-3.3.3/library&quot; ## methods &quot;3.3.3&quot; &quot;C:/Program Files/R/R-3.3.3/library&quot; ## mgcv &quot;1.8-17&quot; &quot;C:/Program Files/R/R-3.3.3/library&quot; ## nlme &quot;3.1-131&quot; &quot;C:/Program Files/R/R-3.3.3/library&quot; ## nnet &quot;7.3-12&quot; &quot;C:/Program Files/R/R-3.3.3/library&quot; ## parallel &quot;3.3.3&quot; &quot;C:/Program Files/R/R-3.3.3/library&quot; ## rpart &quot;4.1-10&quot; &quot;C:/Program Files/R/R-3.3.3/library&quot; ## spatial &quot;7.3-11&quot; &quot;C:/Program Files/R/R-3.3.3/library&quot; ## splines &quot;3.3.3&quot; &quot;C:/Program Files/R/R-3.3.3/library&quot; ## stats &quot;3.3.3&quot; &quot;C:/Program Files/R/R-3.3.3/library&quot; ## stats4 &quot;3.3.3&quot; &quot;C:/Program Files/R/R-3.3.3/library&quot; ## survival &quot;2.40-1&quot; &quot;C:/Program Files/R/R-3.3.3/library&quot; ## tcltk &quot;3.3.3&quot; &quot;C:/Program Files/R/R-3.3.3/library&quot; ## tools &quot;3.3.3&quot; &quot;C:/Program Files/R/R-3.3.3/library&quot; ## utils &quot;3.3.3&quot; &quot;C:/Program Files/R/R-3.3.3/library&quot; 1.1.2 Package Install / Remove install.packages( c(&#39;ggplot&#39;, &#39;ggExtra&#39;)) 1.1.3 Package Update old.packages() ## list all old packages, their old version and available new version update.packages() ## need to input y/n/c for each old package to update update.packages( ask=FALSE ) ## automatically update all packages without asking "],
["1-2-data-types.html", "1.2 Data Types", " 1.2 Data Types "],
["1-3-conditioinal-decision.html", "1.3 Conditioinal Decision", " 1.3 Conditioinal Decision "],
["1-4-looping.html", "1.4 Looping", " 1.4 Looping "],
["1-5-apply-family.html", "1.5 Apply Family", " 1.5 Apply Family 1.5.1 Overview Apply and its family is a useful function 1.5.2 apply 1.5.3 sapply 1.5.4 tapply -->"],
["2-data-generation.html", "Chapter 2 Data Generation", " Chapter 2 Data Generation Here is a review of existing methods. "],
["2-1-sequential-number.html", "2.1 Sequential Number", " 2.1 Sequential Number 2.1.1 Using : : return vector Produce sequantial integer with fix incremental or decremental by 1 Incremental 3:6 # incremental integer 1.25:9.25 # incremental decimal c(3:6, 4.25:8.25) # combination of multiple sequence ## [1] 3 4 5 6 ## [1] 1.25 2.25 3.25 4.25 5.25 6.25 7.25 8.25 9.25 ## [1] 3.00 4.00 5.00 6.00 4.25 5.25 6.25 7.25 8.25 Decremental 6:3 # decremental integer 9.25: 1.25 # decremental decimal ## [1] 6 5 4 3 ## [1] 9.25 8.25 7.25 6.25 5.25 4.25 3.25 2.25 1.25 2.1.2 Using seq : return vector Improvement from :, seq allows specifying incremental steps with by=. seq( from, to ) seq( from, to, by = ) seq( from, to, length.out = ) # potentially return decimal Incremental seq (3, 12) # default increment by=1 seq (3, 12, by = 4) # increment of integer seq (3.25, 12.25, by = 2.25) # increment of decimal ## [1] 3 4 5 6 7 8 9 10 11 12 ## [1] 3 7 11 ## [1] 3.25 5.50 7.75 10.00 12.25 Decremental - from must be larger than to, and by has to be negative. seq (12, 3) # default decrement by=-1 seq (12, 3, by = -4) # decrement of integer seq (12.25, 3.25, by = -2.25) # decrement of decimal ## [1] 12 11 10 9 8 7 6 5 4 3 ## [1] 12 8 4 ## [1] 12.25 10.00 7.75 5.50 3.25 Equal Spreading - with length.out= Equal Spreading of Integer seq (10, 50, length.out = 9) # incremental spreding of integer seq (50, 10, length.out = 9) # decremental spreading of integer ## [1] 10 15 20 25 30 35 40 45 50 ## [1] 50 45 40 35 30 25 20 15 10 Equal Spreading of Decimal seq (10.33, 50.55, length.out = 9) # incremental spreading of decimal seq (50.55, 10.33, length.out = 9) # decremental spreading of decimal ## [1] 10.3300 15.3575 20.3850 25.4125 30.4400 35.4675 40.4950 45.5225 50.5500 ## [1] 50.5500 45.5225 40.4950 35.4675 30.4400 25.4125 20.3850 15.3575 10.3300 "],
["2-2-random-number.html", "2.2 Random Number", " 2.2 Random Number 2.2.1 Random Unified Distribution runif( n ) # default min=0, max=1 runif( n, min=, max= ) set.seed(123) runif(5) # geenrate 5 numbers within default min=0, max=1 runif(5, min=3, max=9) ## [1] 0.2875775 0.7883051 0.4089769 0.8830174 0.9404673 ## [1] 3.273339 6.168633 8.354514 6.308610 5.739688 Notice that the numbers generated are uniformly distributed. hist(runif(300, min=3, max=9)) 2.2.2 Random Normal Distribution rnorm( n ) # default mean=0, sd=1 rnorm( n, mean=, sd= ) set.seed(123) rnorm(5) # geenrate 5 numbers within default min=0, max=1 rnorm(5, mean=3, sd=1.5) ## [1] -0.56047565 -0.23017749 1.55870831 0.07050839 0.12928774 ## [1] 5.572597 3.691374 1.102408 1.969721 2.331507 Notice that the numbers generated are uniformly distributed. hist(rnorm(300, mean=3, sd=1.5)) 2.2.3 Drawing From A Bag A bag has been occupied with vector x (produced using : or any other vector) sample() will draw from this bag Specifying replace=T emulate that the drwan sample will be put back to the bag for next draw R will generate error if no enough samples to draw from ( size &gt; length(x) ) sample( x=, size= ) sample( x=, size=, replace=T) # recycle number even though it is not exhausted set.seed (123) sample (10, 5) # choose 5 numbers from the &#39;bag&#39; containing 1:10 sample (3:10, 5) # choose 5 numbers from the &#39;bag&#39; containing 3:10 bag = runif(8, min=3, max=9) # define the content of the bag sample (bag, 5, replace=T) # draw from the bag, recycling numbers ## [1] 3 8 4 7 6 ## [1] 3 6 8 5 4 ## [1] 7.065424 3.252357 3.252357 8.398950 8.398950 -->"],
["3-data-summarization.html", "Chapter 3 Data Summarization", " Chapter 3 Data Summarization This capter explore manipulating table-like data, summarization and aggregation. "],
["3-1-sample-data.html", "3.1 Sample Data", " 3.1 Sample Data Sample data used simulate two categorical-alike feature, and two numeric value feature: dept is random character between ‘D1’, ‘D2’ and ‘D3’ grp is random character with randomly generated ‘G1’, ‘G2’ value1 represents numeric value, normally distributed at mean 50 value2 is numeric value, normally distributed at mean 25 set.seed(1234) my.df = data.frame( com = paste(&#39;C&#39;,sample(1:2, 100, replace = T),sep=&#39;&#39;), dept = paste(&#39;D&#39;,sample(1:3, 100, replace = T),sep=&#39;&#39;), grp = paste(&#39;G&#39;,sample(1:2, 100, replace = T),sep=&#39;&#39;), team = paste(&#39;T&#39;,sample(1:2, 100, replace = T),sep=&#39;&#39;), value1 = rnorm(1:100, mean = 50, sd = 5), value2 = rnorm(1:100, mean = 20, sd = 3), value3 = rnorm(1:100, mean = 5, sd = 1), stringsAsFactors = F ) head(my.df) ## com dept grp team value1 value2 value3 ## 1 C1 D1 G2 T1 52.42613 18.26013 3.773185 ## 2 C2 D2 G2 T2 53.48384 17.14016 5.036153 ## 3 C2 D1 G1 T2 50.92757 19.46171 4.578607 ## 4 C2 D1 G2 T1 53.50367 23.02942 4.100636 ## 5 C2 D1 G2 T2 51.55841 20.07088 5.417441 ## 6 C2 D1 G2 T1 53.80231 18.05292 5.153445 "],
["3-2-frequency-table.html", "3.2 Frequency Table", " 3.2 Frequency Table table return table object (which is also array) that report frequency count base of categorical-alike data provided. table has the below data type characteristics. Note that only 2-dimensional table object is a matrix Dimension is.atomic is.vector is.matrix is.array is.table t1 T F F T T t2 T F T T T t3 T F F T T t4 T F F T T ftable is technically a matrix with two dimensional data (it flatten multiple dimension data). It has below data type characteristic. Dimension is.atomic is.vector is.matrix is.array is.table 1 T F T T F 2 T F T T F 3 T F T T F 4 T F T T F 3.2.1 Single Dimension Data t1 = table( my.df$com ) t1 ## ## C1 C2 ## 55 45 str(t1) ## &#39;table&#39; int [1:2(1d)] 55 45 ## - attr(*, &quot;dimnames&quot;)=List of 1 ## ..$ : chr [1:2] &quot;C1&quot; &quot;C2&quot; 3.2.2 Two Dimension Data t2 = table( my.df$com, my.df$dept ) t2 ## ## D1 D2 D3 ## C1 16 18 21 ## C2 15 18 12 str(t2) ## &#39;table&#39; int [1:2, 1:3] 16 15 18 18 21 12 ## - attr(*, &quot;dimnames&quot;)=List of 2 ## ..$ : chr [1:2] &quot;C1&quot; &quot;C2&quot; ## ..$ : chr [1:3] &quot;D1&quot; &quot;D2&quot; &quot;D3&quot; 3.2.3 Three Dimension Data When table contain three or more dimension, use ftable (flat table) to put multi dimension table into one flat output t3 = table( my.df$com, my.df$dept, my.df$grp ) t3 ## , , = G1 ## ## ## D1 D2 D3 ## C1 10 7 11 ## C2 7 9 4 ## ## , , = G2 ## ## ## D1 D2 D3 ## C1 6 11 10 ## C2 8 9 8 str(t3) ## &#39;table&#39; int [1:2, 1:3, 1:2] 10 7 7 9 11 4 6 8 11 9 ... ## - attr(*, &quot;dimnames&quot;)=List of 3 ## ..$ : chr [1:2] &quot;C1&quot; &quot;C2&quot; ## ..$ : chr [1:3] &quot;D1&quot; &quot;D2&quot; &quot;D3&quot; ## ..$ : chr [1:2] &quot;G1&quot; &quot;G2&quot; f3 = ftable( t3 ) f3 ## G1 G2 ## ## C1 D1 10 6 ## D2 7 11 ## D3 11 10 ## C2 D1 7 8 ## D2 9 9 ## D3 4 8 str(f3) ## &#39;ftable&#39; int [1:6, 1:2] 10 7 11 7 9 4 6 11 10 8 ... ## - attr(*, &quot;row.vars&quot;)=List of 2 ## ..$ : chr [1:2] &quot;C1&quot; &quot;C2&quot; ## ..$ : chr [1:3] &quot;D1&quot; &quot;D2&quot; &quot;D3&quot; ## - attr(*, &quot;col.vars&quot;)=List of 1 ## ..$ : chr [1:2] &quot;G1&quot; &quot;G2&quot; 3.2.4 Four Dimension Data When table contain three or more dimension, use ftable (flat table) to put multi dimension table into one flat output t4 = table( my.df$com, my.df$dept, my.df$grp, my.df$team ) t4 ## , , = G1, = T1 ## ## ## D1 D2 D3 ## C1 4 5 4 ## C2 4 5 1 ## ## , , = G2, = T1 ## ## ## D1 D2 D3 ## C1 3 6 6 ## C2 5 5 4 ## ## , , = G1, = T2 ## ## ## D1 D2 D3 ## C1 6 2 7 ## C2 3 4 3 ## ## , , = G2, = T2 ## ## ## D1 D2 D3 ## C1 3 5 4 ## C2 3 4 4 str(t4) ## &#39;table&#39; int [1:2, 1:3, 1:2, 1:2] 4 4 5 5 4 1 3 5 6 5 ... ## - attr(*, &quot;dimnames&quot;)=List of 4 ## ..$ : chr [1:2] &quot;C1&quot; &quot;C2&quot; ## ..$ : chr [1:3] &quot;D1&quot; &quot;D2&quot; &quot;D3&quot; ## ..$ : chr [1:2] &quot;G1&quot; &quot;G2&quot; ## ..$ : chr [1:2] &quot;T1&quot; &quot;T2&quot; f4 = ftable( t4 ) f4 ## T1 T2 ## ## C1 D1 G1 4 6 ## G2 3 3 ## D2 G1 5 2 ## G2 6 5 ## D3 G1 4 7 ## G2 6 4 ## C2 D1 G1 4 3 ## G2 5 3 ## D2 G1 5 4 ## G2 5 4 ## D3 G1 1 3 ## G2 4 4 str(f4) ## &#39;ftable&#39; int [1:12, 1:2] 4 3 5 6 4 6 4 5 5 5 ... ## - attr(*, &quot;row.vars&quot;)=List of 3 ## ..$ : chr [1:2] &quot;C1&quot; &quot;C2&quot; ## ..$ : chr [1:3] &quot;D1&quot; &quot;D2&quot; &quot;D3&quot; ## ..$ : chr [1:2] &quot;G1&quot; &quot;G2&quot; ## - attr(*, &quot;col.vars&quot;)=List of 1 ## ..$ : chr [1:2] &quot;T1&quot; &quot;T2&quot; 3.2.5 Proportion Table prop.table converts table or ftable object into proportion. 3.2.5.1 Proportion Table on ‘table’ object pt1 = prop.table( t1 ) pt2 = prop.table( t2 ) pt1 ## ## C1 C2 ## 0.55 0.45 pt2 ## ## D1 D2 D3 ## C1 0.16 0.18 0.21 ## C2 0.15 0.18 0.12 3.2.5.2 Proportion Table on ‘ftable’ object pf3 = prop.table( f3 ) pf4 = prop.table( f4 ) pf3 ## G1 G2 ## ## C1 D1 0.10 0.06 ## D2 0.07 0.11 ## D3 0.11 0.10 ## C2 D1 0.07 0.08 ## D2 0.09 0.09 ## D3 0.04 0.08 pf4 ## T1 T2 ## ## C1 D1 G1 0.04 0.06 ## G2 0.03 0.03 ## D2 G1 0.05 0.02 ## G2 0.06 0.05 ## D3 G1 0.04 0.07 ## G2 0.06 0.04 ## C2 D1 G1 0.04 0.03 ## G2 0.05 0.03 ## D2 G1 0.05 0.04 ## G2 0.05 0.04 ## D3 G1 0.01 0.03 ## G2 0.04 0.04 3.2.6 Adding Margin Info 3.2.6.1 Margin Info on ‘table’ object addmargins( t2 ) ## ## D1 D2 D3 Sum ## C1 16 18 21 55 ## C2 15 18 12 45 ## Sum 31 36 33 100 addmargins( t3 ) ## , , = G1 ## ## ## D1 D2 D3 Sum ## C1 10 7 11 28 ## C2 7 9 4 20 ## Sum 17 16 15 48 ## ## , , = G2 ## ## ## D1 D2 D3 Sum ## C1 6 11 10 27 ## C2 8 9 8 25 ## Sum 14 20 18 52 ## ## , , = Sum ## ## ## D1 D2 D3 Sum ## C1 16 18 21 55 ## C2 15 18 12 45 ## Sum 31 36 33 100 addmargins( t4 ) ## , , = G1, = T1 ## ## ## D1 D2 D3 Sum ## C1 4 5 4 13 ## C2 4 5 1 10 ## Sum 8 10 5 23 ## ## , , = G2, = T1 ## ## ## D1 D2 D3 Sum ## C1 3 6 6 15 ## C2 5 5 4 14 ## Sum 8 11 10 29 ## ## , , = Sum, = T1 ## ## ## D1 D2 D3 Sum ## C1 7 11 10 28 ## C2 9 10 5 24 ## Sum 16 21 15 52 ## ## , , = G1, = T2 ## ## ## D1 D2 D3 Sum ## C1 6 2 7 15 ## C2 3 4 3 10 ## Sum 9 6 10 25 ## ## , , = G2, = T2 ## ## ## D1 D2 D3 Sum ## C1 3 5 4 12 ## C2 3 4 4 11 ## Sum 6 9 8 23 ## ## , , = Sum, = T2 ## ## ## D1 D2 D3 Sum ## C1 9 7 11 27 ## C2 6 8 7 21 ## Sum 15 15 18 48 ## ## , , = G1, = Sum ## ## ## D1 D2 D3 Sum ## C1 10 7 11 28 ## C2 7 9 4 20 ## Sum 17 16 15 48 ## ## , , = G2, = Sum ## ## ## D1 D2 D3 Sum ## C1 6 11 10 27 ## C2 8 9 8 25 ## Sum 14 20 18 52 ## ## , , = Sum, = Sum ## ## ## D1 D2 D3 Sum ## C1 16 18 21 55 ## C2 15 18 12 45 ## Sum 31 36 33 100 3.2.6.2 Margin Info on ‘ftable’ object addmargins( f3 ) ## Sum ## 10 6 16 ## 7 11 18 ## 11 10 21 ## 7 8 15 ## 9 9 18 ## 4 8 12 ## Sum 48 52 100 addmargins( f4 ) ## Sum ## 4 6 10 ## 3 3 6 ## 5 2 7 ## 6 5 11 ## 4 7 11 ## 6 4 10 ## 4 3 7 ## 5 3 8 ## 5 4 9 ## 5 4 9 ## 1 3 4 ## 4 4 8 ## Sum 52 48 100 3.2.7 Proportion Table with Margin First to obtain the proportion table, then only add the margin. addmargins( prop.table( t2 )) ## ## D1 D2 D3 Sum ## C1 0.16 0.18 0.21 0.55 ## C2 0.15 0.18 0.12 0.45 ## Sum 0.31 0.36 0.33 1.00 addmargins( prop.table( t3 )) ## , , = G1 ## ## ## D1 D2 D3 Sum ## C1 0.10 0.07 0.11 0.28 ## C2 0.07 0.09 0.04 0.20 ## Sum 0.17 0.16 0.15 0.48 ## ## , , = G2 ## ## ## D1 D2 D3 Sum ## C1 0.06 0.11 0.10 0.27 ## C2 0.08 0.09 0.08 0.25 ## Sum 0.14 0.20 0.18 0.52 ## ## , , = Sum ## ## ## D1 D2 D3 Sum ## C1 0.16 0.18 0.21 0.55 ## C2 0.15 0.18 0.12 0.45 ## Sum 0.31 0.36 0.33 1.00 addmargins( prop.table( t4 )) ## , , = G1, = T1 ## ## ## D1 D2 D3 Sum ## C1 0.04 0.05 0.04 0.13 ## C2 0.04 0.05 0.01 0.10 ## Sum 0.08 0.10 0.05 0.23 ## ## , , = G2, = T1 ## ## ## D1 D2 D3 Sum ## C1 0.03 0.06 0.06 0.15 ## C2 0.05 0.05 0.04 0.14 ## Sum 0.08 0.11 0.10 0.29 ## ## , , = Sum, = T1 ## ## ## D1 D2 D3 Sum ## C1 0.07 0.11 0.10 0.28 ## C2 0.09 0.10 0.05 0.24 ## Sum 0.16 0.21 0.15 0.52 ## ## , , = G1, = T2 ## ## ## D1 D2 D3 Sum ## C1 0.06 0.02 0.07 0.15 ## C2 0.03 0.04 0.03 0.10 ## Sum 0.09 0.06 0.10 0.25 ## ## , , = G2, = T2 ## ## ## D1 D2 D3 Sum ## C1 0.03 0.05 0.04 0.12 ## C2 0.03 0.04 0.04 0.11 ## Sum 0.06 0.09 0.08 0.23 ## ## , , = Sum, = T2 ## ## ## D1 D2 D3 Sum ## C1 0.09 0.07 0.11 0.27 ## C2 0.06 0.08 0.07 0.21 ## Sum 0.15 0.15 0.18 0.48 ## ## , , = G1, = Sum ## ## ## D1 D2 D3 Sum ## C1 0.10 0.07 0.11 0.28 ## C2 0.07 0.09 0.04 0.20 ## Sum 0.17 0.16 0.15 0.48 ## ## , , = G2, = Sum ## ## ## D1 D2 D3 Sum ## C1 0.06 0.11 0.10 0.27 ## C2 0.08 0.09 0.08 0.25 ## Sum 0.14 0.20 0.18 0.52 ## ## , , = Sum, = Sum ## ## ## D1 D2 D3 Sum ## C1 0.16 0.18 0.21 0.55 ## C2 0.15 0.18 0.12 0.45 ## Sum 0.31 0.36 0.33 1.00 addmargins( prop.table( f3 )) ## Sum ## 0.10 0.06 0.16 ## 0.07 0.11 0.18 ## 0.11 0.10 0.21 ## 0.07 0.08 0.15 ## 0.09 0.09 0.18 ## 0.04 0.08 0.12 ## Sum 0.48 0.52 1.00 addmargins( prop.table( f4 )) ## Sum ## 0.04 0.06 0.10 ## 0.03 0.03 0.06 ## 0.05 0.02 0.07 ## 0.06 0.05 0.11 ## 0.04 0.07 0.11 ## 0.06 0.04 0.10 ## 0.04 0.03 0.07 ## 0.05 0.03 0.08 ## 0.05 0.04 0.09 ## 0.05 0.04 0.09 ## 0.01 0.03 0.04 ## 0.04 0.04 0.08 ## Sum 0.52 0.48 1.00 "],
["3-3-data-aggregation.html", "3.3 Data Aggregation", " 3.3 Data Aggregation This chapter explore multiple methods to group data columns and computes value within groups. 3.3.1 aggretate - base R Aggregate is a very useful base R function and provides quick way to group data and values: Input in list/data.frame, computes and output new data.frame. It groups categorical variable(s) and compute value variable(s) based on function FUN. FUN can be min, max, mean, sd, sum or length (frequency count). ONLY ONE function is supported, and it applies to all value variables !!! 3.3.1.1 Basic Syntax (formula method) - data source is data.frame The formula method use ‘data’ parameter and therefore apply for single data source only. The objective is simplicy and without flexibility to customize column names aggregate (data = df, formula, FUN = function) Formula in the form: value~categorical one value variable ~ one categorical variable aggregate (data = my.df, value1 ~ grp, FUN = length) ## grp value1 ## 1 G1 48 ## 2 G2 52 one value variable ~ multiple categorical variables aggregate (data = my.df, value1 ~ grp + dept, FUN = length) ## grp dept value1 ## 1 G1 D1 17 ## 2 G2 D1 14 ## 3 G1 D2 16 ## 4 G2 D2 20 ## 5 G1 D3 15 ## 6 G2 D3 18 multiple value variables ~ one categorical variable, use cbind() aggregate (data = my.df, cbind(value1,value2) ~ grp, FUN = length) ## grp value1 value2 ## 1 G1 48 48 ## 2 G2 52 52 multiple value variables ~ multiple categorical variable aggregate (data = my.df, cbind(value1,value2) ~ grp + dept, FUN = length) ## grp dept value1 value2 ## 1 G1 D1 17 17 ## 2 G2 D1 14 14 ## 3 G1 D2 16 16 ## 4 G2 D2 20 20 ## 5 G1 D3 15 15 ## 6 G2 D3 18 18 ALL value variables ~ multiple categorical variable, use dot notation Change from FUN=length to sum results in error because sum() cannot be applied to non-numerical variable ‘team’ aggregate (data = my.df, . ~ grp + dept, FUN = length) ## grp dept com team value1 value2 value3 ## 1 G1 D1 17 17 17 17 17 ## 2 G2 D1 14 14 14 14 14 ## 3 G1 D2 16 16 16 16 16 ## 4 G2 D2 20 20 20 20 20 ## 5 G1 D3 15 15 15 15 15 ## 6 G2 D3 18 18 18 18 18 3.3.1.2 Advance Syntax (by method) - data source is either list or data.frame The advantage of ’by method’ are: Can use list/data.frame subset method to choose column to display, hence flexible Can customize output column names (list subset method only) Flexibility to use multiple data sources, hence ‘data’ is not used and has no effect if specified Using list subseting: column name is not preserved, hence must specify meaningful column names. If not supplied, generic names and undesirable column names derived from data value will be used as column name aggregate (x = list(…value_variables…), by = list(…categorical_variables…), FUN = function) aggregate (x = list( v1_mean = my.df$value1, my.df$value2 ), by = list( my.df$grp, DEPT = my.df$dept), FUN=mean) ## Group.1 DEPT v1_mean ## 1 G1 D1 49.19603 ## 2 G2 D1 50.23397 ## 3 G1 D2 50.64011 ## 4 G2 D2 52.20157 ## 5 G1 D3 51.81631 ## 6 G2 D3 50.34312 ## c.18.2601290329418..17.1401638941593..19.4617142391631..23.0294246448329.. ## 1 19.74277 ## 2 20.99601 ## 3 19.87320 ## 4 19.65301 ## 5 19.13836 ## 6 20.54948 Using data.frame subseting: column names are preserved and no option to change. Notice attempt below to change the column name does not succeed aggregate( x = df[,c(…)], by = df[,c(…)]), FUN = function) aggregate( x = df[, p:q], by = df[,s:t]), FUN = function) aggregate(x=my.df[, c(v1_mean=&#39;value1&#39;, &#39;value2&#39;)], by=my.df[,c(GRP=&#39;grp&#39;, &#39;dept&#39;)], FUN=mean) # aggregate(x = my.df[, 4:5], by = my.df[, 1:2], FUN = mean) # produce similar result as above ## grp dept value1 value2 ## 1 G1 D1 49.19603 19.74277 ## 2 G2 D1 50.23397 20.99601 ## 3 G1 D2 50.64011 19.87320 ## 4 G2 D2 52.20157 19.65301 ## 5 G1 D3 51.81631 19.13836 ## 6 G2 D3 50.34312 20.54948 3.3.2 group_by - dplyr Package "],
["3-4-r-markdown.html", "3.4 R Markdown", " 3.4 R Markdown This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com. When you click the Knit button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this: summary(cars) ## speed dist ## Min. : 4.0 Min. : 2.00 ## 1st Qu.:12.0 1st Qu.: 26.00 ## Median :15.0 Median : 36.00 ## Mean :15.4 Mean : 42.98 ## 3rd Qu.:19.0 3rd Qu.: 56.00 ## Max. :25.0 Max. :120.00 "],
["3-5-including-plots.html", "3.5 Including Plots", " 3.5 Including Plots You can also embed plots, for example: Note that the echo = FALSE parameter was added to the code chunk to prevent printing of the R code that generated the plot. -->"],
["4-sort-and-filter-data.html", "Chapter 4 Sort and Filter Data ", " Chapter 4 Sort and Filter Data "],
["4-1-sorting-data.html", "4.1 Sorting Data", " 4.1 Sorting Data "],
["4-2-filtering-data.html", "4.2 Filtering Data", " 4.2 Filtering Data ## global chunk option ## knitr::opts_chunk$set(echo=TRUE, message=FALSE, fig.width=2.6736, fig.height=2.5, fig.show=&#39;hold&#39;) -->"],
["5-graphic-visualization.html", "Chapter 5 Graphic Visualization", " Chapter 5 Graphic Visualization This chapter compares various method to plotting using base-R and ggplot. "],
["5-1-library-used.html", "5.1 Library used", " 5.1 Library used Loading necessary library as below: Base R library already included functions: ** hist, plot, barplot, boxplot** library(ggplot2) ## ggplot, qplot "],
["5-2-sample-data-1.html", "5.2 Sample Data", " 5.2 Sample Data This chapter uses the sample data generate with below code. The idea is to simulate two categorical-alike feature, and two numeric value feature: dept is random character between ‘D1’, ‘D2’, ‘D3’, ‘D4’ and ‘D5’ grp is random character with randomly generated ‘G1’, ‘G2’ value1 represents numeric value, normally distributed at mean 50 value2 is numeric value, normally distributed at mean 25 set.seed(1234) my.df = data.frame( dept = paste(&#39;D&#39;,sample(1:5, 100, replace = T),sep=&#39;&#39;), grp = paste(&#39;G&#39;,sample(1:2, 100, replace = T),sep=&#39;&#39;), value1 = rnorm(1:100, mean = 50, sd = 5), value2 = rnorm(1:100, mean = 20, sd = 3), stringsAsFactors = F ) head(my.df) ## dept grp value1 value2 ## 1 D1 G1 52.07262 21.45568 ## 2 D4 G2 47.62641 22.09031 ## 3 D4 G1 50.32997 20.55654 ## 4 D4 G1 47.48761 22.10220 ## 5 D5 G1 45.87001 20.93504 ## 6 D4 G1 50.83495 22.28139 "],
["5-3-histogram.html", "5.3 Histogram", " 5.3 Histogram 5.3.1 Single Dimension Data Require x as numerical data In hist, binwidth setting is not available, only breaks (number of bins) can be specified Default hist/ggplot/qplot number of bins is 30 In qplot, single x numerical variable default to histogram You can’t specify both bins/breaks and bindwidth together, as it implies each other par(mfrow=c(1,2)) hist (my.df$value1) # default breaks = 30 hist (my.df$value1, breaks=3) qplot (data = my.df, x=value1) qplot (data = my.df, x=value1, geom=&#39;histogram&#39;) qplot (data = my.df, x=value1, bins=15) ggplot(data = my.df, aes(x=value1)) + geom_histogram() # default bins = 30 ggplot(data = my.df, aes(x=value1)) + geom_histogram(bins = 15) ggplot(data = my.df, aes(x=value1)) + geom_histogram(binwidth = 10) 5.3.2 Two Dimension Data x = numerical data fill = categorica-alike data qplot (data = my.df, x=value1, fill=grp, geom=&#39;histogram&#39;) ggplot(data = my.df, aes(x=value1, fill=grp)) + geom_histogram() "],
["5-4-scatter-plot.html", "5.4 Scatter Plot", " 5.4 Scatter Plot 5.4.1 Two Dimension Data Use scatter plot to represent correlation between two numeric variables x = number, y = number qplot default to geom_point when two numerical value is supplied for x and y plot (my.df$value1, my.df$value2) qplot (data = my.df, x = value1, y = value2) qplot (data = my.df, x = value1, y = value2, geom=&#39;point&#39;) ggplot(data = my.df, aes(x=value1, y=value2)) + geom_point() 5.4.2 Two + One Dimension Data Base-R does not support extra dimension visualization In qplot/ggplot, the third dimension of data can be represented by assigning color parameter to the third variable Note that fill has not effect on scatter plot. fill should only be used for bar like chart eg. geom_hist or gem_bar plot (my.df$value1, my.df$value2) qplot (data = my.df, x = value1, y = value2, color = grp, geom=&#39;point&#39;) ggplot(data = my.df, aes(x=value1, y=value2, color = grp)) + geom_point() ggplot(data = my.df, aes(x=value1, y=value2, fill = grp)) + geom_point() "],
["5-5-bar-chart.html", "5.5 Bar Chart", " 5.5 Bar Chart 5.5.1 Single Dimension Data Use bar to repfresent frequency chart plot requre a factor to plot frequency chart barplot require conversion of vector into table for plotting qplot default to geom_bar when single categorical-alike feature is used par(mfrow=c(1,2)) plot(as.factor(my.df$dept)) barplot(table(my.df$dept)) qplot (data = my.df, x=dept) qplot (data = my.df, x=dept, geom=&#39;bar&#39;) ggplot(data = my.df, aes(x=dept)) + geom_bar() 5.5.2 Two + One Dimension Data Use fill to introduce extra variable visualizion in filling the bar Use color to have the extra variable represented with border color qplot (data = my.df, dept, fill = grp) qplot (data = my.df, x = dept, fill = grp, geom=&#39;bar&#39;) ggplot(data = my.df, aes(x = dept, fill = grp)) + geom_bar() ggplot(data = my.df, aes(x = dept, color= grp)) + geom_bar() 5.5.3 Reordering qplot (data = my.df, x=dept) qplot (data = my.df, x=dept, geom=&#39;bar&#39;) ggplot(data = my.df, aes(x=dept)) + geom_bar() 5.5.4 Positioning qplot does not support positioning For ggplot/qplot, default position is stack position = ‘dodge’ similar to position = position_dodge(), however the later is more flexible with ability to adjust overlapping level between sub-bar (default is 0.9) g = ggplot(data = my.df, aes(x=dept, fill=grp)) g + geom_bar(position=&#39;stack&#39;) # default position g + geom_bar(position=&#39;dodge&#39;) g + geom_bar(position=position_dodge()) # default 0.9 g + geom_bar(position=position_dodge(0.5)) g + geom_bar(position=position_dodge(1.0)) 5.5.5 In-Bar Text Labeling "],
["5-6-box-plot.html", "5.6 Box Plot", " 5.6 Box Plot 5.6.1 One Dimension Data In boxplot(), only single variable need to be supplied In ggplot/qplot, variable x and y is required. Variable y is the actual value, variable x is the group variable. Case of one dimension, use x=’’ when no grouping is desired boxplot(my.df$value1) qplot (data = my.df, x = &#39;&#39; , y = value1, geom=&#39;boxplot&#39;) ggplot (data = my.df, aes( x= &#39;&#39;, y=value1 )) + geom_boxplot() 5.6.2 Two Dimension Data In boxplot, use ~ to specify y~x, where x is grouping variable boxplot(data = my.df, value1~grp) qplot (data = my.df, x = grp , y = value1, geom=&#39;boxplot&#39;) ggplot (data = my.df, aes(x=grp, y=value1)) + geom_boxplot() 5.6.3 Two + One Dimension Data Extra dimension can be included in for x-axis In boxplot, use + to specify extra dimension In qplot/ggplot, use interaction to specify extra dimension boxplot(data = my.df, value1~grp+dept) qplot (data = my.df, x=interaction(grp,dept) , y=value1, geom=&#39;boxplot&#39;) ggplot (data = my.df, aes(x=interaction(grp,dept) , y=value1)) + geom_boxplot() "],
["5-7-pie-chart.html", "5.7 Pie Chart", " 5.7 Pie Chart -->"],
["6-statistics.html", "Chapter 6 Statistics ", " Chapter 6 Statistics "],
["6-1-sample-data-2.html", "6.1 Sample Data", " 6.1 Sample Data This chapter uses the sample data generate with below code. The idea is to simulate two categorical-alike feature, and two numeric value feature: dept is random character between ‘D1’, ‘D2’, ‘D3’, ‘D4’ and ‘D5’ grp is random character with randomly generated ‘G1’, ‘G2’ value1 represents numeric value, normally distributed at mean 50 value2 is numeric value, normally distributed at mean 25 is.matrix(state.x77) str(state.x77) dimnames(state.x77)[1] dimnames(state.x77)[2] head(state.x77) ## [1] TRUE ## num [1:50, 1:8] 3615 365 2212 2110 21198 ... ## - attr(*, &quot;dimnames&quot;)=List of 2 ## ..$ : chr [1:50] &quot;Alabama&quot; &quot;Alaska&quot; &quot;Arizona&quot; &quot;Arkansas&quot; ... ## ..$ : chr [1:8] &quot;Population&quot; &quot;Income&quot; &quot;Illiteracy&quot; &quot;Life Exp&quot; ... ## [[1]] ## [1] &quot;Alabama&quot; &quot;Alaska&quot; &quot;Arizona&quot; &quot;Arkansas&quot; &quot;California&quot; &quot;Colorado&quot; ## [7] &quot;Connecticut&quot; &quot;Delaware&quot; &quot;Florida&quot; &quot;Georgia&quot; &quot;Hawaii&quot; &quot;Idaho&quot; ## [13] &quot;Illinois&quot; &quot;Indiana&quot; &quot;Iowa&quot; &quot;Kansas&quot; &quot;Kentucky&quot; &quot;Louisiana&quot; ## [19] &quot;Maine&quot; &quot;Maryland&quot; &quot;Massachusetts&quot; &quot;Michigan&quot; &quot;Minnesota&quot; &quot;Mississippi&quot; ## [25] &quot;Missouri&quot; &quot;Montana&quot; &quot;Nebraska&quot; &quot;Nevada&quot; &quot;New Hampshire&quot; &quot;New Jersey&quot; ## [31] &quot;New Mexico&quot; &quot;New York&quot; &quot;North Carolina&quot; &quot;North Dakota&quot; &quot;Ohio&quot; &quot;Oklahoma&quot; ## [37] &quot;Oregon&quot; &quot;Pennsylvania&quot; &quot;Rhode Island&quot; &quot;South Carolina&quot; &quot;South Dakota&quot; &quot;Tennessee&quot; ## [43] &quot;Texas&quot; &quot;Utah&quot; &quot;Vermont&quot; &quot;Virginia&quot; &quot;Washington&quot; &quot;West Virginia&quot; ## [49] &quot;Wisconsin&quot; &quot;Wyoming&quot; ## ## [[1]] ## [1] &quot;Population&quot; &quot;Income&quot; &quot;Illiteracy&quot; &quot;Life Exp&quot; &quot;Murder&quot; &quot;HS Grad&quot; &quot;Frost&quot; &quot;Area&quot; ## ## Population Income Illiteracy Life Exp Murder HS Grad Frost Area ## Alabama 3615 3624 2.1 69.05 15.1 41.3 20 50708 ## Alaska 365 6315 1.5 69.31 11.3 66.7 152 566432 ## Arizona 2212 4530 1.8 70.55 7.8 58.1 15 113417 ## Arkansas 2110 3378 1.9 70.66 10.1 39.9 65 51945 ## California 21198 5114 1.1 71.71 10.3 62.6 20 156361 ## Colorado 2541 4884 0.7 72.06 6.8 63.9 166 103766 set.seed(1234) my.df = data.frame( dept = paste(&#39;D&#39;,sample(1:5, 100, replace = T),sep=&#39;&#39;), grp = paste(&#39;G&#39;,sample(1:2, 100, replace = T),sep=&#39;&#39;), value1 = rnorm(1:100, mean = 50, sd = 5), value2 = rnorm(1:100, mean = 20, sd = 3), stringsAsFactors = T ) head(my.df) ## dept grp value1 value2 ## 1 D1 G1 52.07262 21.45568 ## 2 D4 G2 47.62641 22.09031 ## 3 D4 G1 50.32997 20.55654 ## 4 D4 G1 47.48761 22.10220 ## 5 D5 G1 45.87001 20.93504 ## 6 D4 G1 50.83495 22.28139 "],
["6-2-descriptive-summary.html", "6.2 Descriptive Summary", " 6.2 Descriptive Summary 6.2.1 Single Vector summary provides min, max, quantiles, mean for numerical vector. But it doesn’t provide standard deviation. Other functions below take single vector as input, and output a single value summary(my.df$value1) mean (my.df$value1) max (my.df$value1) median (my.df$value1) sd (my.df$value1) # standard deviation var (my.df$value1) # variance length (my.df$value1) # number of elements ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 35.72 47.20 50.16 50.21 53.14 65.22 ## [1] 50.20622 ## [1] 65.21883 ## [1] 50.16402 ## [1] 5.160937 ## [1] 26.63527 ## [1] 100 6.2.2 Multiple Vectors summary can be used for multiple columns in a data frame, which each columns is evaluated For factor data, summary provides frequency count For individual functions (mean, max, min, sd, var) that take only single vecotr and output single value, use sapply to provide calculation for multiple columns of a dataframe summary (my.df) sapply (my.df[,3:4], min) sapply (my.df[,3:4], max) sapply (my.df[,3:4], median) sapply (my.df[,3:4], sd) sapply (my.df[,3:4], var) ## dept grp value1 value2 ## D1:25 G1:48 Min. :35.72 Min. :10.30 ## D2:26 G2:52 1st Qu.:47.20 1st Qu.:18.87 ## D3:17 Median :50.16 Median :20.83 ## D4:17 Mean :50.21 Mean :20.46 ## D5:15 3rd Qu.:53.14 3rd Qu.:22.05 ## Max. :65.22 Max. :28.76 ## value1 value2 ## 35.72121 10.30054 ## value1 value2 ## 65.21883 28.75742 ## value1 value2 ## 50.16402 20.83363 ## value1 value2 ## 5.160937 2.880463 ## value1 value2 ## 26.635268 8.297068 6.2.3 Custom Function Custom function can be built to accept single vector and return single vector Use sapply with the custom function to sweep through multiple columns in a dataframe and return a matrix (with row and col names) as a result mystats = function(x, na.omit=FALSE){ if (na.omit) x =x[!is.na(x)] m = mean(x) med = median(x) v = var(x) s = sd(x) n = length(x) skew = sum((x-m)^3/s^3)/n kurt = sum((x-m)^4/s^4)/n - 3 return(c(length=n, mean=m, median=med, stdev=s, skew=skew, kurtosis=kurt)) } sapply(my.df[,3:4], mystats) ## value1 value2 ## length 100.00000000 100.0000000 ## mean 50.20621590 20.4638110 ## median 50.16401641 20.8336304 ## stdev 5.16093675 2.8804631 ## skew -0.01832526 -0.2217617 ## kurtosis 0.52800386 1.0046773 "],
["6-3-t-test.html", "6.3 T-Test", " 6.3 T-Test "],
["6-4-covariance-correlation.html", "6.4 Covariance / Correlation", " 6.4 Covariance / Correlation If two variables are independent, their covariance/correlation is 0. But, having a covariance/correlation of 0 does not imply the variables are independent. 6.4.1 Covariance \\[Pearson - Cov(X,Y)= \\frac{\\sum_{i=1}^n (X_i-\\bar{X})*(Y_i-\\bar{Y})}{n-1}\\] Covariance doesn’t really tell you about the strength of the relationship between the two variables. - A large covariance can simply means the variables are made of large numbers, doesn’t means that the relation are strong. Hence correlation (scaled covariance) is a better indicator of the relation strenght. 6.4.2 Correlation \\[Pearson-Cor(X,Y)= \\frac{Cov(X,Y)}{sd(X)sd(Y)}\\] Correlation is a scaled version of covariance that takes on values between -1 and 1. 0 indicates no correlation. +1 and -1 indicates perfect correlation. Correlation are used to measure the strength of relationship among linearly related quntitative variables (numerical). cor(x, y, use= , method= ) x= matrix or dataframe y= matrix or dataframe, default = x method= pearson, spearman, kendall, default is pearson use= everthing:missing value will set to missing, complete.obs:listwise deletion, pairwise.complete.obs:pairwise deletion If y is not specified, you get cross matrices by default (all variables crossed with all other variables). states = state.x77[,1:5] cor(states) cor(states, method = &#39;kendall&#39;) ## Population Income Illiteracy Life Exp Murder ## Population 1.00000000 0.2082276 0.1076224 -0.06805195 0.3436428 ## Income 0.20822756 1.0000000 -0.4370752 0.34025534 -0.2300776 ## Illiteracy 0.10762237 -0.4370752 1.0000000 -0.58847793 0.7029752 ## Life Exp -0.06805195 0.3402553 -0.5884779 1.00000000 -0.7808458 ## Murder 0.34364275 -0.2300776 0.7029752 -0.78084575 1.0000000 ## Population Income Illiteracy Life Exp Murder ## Population 1.00000000 0.08408163 0.2123063 -0.06865555 0.2364983 ## Income 0.08408163 1.00000000 -0.1970811 0.21904389 -0.1448450 ## Illiteracy 0.21230629 -0.19708113 1.0000000 -0.42852098 0.5155359 ## Life Exp -0.06865555 0.21904389 -0.4285210 1.00000000 -0.5997547 ## Murder 0.23649826 -0.14484495 0.5155359 -0.59975465 1.0000000 If x and y are specified, you can produce non squared correlation matrices with only the variables specified for both x and y axis cor(states[,1:5], states[,3:5]) ## Illiteracy Life Exp Murder ## Population 0.1076224 -0.06805195 0.3436428 ## Income -0.4370752 0.34025534 -0.2300776 ## Illiteracy 1.0000000 -0.58847793 0.7029752 ## Life Exp -0.5884779 1.00000000 -0.7808458 ## Murder 0.7029752 -0.78084575 1.0000000 6.4.3 Correlation Test for significance From the cor function, we know that Murder rate and Illiteracy are highly correlated (&gt;0.7). However, is this merely by chance, or it is statistically significant that there are indeedc correlated ? To answer this question, we need to perform hypotesis testing: \\(H_0\\) : (population) correlation betwen Murder rate and Illiteracy = zero \\(H_1\\) : (sample) correlation between Murder rate and Illiteracy &lt;&gt; zero We then test our sample data using cor.test to find out the p-value. If the p-value &lt; 0.025 (two sided), then we reject the null hypothesis. cor.test(x, y, method = , alternative= , conf.level= ) x= vector 1 y= vector 2 method= pearson (default), spearman, kendall alternative= two.sided (default), less, more conf.level = 0.95(default), any value between 0 to 1 cor.test (states[,&#39;Murder&#39;], states[,&#39;Illiteracy&#39;]) ## ## Pearson&#39;s product-moment correlation ## ## data: states[, &quot;Murder&quot;] and states[, &quot;Illiteracy&quot;] ## t = 6.8479, df = 48, p-value = 1.258e-08 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.5279280 0.8207295 ## sample estimates: ## cor ## 0.7029752 If \\(H_0\\) is true, then chance of observing the sample data (with correlation of 0.7) is 1.258e-08 (too low to be true). Hence we reject \\(H_0\\), and accept \\(H_1\\) that there is indeed a significant correlation between the variables. -->"],
["7-clustering-analysis.html", "Chapter 7 Clustering Analysis", " Chapter 7 Clustering Analysis Cluster analysis is a data-reduction technique designed to uncover subgroups of observations within a dataset. It reduce a large number of observations to a much smaller number of clusters or types. A cluster is defined as a group of observations that are more similar to each other than they are to the observations in other groups. "],
["7-1-application.html", "7.1 Application", " 7.1 Application Business : researchers use cluster analysis for customer segmentation. Customers are arranged into clusters based on the similarity of their demographics and buying behaviours. Marketing campaings are then tailored to appeal to the groups. Psychological: researchers cluster data on the symptoms and demographics of depressed patients, seeking to uncover subtypes of depression, with the hope of finding more effective targeted treatments and a better understanding of the disorder. Medical: researchers use cluster analysis to help catalog gene-expression patterns obtained from DNA microarray data. This can help them to understand normal growth and development and the underlying causes of many human diseases Information Retrieval: The world wide web consists of billions of Web pages, and the results of a query to a search engine can return thousands of pages. Clustering can be used to group these search results into a small number of clusters, each of which captures a particular aspect of the query. For example, a query of “movie” might return Web pages grouped into categories such as reviews, trailers, starts and theaters. Each category (Cluster) can be bnorken into subcategories (sub-clusters), producing a hierachical structure that further assists a user’s exploration of the query results. "],
["7-2-general-steps.html", "7.2 General Steps", " 7.2 General Steps Choose appropriate attributes This is the most important steps. Choose attributes that that actions can be taken upon A sohisticated cluster analysis can’t compensate for a poor choice of variables Scale Data When NOT to scale If you have attributes with a well-defined meaning. Say, latitude and longitude, then you should not scale your data, because this will cause distortion. When To Scale If you have mixed numerica0l data, where each attribute is something entirely different (say, shoe size and weight), has different units attached (lb, tons, m, kg …) then these values aren’t really comparable anyway; z-standardizing them is a best-practise to give equal weight to them. If variables vary in range, then the variable with the largest value will have the greatest impact on result. This is undesirable. Therefore data must be scaled so that they can be compared fairly Methods of Scaling Popular scaling methods are: Normalize to mean=0 and sd=1 Divide by Max Minus min, divide by Min-Max range Screen for Outliers Outliers can distort results. Screen to remove them Calculate Distance Popular measure of distance between two data point is Euclidean distance Others are Manhattan, Canberra, Asymmetric Binary, Maximum and Minkowski also available Chosoe a Clustering Alrorithm Try few Clustering Solutions Visualize the result Visualization can help you determine the meaning and usefulness of the cluster solution Hierarchical clustering are usually presented as a dendrogram Partitioning results are typically visualized using a bivariate cluster plot Intepret the Cluster Once a cluster solution has been obtained, you must interpret (and possibly name) the clusters What do the observations in a cluster have in common? How do they differ from the observations in other clusters? This step is typically accomplished by obtaining summary statistics for each variable by cluster For continuous data, the mean or median for each variable within each cluster is calculated. For mixed data (data that contain categorical variables), the summary statistics will also include modes or category distributions Validate Result Validating the cluster solution involves asking the question: “Are these groupings in some sense real, and not a manifestation of unique aspects of this dataset or statistical technique?” If a different cluster method or different sample is employed, would the same clusters be obtained? The fpc, clv, and clValid packages each contain functions for evaluating the stability of a clustering solution "],
["7-3-sample-data-3.html", "7.3 Sample Data", " 7.3 Sample Data Sample data used in this chapter emulate two dimensional data points with three groups with clear grouping when visualize. set.seed(1234) set.seed(1234) my.df = data.frame( id = paste(&#39;ID_&#39;, 1:15, sep = &#39;&#39;), grp = c(rep(&#39;G1&#39;, 5), rep(&#39;G2&#39;, 5), rep(&#39;G3&#39;, 5)), value1 = c( round(rnorm(5, mean = 10, sd = 3)), round(rnorm(5, mean = 10, sd = 3)), round(rnorm(5, mean = 30, sd = 3))), value2 = c( round(rnorm(5, mean = 10, sd = 3)), round(rnorm(5, mean = 20, sd = 3)), round(rnorm(5, mean = 20, sd = 3))), stringsAsFactors = F ) str(my.df) plot(my.df$value1, my.df$value2) ## &#39;data.frame&#39;: 15 obs. of 4 variables: ## $ id : chr &quot;ID_1&quot; &quot;ID_2&quot; &quot;ID_3&quot; &quot;ID_4&quot; ... ## $ grp : chr &quot;G1&quot; &quot;G1&quot; &quot;G1&quot; &quot;G1&quot; ... ## $ value1: num 6 11 13 3 11 12 8 8 8 7 ... ## $ value2: num 10 8 7 7 17 20 19 19 21 18 ... "],
["7-4-distance-algorithm.html", "7.4 Distance Algorithm", " 7.4 Distance Algorithm The choice of an appropriate metric will influence the shape of the clusters, as some elements may be close to one another according to one distance and farther away according to another. For example, in a two dimensional data, the distance between the point (1,1) and the origin (0,0) can be 2 under Manhattan distance, \\(\\sqrt{2}\\) under Euclidean distance, or 1 under Maximum distance. dist is used to measure distance for all numeric elements in dataframe or matrix. Supplying non-numeric columns for dist will incur warning. dist( x, method = ) default method = 'euclidean' \\(\\quad\\) method = 'euclidean', &quot;maximum&quot;, &quot;manhattan&quot;, &quot;canberra&quot;, &quot;binary&quot; or &quot;minkowski&quot; 7.4.1 Euclidean Distance \\[Euclidean-d(p,q) = \\sqrt{\\sum_{i=1}^n (p_i-q_i)^2} \\quad,n = dimension\\] The Euclidean distance is a distance measure between two points or or vectors in a two- or multidimensional (Euclidean) space based on Pythagoras’ theorem The distance is calculated by taking the square root of the sum of the squared pair-wise distances of every dimension Below command measures distance for numeric columns of all data points in my.df, using euclidean algorithmn. d1 = dist( my.df[,3:4]) round (d1,1) ## 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ## 2 5.4 ## 3 7.6 2.2 ## 4 4.2 8.1 10.0 ## 5 8.6 9.0 10.2 12.8 ## 6 11.7 12.0 13.0 15.8 3.2 ## 7 9.2 11.4 13.0 13.0 3.6 4.1 ## 8 9.2 11.4 13.0 13.0 3.6 4.1 0.0 ## 9 11.2 13.3 14.9 14.9 5.0 4.1 2.0 2.0 ## 10 8.1 10.8 12.5 11.7 4.1 5.4 1.4 1.4 3.2 ## 11 23.8 19.7 18.4 27.5 18.0 17.5 21.2 21.2 21.6 22.1 ## 12 24.2 21.3 20.5 28.3 16.8 15.1 19.2 19.2 19.0 20.4 6.3 ## 13 23.1 19.2 18.0 26.9 17.0 16.3 20.1 20.1 20.4 21.0 1.4 5.1 ## 14 26.0 22.5 21.4 30.0 19.2 18.0 22.0 22.0 22.0 23.1 4.1 3.6 3.6 ## 15 27.9 23.8 22.4 31.6 22.0 21.2 25.1 25.1 25.3 26.0 4.1 7.8 5.0 4.2 7.4.2 Manhattan Distance (n dimension) \\[Manhattan - d(p,q) = \\sum_{i=1}^n |p_i-q_i| \\quad,n = dimension\\] The Manhattan distance (sometimes also called Taxicab distance) metric is related to the Euclidean distance But instead of calculating the shortest diagonal path (“beeline”) between two points, it calculates the distance based on gridlines Below command measures distance for numeric columns of all data points in my.df, using manhattan algorithm. d2 = dist( my.df[,3:4], method=&#39;manhattan&#39;) d2 ## 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ## 2 7 ## 3 10 3 ## 4 6 9 10 ## 5 12 9 12 18 ## 6 16 13 14 22 4 ## 7 11 14 17 17 5 5 ## 8 11 14 17 17 5 5 0 ## 9 13 16 19 19 7 5 2 2 ## 10 9 14 17 15 5 7 2 2 4 ## 11 29 26 25 35 19 21 24 24 26 24 ## 12 33 30 29 39 21 17 22 22 20 24 8 ## 13 29 26 25 35 17 19 22 22 24 22 2 6 ## 14 34 31 30 40 22 18 23 23 23 25 5 5 5 ## 15 34 31 30 40 22 24 27 27 29 27 5 11 5 6 "],
["7-5-clustering-algorithm.html", "7.5 Clustering Algorithm", " 7.5 Clustering Algorithm 7.5.1 Hierarchical Clustering 7.5.1.1 Clustering Process This is how Hierarchical Clustering works: 1. Initially, put each data point in its own cluster 2. Calucate the distances between each cluster and all other clusters 3. Combine the two clusters with the smallest distance - This reduce cluster number by one 4. Repeat step (2) and (3) until all clusters have been merged into single cluster 7.5.1.2 Cluster Distance Once distance for all data points has been measured, decide which of the five (5) methods below to measure distance between clusters: Single Linkage: Shortest distance between points belonging to two clusters Complete Linkage (common): Longest distance between points belonging to two clusters Average Linkage (common): Average distance between all points in one cluster with all points the another cluster Centroid: Find the centroid of each cluster and calculate the distance between centroids between both hclust (d, method =) \\(\\quad\\) `method = ‘single’, ‘complete’, ‘average’, ‘centroid’ c1 = hclust (d1) c1 plot(c1) ## ## Call: ## hclust(d = d1) ## ## Cluster method : complete ## Distance : euclidean ## Number of objects: 15 ### K-Mean Clustering -->"],
["8-classification.html", "Chapter 8 Classification ", " Chapter 8 Classification "],
["8-1-introduction.html", "8.1 Introduction", " 8.1 Introduction "],
["8-2-application-1.html", "8.2 Application", " 8.2 Application "],
["8-3-choosing-the-right-algorithm.html", "8.3 Choosing The Right Algorithm", " 8.3 Choosing The Right Algorithm "],
["8-4-logistic-regression.html", "8.4 Logistic Regression", " 8.4 Logistic Regression "],
["8-5-decision-tree-regression.html", "8.5 Decision Tree Regression", " 8.5 Decision Tree Regression -->"],
["9-regression.html", "Chapter 9 Regression ", " Chapter 9 Regression "],
["9-1-introduction-1.html", "9.1 Introduction", " 9.1 Introduction "],
["9-2-application-2.html", "9.2 Application", " 9.2 Application "],
["9-3-choosing-the-right-algorithm-1.html", "9.3 Choosing The Right Algorithm", " 9.3 Choosing The Right Algorithm "],
["9-4-linear-regression.html", "9.4 LInear Regression", " 9.4 LInear Regression -->"]
]
