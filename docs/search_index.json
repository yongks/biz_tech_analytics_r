[
["11-classification.html", "Chapter 11 Classification ", " Chapter 11 Classification "],
["11-1-introduction.html", "11.1 Introduction", " 11.1 Introduction "],
["11-2-application.html", "11.2 Application", " 11.2 Application "],
["11-3-comparing-algorithm.html", "11.3 Comparing Algorithm", " 11.3 Comparing Algorithm "],
["11-4-performance-measurement.html", "11.4 Performance Measurement", " 11.4 Performance Measurement There are many performance measurement used for binary classification. Here are the rules of thumb which one to use: Recall: If you don’t mind getting some inaccurate result, as long as you get as much correct ones Precision: If you demand rate of correctness and willing to reject some correct results F1 Score: For a more balanced measurement, taking into consideraton both recall and precision 11.4.1 Confusion Matrix Confusion Matrix and Performance Measurement 11.4.1.1 Accuracy Accuracy answers the question: From the total samples, how many had been correctly predicted by the model ? \\(Accuracy = \\frac{TP+TN}{TP+TN+FP+FN}\\) This measurement is useful when the both classes are balanced (that is, the number of TP and TN cases are almost balanced) In practice, it seems that the best accuracy is usually achieved when the cutpoint is near the Probability(actual TRUE) Accuracy is completely usessless in highly skewed class. For example, with a disease that only affects 1 in a million people a completely bogus screening test that always reports “negative” will be 99.9999% accurate 11.4.1.2 Recall Recall answers the question: Out of all actual positive samples, how many were correctly predicted by classifiers ? \\(Recall = \\frac{TP}{TP+FN}\\) 11.4.1.3 Precision Precison answers the question: Out of all the samples classifier predicted as positive, what fraction were correct ? \\(Precision = \\frac{TP}{TP+FN}\\) 11.4.1.4 F1 Score F1 score is the harmonic mean of Precision and Recall. Intuitively, F1 Score is the weighted average of Precision and Recall. It takes into account all three measures: TP, FP and FN \\(Precision = 2*\\frac{Recall * Precision}{Recall + Precision}\\) F1 is usually more useful than accuracy, especially if you have an unbalanced class distribution 11.4.1.5 Area Under ROC (AUROC) ROC curve is basically a graph of TPR vs FPR (some refer to as Recall vs (1-Sensitivity), plotted for different thresholds Comparing two different models, the model with higher AUROC is considered to have higher overall Accuracy AUROC (Area Under ROC) measures : AUC of 0.5: means the model is as good as tossing a coin, worthless AUC of 1.0: means for all cutoff points, TPR=1 and FPR=0. Intuitively it means, all samples had been correctly classified into TP (TPR=1) and TN(FPR=0), and there is no FP and FN. Ultiamtely it means Accuracy is 100% AUROC and Thresholds 11.4.2 Cutoff Threshold Optimization 11.4.2.1 Understanding Cutoff Impacts Cutoff threshold direclty influence the value of TP, FP, TN, FN. If cutoff threshold is lowered (lower probability to classify as Postive), the results are: More linient and hence more samples will be classified as Positive More predicted Positives means more TP and FP, hence TPR and FPR increases However, TPR and FPR increases at different rate: If TPR increases faster than FPR -&gt; this is good, as the lowered threshold generated more TP than FP If FPR increases faster then TPR -&gt; this is not good, as the lowered threhsold generated more FP than TP Different threshold produces different performance metrics (Accuracy, Recall, Precision and Specificity and F1-score). As an example, picture below shows how threshold influences the ROC curve. Threshold and ROC The only way to estimate the optimum threshold for each of the performance measurement will be to measure them for a wide range of threshold. 11.4.2.2 Cutoff Impact Visualization Cutoff and Performance is best visualized using the graphs below: Threshold vs Accuracy Threshold vs Recall / TPR Threshold vs Precision ROC Curve (TPR vs FPR) 11.4.3 Run The Code 11.4.3.1 Measurement Function ####################################################################################### #### Function Return Various Performance by Cutoff Threshold ##### Input : fit: glm model object that uses probability classifier ####################################################################################### eval.binclass = function(fit = NULL, cutoff.min = 0, cutoff.max = 1, cutoff.by = 0.1) { ## Calculate Key Measurements For One Cutoff calc.metrics = function(x) { actual = factor(as.logical(fit$model[[1]]), levels = c(TRUE, FALSE)) predicted = factor(fit$fitted.values &gt; x, levels = c(TRUE, FALSE)) ct = table(actual, predicted, useNA = &#39;no&#39;, exclude = c(NA)) #confusion table accuracy = (ct[1] + ct[4]) / (sum(ct)) recall = ct[1] / (ct[1] + ct[3]) precision = ct[1] / (ct[1] + ct[2]) specificity = ct[4] / (ct[2] + ct[4]) data.frame(cutoff = x, accuracy = accuracy, recall = recall, precision = precision, specificity = specificity, fscore = 2 * (precision * recall / (precision + recall)), fpr = 1 - specificity, tpr_fpr = recall / (1 - specificity), tp = ct[1], fp = ct[2], fn=ct[3], tn=ct[4]) } ## Derive cutoff breakpoints, and loop to calculate measures for each breakpoint cutoffs = seq(cutoff.min, cutoff.max, by = cutoff.by) perf.tab = do.call(rbind, lapply(cutoffs, FUN = calc.metrics)) ## Calculate AUC h = (perf.tab$recall[-1] + perf.tab$recall[-length(perf.tab$recall)]) / 2 w = abs(diff(perf.tab$fpr)) auc = sum(h*w) ## Summarize optimal cutoff for max performance on various metrics best.metrics = rbind( best.accuracy = c(max = max(perf.tab$accuracy, na.rm = TRUE), cutoff = perf.tab$cutoff[which.max(perf.tab$accuracy)]), best.precision = c(max = max(perf.tab$precision, na.rm = TRUE), cutoff = perf.tab$cutoff[which.max(perf.tab$precision)]), best.recall = c(max = max(perf.tab$recall, na.rm = TRUE), cutoff = perf.tab$cutoff[which.max(perf.tab$recall)]), best.fscore = c(max = max(perf.tab$fscore, na.rm = TRUE), cutoff = perf.tab$cutoff[which.max(perf.tab$fscore)]) ) ## Plot ROC graphs plot(perf.tab$fpr, perf.tab$recall, type = &#39;b&#39;, main = paste(&#39;ROC Curve, auc=&#39;,auc), xlab=&#39;False Positive Rate, or (1-Specificity)&#39;, ylab=&#39;Recall or True Positive Rate&#39;) grid() abline(0, 1, col = &quot;red&quot;, lty = 2) ## Plot Accuracy graph plot(perf.tab$cutoff, perf.tab$accuracy, type = &#39;b&#39;, main = &#39;Accuracy over Cutoffs&#39;, xlab = &#39;Cutoff Thresholds&#39;, ylab = &#39;Accuracy&#39;) grid() text(best.metrics[&#39;best.accuracy&#39;, &#39;cutoff&#39;], min(perf.tab$accuracy, na.rm = T) + diff(range(perf.tab$accuracy, na.rm = T)) / 10, best.metrics[&#39;best.accuracy&#39;, &#39;cutoff&#39;], col = &quot;red&quot;) abline(v = best.metrics[&#39;best.accuracy&#39;, &#39;cutoff&#39;], col = &quot;red&quot;, lty = 2) abline(h = best.metrics[&#39;best.accuracy&#39;, &#39;max&#39;], col = &quot;red&quot;, lty = 2) ## Plot Precision graph plot(perf.tab$cutoff, perf.tab$precision, type = &#39;b&#39;, main = &#39;Precision over Cutoffs&#39;, xlab = &#39;Cutoff Thresholds&#39;, ylab = &#39;Precision&#39;) grid() text(best.metrics[&#39;best.precision&#39;, &#39;cutoff&#39;], min(perf.tab$precision,na.rm = T)+diff(range(perf.tab$precision, na.rm=T))/10, best.metrics[&#39;best.precision&#39;, &#39;cutoff&#39;], col = &quot;red&quot;) abline(v = best.metrics[&#39;best.precision&#39;, &#39;cutoff&#39;], col = &quot;red&quot;, lty = 2) abline(h = best.metrics[&#39;best.precision&#39;, &#39;max&#39;], col = &quot;red&quot;, lty = 2) ## Plot F1 Scoregraph plot(perf.tab$cutoff, perf.tab$fscore, type = &#39;b&#39;, main = &#39;F1-Score over Cutoffs&#39;, xlab = &#39;Cutoff Thresholds&#39;, ylab = &#39;F1-Score&#39;) grid() text(best.metrics[&#39;best.fscore&#39;, &#39;cutoff&#39;], min(perf.tab$fscore, na.rm = T) + diff(range(perf.tab$fscore, na.rm = T)) / 10, best.metrics[&#39;best.fscore&#39;, &#39;cutoff&#39;], col = &quot;red&quot;) abline(v = best.metrics[&#39;best.fscore&#39;, &#39;cutoff&#39;], col = &quot;red&quot;, lty = 2) abline(h = best.metrics[&#39;best.fscore&#39;, &#39;max&#39;], col = &quot;red&quot;, lty = 2) # return performance table, and optimal values per metric return(list(perf.tab = perf.tab, auroc = auc, best.metrics = best.metrics)) } 11.4.3.2 Train The Model ### Load The Data and Build The Model train.data = read.csv(&#39;./datasets/hr.train.csv&#39;) str(train.data) ## &#39;data.frame&#39;: 12000 obs. of 7 variables: ## $ S : num 0.38 0.8 0.11 0.72 0.37 0.41 0.1 0.92 0.89 0.42 ... ## $ LPE : num 0.53 0.86 0.88 0.87 0.52 0.5 0.77 0.85 1 0.53 ... ## $ NP : int 2 5 7 5 2 2 6 5 5 2 ... ## $ ANH : int 157 262 272 223 159 153 247 259 224 142 ... ## $ TIC : int 3 6 4 5 3 3 4 5 5 3 ... ## $ Newborn: int 0 0 0 0 0 0 0 0 0 0 ... ## $ left : int 1 1 1 1 1 1 1 1 1 1 ... ## Trian The Model Using Logistic Regression my.fit = glm(left ~ ., family = binomial(link = logit), data = train.data) 11.4.3.3 Run the Performance Eval Function List The Performance Table par(mfrow=c(2,2)) r = eval.binclass(my.fit, cutoff.by = 0.1) kable(r$perf.tab) cutoff accuracy recall precision specificity fscore fpr tpr_fpr tp fp fn tn 0.0 0.1666667 1.0000 0.1666667 0.0000 0.2857143 1.0000 1.0000000 2000 10000 0 0 0.1 0.6498333 0.9505 0.3166223 0.5897 0.4750125 0.4103 2.3165976 1901 4103 99 5897 0.2 0.8126667 0.8025 0.4641411 0.8147 0.5881275 0.1853 4.3308149 1605 1853 395 8147 0.3 0.8195000 0.4730 0.4596696 0.8888 0.4662395 0.1112 4.2535971 946 1112 1054 8888 0.4 0.8129167 0.2610 0.4049651 0.9233 0.3174217 0.0767 3.4028683 522 767 1478 9233 0.5 0.8204167 0.1905 0.4154853 0.9464 0.2612273 0.0536 3.5541045 381 536 1619 9464 0.6 0.8105000 0.0385 0.1799065 0.9649 0.0634267 0.0351 1.0968661 77 351 1923 9649 0.7 0.8190833 0.0120 0.1095890 0.9805 0.0216314 0.0195 0.6153846 24 195 1976 9805 0.8 0.8280833 0.0000 0.0000000 0.9937 NaN 0.0063 0.0000000 0 63 2000 9937 0.9 0.8331667 0.0000 0.0000000 0.9998 NaN 0.0002 0.0000000 0 2 2000 9998 1.0 0.8333333 0.0000 NaN 1.0000 NaN 0.0000 NaN 0 0 2000 10000 $auroc is the area under ROC, calculated for each simulated threshold. r$auroc ## [1] 0.8400424 $best.metrics provides optimal threshold that maximize individual metrics, based on the simulated thresholds. r$best.metrics ## max cutoff ## best.accuracy 0.8333333 1.0 ## best.precision 0.4641411 0.2 ## best.recall 1.0000000 0.0 ## best.fscore 0.5881275 0.2 "],
["11-5-logistic-regression.html", "11.5 Logistic Regression", " 11.5 Logistic Regression 11.5.1 The Concept Logistic Regression is a actually a classification algorithm. It is used to predict: Binary outcome (1=Yes/Sucess, 0=No/Failure), given a set of independent variables. Multinomial outcome (more than two categories) - however, reference category for comparison must be specified, otehrwise, must run multiple regressions with different refence categories Logistic Regression as a special case of linear regression where: The outcome variable is categorical Ln of odds as dependent variable Linear regression cannot be used for classification because: Binary data does not have a normal distribution, which is a condition for many types of regressions Predicted values can go beyond 0 and 1, which violates the definition of probability Probabilities are often not linear 11.5.2 Assumptions Since logistic regression is related to linear combination of IVs, it share some common asssumptions regarding IVs and error terms: Dependent variable must be 1/0 type eg. ‘sucess/failure’, ‘male/female’, ‘yes/no’. Must not be ordinal and continous Observations must be independent Like OLS, Linearity between logit with all independent variables Like OLS, NO multicollinearity - if found, create interaction term, or drop one of the IVs Like OLS, error terms are assumed uncorrelated Although logit is a linear relation with independent variables, logistic regression (which use MLE) is different from OLS Linear Regression as below, due to the fact that DV is categorical and not continuuous: Can handle categorical independent variables Does not assume normality of DV and IVs: becauae \\(p\\) follow Bernoulli distribution Does not assume linearity between DV and IVs: because DV is categorical Does not assume homoscedasticity Does not assume normal errors 11.5.3 Equations The goal of logistic regression is to estimate \\(p\\) (the probability of ‘Success’) for a linear combination of the independent variables This is done by ‘linking’ the linear combination of independent variables to Bernoulli probability distribution (with domain from 0 to 1), to predict the probability of success The link function is called logit, which is the natural log of odds ratio. It is a linear function against independent variables: \\(logit(p) = ln(odds) = ln\\bigg(\\frac{p}{1-p}\\bigg) = \\beta_0 + \\beta_1 x_1 + ... + \\beta_n x_n\\) Derive Odd ratio by anti-log. It measures the ‘strength’ of IV in affecting the outcome, p: \\(odds = \\frac{p}{1-p} = e^{\\beta_0 + \\beta_1 x_1 + ... + \\beta_n x_n}\\) \\(p\\) can be further derived as below sigmoid function. \\(p\\) is non-linear against independent varibales : \\(p = \\frac{1}{1+e^{-\\beta_0 + \\beta_1x_1 + ... + \\beta_nx_n}}\\) 11.5.4 Sample Data 11.5.5 Run The Code glm (formula, family=gaussian(link=identity), data) \\(\\quad\\) formula : example y ~ x1 + x2 + x3 \\(\\quad\\) family : binomial, gaussian, poisson, quasi etc $\\quad$link : logit-default for binomial(), identity-default for gaussian, log-default for poisson` 11.5.5.1 Binomial Binomial Example glm (y ~ x1 + x2 + x3 , family=binomial(logit), data=my.df) -->"]
]
