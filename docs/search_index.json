[
["11-classification.html", "Chapter 11 Classification ", " Chapter 11 Classification "],
["11-1-introduction.html", "11.1 Introduction", " 11.1 Introduction The goal of classification prediction is to estimate binary outcome of TRUE or FALSE. 11.1.1 Application 11.1.2 Types of Classification 11.1.2.1 Probability Outcome Logistic regression falls into this category It predicts the probability of binary outcome Researcher need to determine the cutoff threshold to derive binary outcome Cutoff determines variuos performance metrics, hence usefulness of the model 11.1.2.2 Binary Outcome Here are the list of algorithm produce direct binary prediction: 1.Decision Tree 2.Conditional Forest 3.Ensemble Learning 4.Support Vector Machine (SVM) 5.Neural Networks 6.Tree Ensembles - Random Forest - Gradient Boosted Trees 7.Deep Learning 11.1.3 Comparing Algorithm "],
["11-2-library.html", "11.2 Library", " 11.2 Library library(ROCR) "],
["11-3-sample-dataset.html", "11.3 Sample Dataset", " 11.3 Sample Dataset This chapter will use a Human Resource dataset that contain 12,000 training records. ### Load The Data train = read.csv(&#39;./datasets/hr.train.csv&#39;) test = read.csv(&#39;./datasets/hr.test.csv&#39;) 11.3.1 The Variables 11.3.1.1 Dependent Variable left: employee had left the company (binary) 11.3.1.2 Independent Variables S: Satisfied Employee (binary, obtained through survey) NP: Number of Project Currently Handled LPE: Last Project Evaluation ANH: Average Number of Monthly Hours TIC: Time in company (in years) Newborn: Had a new born in last 12 months str(train) ## &#39;data.frame&#39;: 12000 obs. of 7 variables: ## $ S : num 0.38 0.8 0.11 0.72 0.37 0.41 0.1 0.92 0.89 0.42 ... ## $ LPE : num 0.53 0.86 0.88 0.87 0.52 0.5 0.77 0.85 1 0.53 ... ## $ NP : int 2 5 7 5 2 2 6 5 5 2 ... ## $ ANH : int 157 262 272 223 159 153 247 259 224 142 ... ## $ TIC : int 3 6 4 5 3 3 4 5 5 3 ... ## $ Newborn: int 0 0 0 0 0 0 0 0 0 0 ... ## $ left : int 1 1 1 1 1 1 1 1 1 1 ... 11.3.1.3 Explore The Data Check the structure and samples. summary(train) ## S LPE NP ANH ## Min. :0.0900 Min. :0.3600 Min. :2.000 Min. : 96.0 ## 1st Qu.:0.4800 1st Qu.:0.5700 1st Qu.:3.000 1st Qu.:157.0 ## Median :0.6600 Median :0.7200 Median :4.000 Median :199.5 ## Mean :0.6295 Mean :0.7166 Mean :3.802 Mean :200.4 ## 3rd Qu.:0.8200 3rd Qu.:0.8600 3rd Qu.:5.000 3rd Qu.:243.0 ## Max. :1.0000 Max. :1.0000 Max. :7.000 Max. :310.0 ## TIC Newborn left ## Min. :2.000 Min. :0.0000 Min. :0.0000 ## 1st Qu.:2.000 1st Qu.:0.0000 1st Qu.:0.0000 ## Median :3.000 Median :0.0000 Median :0.0000 ## Mean :3.229 Mean :0.1542 Mean :0.1667 ## 3rd Qu.:4.000 3rd Qu.:0.0000 3rd Qu.:0.0000 ## Max. :6.000 Max. :1.0000 Max. :1.0000 head(train) ## S LPE NP ANH TIC Newborn left ## 1 0.38 0.53 2 157 3 0 1 ## 2 0.80 0.86 5 262 6 0 1 ## 3 0.11 0.88 7 272 4 0 1 ## 4 0.72 0.87 5 223 5 0 1 ## 5 0.37 0.52 2 159 3 0 1 ## 6 0.41 0.50 2 153 3 0 1 Check any class bias and baseline based on the bias. table(train$left) ## ## 0 1 ## 10000 2000 prop.table( table(train$left) ) ## ## 0 1 ## 0.8333333 0.1666667 "],
["11-4-performance-measurement.html", "11.4 Performance Measurement", " 11.4 Performance Measurement There are many performance measurement used for binary classification. Here are the rules of thumb which one to use: Recall: If you don’t mind getting some inaccurate result, as long as you get as much correct ones Precision: If you demand rate of correctness and willing to reject some correct results F1 Score: For a more balanced measurement, taking into consideraton both recall and precision 11.4.1 Confusion Matrix Confusion Matrix and Performance Measurement 11.4.1.1 Accuracy Accuracy answers the question: From the total samples, how many had been correctly predicted by the model ? \\(Accuracy = \\frac{TP+TN}{TP+TN+FP+FN}\\) This measurement is useful when the both classes are balanced (that is, the number of TP and TN cases are almost balanced) In practice, it seems that the best accuracy is usually achieved when the cutpoint is near the Probability(actual TRUE) Accuracy is completely usessless in highly skewed class. For example, with a disease that only affects 1 in a million people a completely bogus screening test that always reports “negative” will be 99.9999% accurate 11.4.1.2 Recall Recall answers the question: Out of all actual positive samples, how many were correctly predicted by classifiers ? \\(Recall = \\frac{TP}{TP+FN}\\) 11.4.1.3 Precision Precison answers the question: Out of all the samples classifier predicted as positive, what fraction were correct ? \\(Precision = \\frac{TP}{TP+FP}\\) 11.4.1.4 F1 Score F1 score is the harmonic mean of Precision and Recall. Intuitively, F1 Score is the weighted average of Precision and Recall. It takes into account all three measures: TP, FP and FN \\(Precision = 2*\\frac{Recall * Precision}{Recall + Precision}\\) F1 is usually more useful than accuracy, especially if you have an unbalanced class distribution 11.4.2 Performance related to Logistic Regression 11.4.2.1 Area Under ROC (AUROC) ROC curve is basically a graph of TPR vs FPR (some refer to as Recall vs (1-Sensitivity), plotted for different thresholds Comparing two different models, the model with higher AUROC is considered to have higher overall Accuracy AUROC (Area Under ROC) measures : AUC of 0.5: means the model is as good as tossing a coin, worthless AUC of 1.0: means for all cutoff points, TPR=1 and FPR=0. Intuitively it means, all samples had been correctly classified into TP (TPR=1) and TN(FPR=0), and there is no FP and FN. Ultiamtely it means Accuracy is 100% AUROC and Thresholds 11.4.2.2 Cutoff Threshold Impact Cutoff threshold direclty influence the value of TP, FP, TN, FN. If cutoff threshold is lowered (lower probability to classify as Postive), the results are: More linient and hence more samples will be classified as Positive More predicted Positives means more TP and FP, hence TPR and FPR increases However, TPR and FPR increases at different rate: If TPR increases faster than FPR -&gt; this is good, as the lowered threshold generated more TP than FP If FPR increases faster then TPR -&gt; this is not good, as the lowered threhsold generated more FP than TP The cutoff with highest TPR/FPR value is the optimal means optimum point whereby It is possible to discover the optimum cutoff by finding the cutoff with highest TPR/FPR Different threshold produces different performance metrics (Accuracy, Recall, Precision and Specificity and F1-score). As an example, picture below shows how threshold influences the ROC curve. Threshold and ROC The only way to estimate the optimum threshold for each of the performance measurement will be to measure them for a wide range of threshold. 11.4.2.3 Cutoff Visualization Selecting a cutoff threshold depends on the objectives of the researcher. To help understanding the how cutoff changes the performance metircs, try visualize them in below graph: 1.Threshold vs Accuracy 2.Threshold vs Recall (TPR) 3.Threshold vs TPR/FPR 4.Threshold vs Precision 5.Threshold vs F1 Score 6.ROC Curve (TPR vs FPR) 11.4.3 Model Evaluation 11.4.3.1 Compare With Baseline In an highly unbalanced dataset (eg. patients that is diagnosed with cancer skewed towards negative). Hence, it is essential to make a lazy baseline comparison with simply classifying every records with negative (cutoff at 1) or positive (cutoff at 0). Put in mind of the dataset used for comparison: Use data from training set as model baseline, when concerning training performance Use dta from test set as model baseline, when concerning test performance Model Accuracy is not better than ‘lazy’ baseline ! In highly bias class, in term of accuracy, the model usually unable to outperform the baseline by good marign Hence accuracy is not a good measurement in such case Model is still useful if the research objective is towards other measurement usch as recall, precision Adjust the threshold to achieve optimum better objectives 11.4.3.2 Combination of Variables Model AIC to measure the usefullness. It is like R-square in linear regression. Use it to compare model with different combinations of variables. Since logistic regression is a actual linear regression of ln(odds), multicolinearity rule apply. 11.4.4 Run The Code 11.4.4.1 Library ROCR is a great and fast library for measuring calssification performance. I library(ROCR) 11.4.4.2 Import Data and Train The Model ### Load The Data train = read.csv(&#39;./datasets/hr.train.csv&#39;) test = read.csv(&#39;./datasets/hr.test.csv&#39;) str(train.data) ## &#39;data.frame&#39;: 12000 obs. of 7 variables: ## $ S : num 0.38 0.8 0.11 0.72 0.37 0.41 0.1 0.92 0.89 0.42 ... ## $ LPE : num 0.53 0.86 0.88 0.87 0.52 0.5 0.77 0.85 1 0.53 ... ## $ NP : int 2 5 7 5 2 2 6 5 5 2 ... ## $ ANH : int 157 262 272 223 159 153 247 259 224 142 ... ## $ TIC : int 3 6 4 5 3 3 4 5 5 3 ... ## $ Newborn: int 0 0 0 0 0 0 0 0 0 0 ... ## $ left : int 1 1 1 1 1 1 1 1 1 1 ... ## Trian The Model Using Logistic Regression fit = glm(left ~ ., family = binomial, data = train) pred = predict(fit, type = &#39;response&#39;, newdata = test) 11.4.4.3 Construct the Performance Metric ROCR::predictionis the main function to crate prediction object that contains key data such as thresholds, TP, FP, TN, FP. With these data points, we can plot and calculate multiple performacne and visualization. It takes only two inputs: 1. Score (probability of being POSITIVE) 2. Label (actual POSITVE) rocr.pred = prediction(pred, test$left) rocr.metrics = data.frame( cutoff = rocr.pred@cutoffs[[1]], accuracy = (rocr.pred@tp[[1]] + rocr.pred@tn[[1]]) / (rocr.pred@tp[[1]] + rocr.pred@tn[[1]] + rocr.pred@fp[[1]] + rocr.pred@fn[[1]]), tpr = rocr.pred@tp[[1]] / (rocr.pred@tp[[1]] + rocr.pred@fn[[1]]), fpr = rocr.pred@fp[[1]] / (rocr.pred@fp[[1]] + rocr.pred@tn[[1]]), ppv = rocr.pred@tp[[1]] / (rocr.pred@tp[[1]] + rocr.pred@fp[[1]]) ) rocr.metrics$fscore = 2 * (rocr.metrics$tpr * rocr.metrics$ppv) / (rocr.metrics$tpr + rocr.metrics$ppv) rocr.metrics$tpr_fpr = rocr.metrics$tpr / rocr.metrics$fpr 11.4.4.4 List The Performance Table The code below discover the optimum threshold for few metrices. Threhold for the highest estiamtes is summarized. ## discovery the optimal threshold for various metrics rocr.best = rbind( best.accuracy = c(max = max(rocr.metrics$accuracy, na.rm = TRUE), cutoff = rocr.metrics$cutoff[which.max(rocr.metrics$accuracy)]), best.ppv = c(max = max(rocr.metrics$ppv, na.rm = TRUE), cutoff = rocr.metrics$cutoff[which.max(rocr.metrics$ppv)]), best.recall = c(max = max(rocr.metrics$tpr, na.rm = TRUE), cutoff = rocr.metrics$cutoff[which.max(rocr.metrics$tpr)]), best.fscore = c(max = max(rocr.metrics$fscore, na.rm = TRUE), cutoff = rocr.metrics$cutoff[which.max(rocr.metrics$fscore)]), best.tpr_fpr = c(max = max(rocr.metrics$tpr_fpr, na.rm = TRUE), cutoff = rocr.metrics$cutoff[which.max(rocr.metrics$tpr_fpr)]) ) kable(rocr.best) max cutoff best.accuracy 0.8400000 0.2563290 best.ppv 0.5143781 0.2563290 best.recall 1.0000000 0.0083197 best.fscore 0.6009448 0.2467452 best.tpr_fpr 5.2960770 0.2563290 11.4.4.5 Visualize The Data and Performance Plot TP, Tn, FP and FN These data are available in the ROCR::prediction object. plot (rocr.pred@cutoffs[[1]], rocr.pred@tp[[1]],xlim=c(0,1), ylim=c(0,12000), col=&#39;green&#39;) lines(rocr.pred@cutoffs[[1]], rocr.pred@tn[[1]], xlim = c(0, 1), ylim = c(0, 12000),col=&#39;red&#39;) lines(rocr.pred@cutoffs[[1]], rocr.pred@fp[[1]], xlim = c(0, 1), ylim = c(0, 12000), col=&#39;blue&#39;) lines(rocr.pred@cutoffs[[1]], rocr.pred@fn[[1]], xlim = c(0, 1), ylim = c(0, 12000),col=&#39;orange&#39;) legend(&quot;top&quot;, inset = .05, cex = 1, title = &quot;Legend&quot;, c(&quot;TP&quot;, &quot;TN&quot;, &quot;FP&quot;,&quot;FN&quot;), horiz = TRUE, lty = c(1, 1), lwd = c(2, 2), col = c(&quot;green&quot;, &quot;red&quot;, &quot;blue&quot;,&#39;orange&#39;), bg = &quot;grey96&quot;) Plot ROC Curve We can build a ROCR::performance object, and plot it ! AUC is calculated with ROCR::performance as well. rocr.perf = performance(rocr.pred, measure = &quot;tpr&quot;, x.measure = &quot;fpr&quot;) rocr.auc = as.numeric(performance(rocr.pred, &quot;auc&quot;)@y.values) plot(rocr.perf, lwd = 3, colorize = TRUE, print.cutoffs.at = seq(0, 1, by = 0.1), text.adj = c(-0.2, 1.7), main = &#39;ROC Curve&#39;) mtext(paste(&#39;auc : &#39;, round(rocr.auc, 5))) abline(0, 1, col = &quot;red&quot;, lty = 2) Plot Accuracy rocr.perf = performance(rocr.pred, measure = &quot;acc&quot;) best.x = rocr.perf@x.values[[1]][which.max(rocr.perf@y.values[[1]])] best.y = max(rocr.perf@y.values[[1]]) plot(rocr.perf, main = &#39;Accuracy vs Cutoff&#39;, xlim = c(0, 1), ylim = c(0, 1)) abline(v = best.x, col = &#39;red&#39;, lty = 2) abline(h = best.y, col = &#39;red&#39;, lty = 2) text(best.x + 0.1, 0.05, round(best.x, 5), col = &quot;red&quot;) text(0.05, best.y + 0.05, round(best.y, 5), col = &quot;red&quot;) Plot Precesion rocr.perf = performance(rocr.pred, measure = &quot;prec&quot;) best.x = rocr.perf@x.values[[1]][which.max(rocr.perf@y.values[[1]])] best.y = max(rocr.perf@y.values[[1]][is.finite(rocr.perf@y.values[[1]])]) plot(rocr.perf, main = &#39;Precision vs Cutoff&#39;, xlim = c(0, 1), ylim = c(0, 1)) abline(v = best.x, col = &#39;red&#39;, lty = 2) abline(h = best.y, col = &#39;red&#39;, lty = 2) text(best.x + 0.1, 0.05, round(best.x, 5), col = &quot;red&quot;) text(0.05, best.y + 0.05, round(best.y, 5), col = &quot;red&quot;) Plot Multiple Metrices Into One Graph As we can see, ROCR::performance is good to build measure and plot for single y-axis measurement. It unfortunately does not support plotting multiple y-axis into one graph. Threfore, we shall build the plot using metrices table constructed earlier (variable rocr.metrics). ### set initial margin of the plot par(mar = c(5, 5, 4, 6)) ## plot graph on left hand side scale plot(rocr.metrics$cutoff, rocr.metrics$tpr, axes = FALSE,, ylab = &#39;&#39;, xlab = &#39;&#39;, col = &#39;green&#39;, main = &#39;Multiple Performances vs Cutoff&#39;, ylim = c(0, 1)) lines(rocr.metrics$cutoff, rocr.metrics$fpr, col = &#39;red&#39;, ylim = c(0, 1)) axis(1, pretty(range(rocr.metrics$cutoff[is.finite(rocr.metrics$cutoff)]), 10)) axis(2, ylim = c(0, 1), col = &quot;black&quot;, las = 1) ## las=1 makes horizontal labels lines(rocr.metrics$fpr, rocr.metrics$tpr, lwd = 3, col = &#39;black&#39;, ylim = c(0, 1), xlim = c(0, 1)) lines(rocr.metrics$cutoff, rocr.metrics$accuracy, col = &#39;purple&#39;, ylim = c(0, 1), xlim = c(0, 1)) lines(rocr.metrics$cutoff, rocr.metrics$ppv, col = &#39;cyan&#39;, ylim = c(0, 1), xlim = c(0, 1)) lines(rocr.metrics$cutoff, rocr.metrics$fscore, col = &#39;pink&#39;, ylim = c(0, 1), xlim = c(0, 1)) abline(0, 1, lty = 2, col = &#39;black&#39;) ## plot graph on right hand side scale par(new = TRUE) plot(rocr.metrics$cutoff, rocr.metrics$tpr_fpr, axes = FALSE, ylab = &#39;&#39;, xlab = &#39;&#39;, col = &#39;orange&#39;) axis(4, col = &quot;orange&quot;, las = 1) ## las=1 makes horizontal labels best = rocr.metrics$cutoff[which.max(rocr.metrics$tpr_fpr[is.finite(rocr.metrics$tpr_fpr)])] abline(v = best, col = &#39;red&#39;, lty = 2) text(best + 0.075, 0.05, round(best, 5), col = &quot;red&quot;) ## axis labels mtext(&quot;cutoff&quot;, side = 1, col = &quot;black&quot;, line = 2.5) mtext(&quot;tpr, fpr, accuracy, precision, f1-scaore&quot;, side = 2, col = &quot;black&quot;, line = 2.5) mtext(&quot;tpr/fpr&quot;, side = 4, col = &quot;black&quot;, line = 2.5) ## legend legend(&quot;topright&quot;, inset = .05, cex = 1, title = &quot;Legend&quot;, c(&quot;TPR&quot;, &quot;FPR&quot;, &quot;TPR/FPR&quot;, &quot;ROC&quot;, &quot;Accuracy&quot;, &quot;Precision&quot;, &quot;F1-Score&quot;), horiz = FALSE, lty = c(1, 1), lwd = c(2, 2), col = c(&quot;green&quot;, &quot;red&quot;, &quot;orange&quot;, &#39;black&#39;, &#39;purple&#39;, &#39;cyan&#39;, &#39;pink&#39;), bg = &quot;grey96&quot;) "],
["11-5-logistic-regression.html", "11.5 Logistic Regression", " 11.5 Logistic Regression 11.5.1 The Concept Logistic Regression is a actually a classification algorithm. It is used to predict: Binary outcome (1=Yes/Sucess, 0=No/Failure), given a set of independent variables. Multinomial outcome (more than two categories) - however, reference category for comparison must be specified, otehrwise, must run multiple regressions with different refence categories Logistic Regression as a special case of linear regression where: The outcome variable is categorical Ln of odds as dependent variable Linear regression cannot be used for classification because: Binary data does not have a normal distribution, which is a condition for many types of regressions Predicted values can go beyond 0 and 1, which violates the definition of probability Probabilities are often not linear 11.5.2 Assumptions Since logistic regression is related to linear combination of IVs, it share some common asssumptions regarding IVs and error terms: Dependent variable must be 1/0 type eg. ‘sucess/failure’, ‘male/female’, ‘yes/no’. Must not be ordinal and continous Observations must be independent Like OLS, Linearity between logit with all independent variables Like OLS, NO multicollinearity - if found, create interaction term, or drop one of the IVs Like OLS, error terms are assumed uncorrelated Although logit is a linear relation with independent variables, logistic regression (which use MLE) is different from OLS Linear Regression as below, due to the fact that DV is categorical and not continuuous: Can handle categorical independent variables Does not assume normality of DV and IVs: becauae \\(p\\) follow Bernoulli distribution Does not assume linearity between DV and IVs: because DV is categorical Does not assume homoscedasticity Does not assume normal errors 11.5.3 Equations The goal of logistic regression is to estimate \\(p\\) (the probability of ‘Success’) for a linear combination of the independent variables This is done by ‘linking’ the linear combination of independent variables to Bernoulli probability distribution (with domain from 0 to 1), to predict the probability of success The link function is called logit, which is the natural log of odds ratio. It is a linear function against independent variables: \\(logit(p) = ln(odds) = ln\\bigg(\\frac{p}{1-p}\\bigg) = \\beta_0 + \\beta_1 x_1 + ... + \\beta_n x_n\\) Derive Odd ratio by anti-log. It measures the ‘strength’ of IV in affecting the outcome, p: \\(odds = \\frac{p}{1-p} = e^{\\beta_0 + \\beta_1 x_1 + ... + \\beta_n x_n}\\) \\(p\\) can be further derived as below sigmoid function. \\(p\\) is non-linear against independent varibales : \\(p = \\frac{1}{1+e^{-\\beta_0 + \\beta_1x_1 + ... + \\beta_nx_n}}\\) The logistic graph below shows P(Y=1) vs \\(\\beta_0 + \\beta_1 x_1 + ... + \\beta_n x_n\\). **Intuitively, larger coefficients and independence varibles values increases the chance of being classified as Y=1*8 Compare the graph below to understand why logistic regression is better than linear regression in binary classification \\(\\quad\\) 11.5.4 High School Formula Some of the high scholl maths are useful for calculating the logistic scores. Remember them by heart. What is ls log \\(y = 10^a\\) \\(log(y) = a\\) \\(y = e^a\\) \\(ln(y) = a\\) Ln is log base e \\(ln(a) = log_e(a)\\) Basic properties of log \\(ln(1) = log(1) = 0\\) \\(ln(0) = log(0) = Inf\\) Log of division becomes substration of log \\(ln\\bigg(\\frac{a}{b}\\bigg) = ln(a) - ln(b)\\) Exponential of summation becomes multiplication of exponential \\(e^{a+b} = e^a * e^b\\) 11.5.5 Sample Data 11.5.6 Run The Code glm (formula, family=gaussian(link=identity), data) \\(\\quad\\) formula : example y ~ x1 + x2 + x3 \\(\\quad\\) family : binomial, gaussian, poisson, quasi etc $\\quad$link : logit-default for binomial(), identity-default for gaussian, log-default for poisson` 11.5.6.1 Binomial Binomial Example glm (y ~ x1 + x2 + x3 , family=binomial(logit), data=my.df) "],
["11-6-decision-tree.html", "11.6 Decision Tree", " 11.6 Decision Tree 11.6.1 The Concept 11.6.2 Run The Code "],
["11-7-random-forest.html", "11.7 Random Forest", " 11.7 Random Forest 11.7.1 The Concept 11.7.2 Run The Code -->"]
]
