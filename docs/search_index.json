[
["index.html", "Business and Technical Analytics with R Preface", " Business and Technical Analytics with R Yong Keh Soon 2017-07-03 Preface "],
["1-fundamentals.html", "Chapter 1 R Fundamentals ", " Chapter 1 R Fundamentals "],
["1-1-package-management.html", "1.1 Package Management", " 1.1 Package Management 1.1.1 Package Storage Here is where all the libraries are stored. Can you guess which are the baseR and third party libraries stored ? .libPaths() ## [1] &quot;C:/Users/YKS-NIC/Documents/R/win-library/3.3&quot; ## [2] &quot;C:/Program Files/R/R-3.3.3/library&quot; 1.1.2 Package Listing Use installed.packages() to return a data frame that list all installed packages. head( installed.packages() ) ## Package LibPath ## abind &quot;abind&quot; &quot;C:/Users/YKS-NIC/Documents/R/win-library/3.3&quot; ## assertthat &quot;assertthat&quot; &quot;C:/Users/YKS-NIC/Documents/R/win-library/3.3&quot; ## backports &quot;backports&quot; &quot;C:/Users/YKS-NIC/Documents/R/win-library/3.3&quot; ## base64enc &quot;base64enc&quot; &quot;C:/Users/YKS-NIC/Documents/R/win-library/3.3&quot; ## BH &quot;BH&quot; &quot;C:/Users/YKS-NIC/Documents/R/win-library/3.3&quot; ## bitops &quot;bitops&quot; &quot;C:/Users/YKS-NIC/Documents/R/win-library/3.3&quot; ## Version Priority Depends Imports LinkingTo ## abind &quot;1.4-5&quot; NA &quot;R (&gt;= 1.5.0)&quot; &quot;methods, utils&quot; NA ## assertthat &quot;0.2.0&quot; NA NA &quot;tools&quot; NA ## backports &quot;1.1.0&quot; NA &quot;R (&gt;= 3.0.0)&quot; &quot;utils&quot; NA ## base64enc &quot;0.1-3&quot; NA &quot;R (&gt;= 2.9.0)&quot; NA NA ## BH &quot;1.62.0-1&quot; NA NA NA NA ## bitops &quot;1.0-6&quot; NA NA NA NA ## Suggests Enhances License License_is_FOSS ## abind NA NA &quot;LGPL (&gt;= 2)&quot; NA ## assertthat &quot;testthat&quot; NA &quot;GPL-3&quot; NA ## backports NA NA &quot;GPL-2&quot; NA ## base64enc NA &quot;png&quot; &quot;GPL-2 | GPL-3&quot; NA ## BH NA NA &quot;BSL-1.0&quot; NA ## bitops NA NA &quot;GPL (&gt;= 2)&quot; NA ## License_restricts_use OS_type MD5sum NeedsCompilation Built ## abind NA NA NA &quot;no&quot; &quot;3.3.2&quot; ## assertthat NA NA NA &quot;no&quot; &quot;3.3.3&quot; ## backports NA NA NA &quot;no&quot; &quot;3.3.3&quot; ## base64enc NA NA NA &quot;yes&quot; &quot;3.3.2&quot; ## BH NA NA NA &quot;no&quot; &quot;3.3.2&quot; ## bitops NA NA NA &quot;yes&quot; &quot;3.3.2&quot; TOO MANY COLUMNS !! Below are the column names and its numbering for filtering purpose. colnames( installed.packages() ) ## [1] &quot;Package&quot; &quot;LibPath&quot; ## [3] &quot;Version&quot; &quot;Priority&quot; ## [5] &quot;Depends&quot; &quot;Imports&quot; ## [7] &quot;LinkingTo&quot; &quot;Suggests&quot; ## [9] &quot;Enhances&quot; &quot;License&quot; ## [11] &quot;License_is_FOSS&quot; &quot;License_restricts_use&quot; ## [13] &quot;OS_type&quot; &quot;MD5sum&quot; ## [15] &quot;NeedsCompilation&quot; &quot;Built&quot; Perform column filter based on column names as necessary. Set parameter priority = ‘NA’ to exclude base R packages. head( installed.packages( priority=&#39;NA&#39; ) [,c(1,3)] ) ## Package Version ## abind &quot;abind&quot; &quot;1.4-5&quot; ## assertthat &quot;assertthat&quot; &quot;0.2.0&quot; ## backports &quot;backports&quot; &quot;1.1.0&quot; ## base64enc &quot;base64enc&quot; &quot;0.1-3&quot; ## BH &quot;BH&quot; &quot;1.62.0-1&quot; ## bitops &quot;bitops&quot; &quot;1.0-6&quot; Set parameter priority = ‘high’ will include ONLY base R packages head( installed.packages( priority=&#39;high&#39; ) [,c(3,2)] ) ## Version LibPath ## boot &quot;1.3-19&quot; &quot;C:/Users/YKS-NIC/Documents/R/win-library/3.3&quot; ## cluster &quot;2.0.6&quot; &quot;C:/Users/YKS-NIC/Documents/R/win-library/3.3&quot; ## foreign &quot;0.8-68&quot; &quot;C:/Users/YKS-NIC/Documents/R/win-library/3.3&quot; ## lattice &quot;0.20-35&quot; &quot;C:/Users/YKS-NIC/Documents/R/win-library/3.3&quot; ## MASS &quot;7.3-47&quot; &quot;C:/Users/YKS-NIC/Documents/R/win-library/3.3&quot; ## Matrix &quot;1.2-10&quot; &quot;C:/Users/YKS-NIC/Documents/R/win-library/3.3&quot; 1.1.3 Package Install / Removal install.packages( c(&#39;ggplot&#39;, &#39;ggExtra&#39;) ) remove.packages ( c(&#39;ggplot&#39;, &#39;ggExtra&#39;) ) 1.1.4 Package Update old.packages() ## list all old packages, their old version and available new version update.packages() ## need to input y/n/c for each old package to update update.packages( ask=FALSE ) ## automatically update all packages without asking 1.1.5 Package Corruption Sometimes a corrupted R package can give below issues: Error loading a library Error installing a library Error removing a library The solution is to: Remove the problematic package folder (see where they are stored using .libPaths() ) Reinstall the package "],
["1-2-data-types.html", "1.2 Data Types", " 1.2 Data Types 1.2.1 String 1.2.1.1 String Comparison 1.2.1.2 String Manipulations Splitting Combining Extracting 1.2.2 Dates Manipulation 1.2.2.1 Formatting 1.2.2.2 Dates Comparison 1.2.2.3 Dates Manipulation Days Between First Day of the Month Last Day of the Month Days/Months/Years After "],
["1-3-conditioinal-decision.html", "1.3 Conditioinal Decision", " 1.3 Conditioinal Decision "],
["1-4-loops.html", "1.4 Loops", " 1.4 Loops 1.4.1 Sample Data my.df ## com dept grp team value1 value2 value3 ## 1 C1 D2 G2 T2 47.61404 19.66914 10.134088 ## 2 C2 D1 G2 T1 45.00807 18.46697 9.509314 ## 3 C2 D1 G1 T1 46.11873 17.26641 9.559452 ## 4 C2 D2 G2 T1 50.32229 17.48848 10.459589 ## 5 C2 D2 G1 T1 54.79747 27.24751 9.306280 1.4.2 Loop Through A Vector 1.4.3 Loop Through Multiple Columns/Rows apply loops through all rows or columns, take each column/row as a vector and supply them as input to a function. The function will compute the vector supplied, and return output in one of the form below base on the function used: Return single value per row/column, eg. sum() Return multiple value per row/column, eg. function(x) x^2 At the end of the ‘loop’, all results from each iteration are combined into one vector or matrix as final output. apply (X, MARGIN, FUN, ...) \\(\\quad\\) X : matrix (anything else will be converted to matrix \\(\\quad\\) MARGIN: 1 - row-wise, 2-column-wise \\(\\quad\\) FUN: function to apply, can be a custom function \\(\\quad\\) ... : optional parameters for FUN 1.4.3.1 Row-Wise Function Iterate through each row with parameter MARGIN=1. Each output column represent a data ROW. # single value returned per row apply( my.df[,5:7], 1, sum ) ## [1] 77.41727 72.98435 72.94460 78.27037 91.35126 # multiple values returned per row apply( my.df[,5:7], 1, function (x,y) x^y, y=3 ) ## [,1] [,2] [,3] [,4] [,5] ## value1 107945.614 91174.0205 98091.6484 127432.820 164543.8026 ## value2 7609.504 6297.7735 5147.6194 5348.803 20229.2717 ## value3 1040.768 859.8993 873.5726 1144.311 805.9875 1.4.3.2 Column-Wise Function Iterate through each column with paramneter MARGIN=2. Each output column represent a data COLUMN. # single value returned per column apply( my.df[,5:7], 2, sum) ## value1 value2 value3 ## 243.86060 100.13852 48.96872 # multiple values returned per column apply( my.df[,5:7], 2, function (x,y) x^y, y=3 ) ## value1 value2 value3 ## [1,] 107945.61 7609.504 1040.7683 ## [2,] 91174.02 6297.773 859.8993 ## [3,] 98091.65 5147.619 873.5726 ## [4,] 127432.82 5348.803 1144.3106 ## [5,] 164543.80 20229.272 805.9875 "],
["1-5-data-import.html", "1.5 Data Import", " 1.5 Data Import 1.5.1 Working Directory To display the current working directory. getwd() ## [1] &quot;C:/Users/YKS-NIC/Dropbox/R/business_technical_analytics_r&quot; To set the working directory, use below: setwd(&quot;...new path...) 1.5.2 Importing CSV read.csv is a similar to read.table but with some defaults value set as below for convenience of CSV import. In the resulting data.frame, row.names attribute are automatically assigned with sequence number starting from 1. read.csv ( file, \\(\\quad\\) header = TRUE - contain header row \\(\\quad\\) sep = “,” - column seperator marked as ‘,’ \\(\\quad\\) dec = “.” - decimals marked as ‘.’ \\(\\quad\\) na.strings = “NA” - vectors that define missing data marking \\(\\quad\\) check.names = TRUE - col names with white space replaced with ‘.’ \\(\\quad\\) stringsAsFactors = TRUE - convert string to factor Examine the below example of data import process (Excel–&gt;csv–&gt;R data.frame): 1.5.2.1 Original Excel Data Source Check out the Yellow areas in codes below ! 1.5.2.2 Exported CSV File from Excel ,,dept,gender,weight,height,date_birth,amount,date_last,date first 1,ID101,D1,Male,35,173,1/7/1973,100,2/29/2016,2013-07-31 2,ID102,D2,Female,37.1,164,28/2/1980,121,4/1/2017,2013-08-31 3,ID103,D3,Female,43.12,178,31/12/1978,152,10/31/2015,2014-12-31 4,ID104,D1,Male,38.123,182,12/1/1997,133,11/1/2016,2015-02-28 5,ID105,D1,Male,54.1234,159,2/1/1982,143,9/30/2016,2012-06-15 6,ID106,D3,Female,34.12345,166,26/7/1973,155,11/27/2015,2013-04-28 7,ID107,D2,Male,49.123456,153,21/8/1985,117,3/31/2017,2014-03-01 8,ID108,D1,Male,50.2,159,2/1/1982,143,9/30/2016,2011-06-15 9,ID109,D3,Female,59.1,166,13/7/1975,155,11/1/2017,2012-04-02 10,ID110,D2,Male,63.2,163,24/8/1982,117,3/12/2016,2013-03-12 11,ID111,D3,Female,75.1,170,9/8/1979,135,2/1/2015, 12,ID112,D2,Male,52.1,169,NULL,128,NA, 13,ID113,D3,NULL,88.8,171,NULL,141,NA, 1.5.2.3 Import Into R Data Frame Example below specify multiple string elements that represents missing data in the CSV file. Also turn FALSE for stirngsAsFactors so all string columns are not converted to factor automatically. ./ is a relative path represents current R working directory. It can be replaced with complete non-relative path. sample.df &lt;- read.csv ( file=&quot;./datasets/import_sample.csv&quot;, stringsAsFactors = FALSE, na.strings=c(&#39;NA&#39;,&#39;NULL&#39;,&#39;&#39;), encoding=&quot;UTF-8&quot;) sample.df ## X X.1 dept gender weight height date_birth amount date_last ## 1 1 ID101 D1 Male 35.00000 173 1/7/1973 100 2/29/2016 ## 2 2 ID102 D2 Female 37.10000 164 28/2/1980 121 4/1/2017 ## 3 3 ID103 D3 Female 43.12000 178 31/12/1978 152 10/31/2015 ## 4 4 ID104 D1 Male 38.12300 182 12/1/1997 133 11/1/2016 ## 5 5 ID105 D1 Male 54.12340 159 2/1/1982 143 9/30/2016 ## 6 6 ID106 D3 Female 34.12345 166 26/7/1973 155 11/27/2015 ## 7 7 ID107 D2 Male 49.12346 153 21/8/1985 117 3/31/2017 ## 8 8 ID108 D1 Male 50.20000 159 2/1/1982 143 9/30/2016 ## 9 9 ID109 D3 Female 59.10000 166 13/7/1975 155 11/1/2017 ## 10 10 ID110 D2 Male 63.20000 163 24/8/1982 117 3/12/2016 ## 11 11 ID111 D3 Female 75.10000 170 9/8/1979 135 2/1/2015 ## 12 12 ID112 D2 Male 52.10000 169 &lt;NA&gt; 128 &lt;NA&gt; ## 13 13 ID113 D3 &lt;NA&gt; 88.80000 171 &lt;NA&gt; 141 &lt;NA&gt; ## date.first ## 1 2013-07-31 ## 2 2013-08-31 ## 3 2014-12-31 ## 4 2015-02-28 ## 5 2012-06-15 ## 6 2013-04-28 ## 7 2014-03-01 ## 8 2011-06-15 ## 9 2012-04-02 ## 10 2013-03-12 ## 11 &lt;NA&gt; ## 12 &lt;NA&gt; ## 13 &lt;NA&gt; Parameter check.names=TRUE automatically named ‘unnamed’ column, as well as replacing white spaces for column names with ‘.’. All non-numeric data are imported as chr due to stringsAsFactor=FALSE. str(sample.df) ## &#39;data.frame&#39;: 13 obs. of 10 variables: ## $ X : int 1 2 3 4 5 6 7 8 9 10 ... ## $ X.1 : chr &quot;ID101&quot; &quot;ID102&quot; &quot;ID103&quot; &quot;ID104&quot; ... ## $ dept : chr &quot;D1&quot; &quot;D2&quot; &quot;D3&quot; &quot;D1&quot; ... ## $ gender : chr &quot;Male&quot; &quot;Female&quot; &quot;Female&quot; &quot;Male&quot; ... ## $ weight : num 35 37.1 43.1 38.1 54.1 ... ## $ height : int 173 164 178 182 159 166 153 159 166 163 ... ## $ date_birth: chr &quot;1/7/1973&quot; &quot;28/2/1980&quot; &quot;31/12/1978&quot; &quot;12/1/1997&quot; ... ## $ amount : int 100 121 152 133 143 155 117 143 155 117 ... ## $ date_last : chr &quot;2/29/2016&quot; &quot;4/1/2017&quot; &quot;10/31/2015&quot; &quot;11/1/2016&quot; ... ## $ date.first: chr &quot;2013-07-31&quot; &quot;2013-08-31&quot; &quot;2014-12-31&quot; &quot;2015-02-28&quot; ... -->"],
["2-data-generation.html", "Chapter 2 Data Generation", " Chapter 2 Data Generation Here is a review of existing methods. "],
["2-1-sequential-number.html", "2.1 Sequential Number", " 2.1 Sequential Number 2.1.1 Using : : return vector Produce sequantial integer with fix incremental or decremental by 1 Incremental 3:6 # incremental integer 1.25:9.25 # incremental decimal c(3:6, 4.25:8.25) # combination of multiple sequence ## [1] 3 4 5 6 ## [1] 1.25 2.25 3.25 4.25 5.25 6.25 7.25 8.25 9.25 ## [1] 3.00 4.00 5.00 6.00 4.25 5.25 6.25 7.25 8.25 Decremental 6:3 # decremental integer 9.25: 1.25 # decremental decimal ## [1] 6 5 4 3 ## [1] 9.25 8.25 7.25 6.25 5.25 4.25 3.25 2.25 1.25 2.1.2 Using seq : return vector Improvement from :, seq allows specifying incremental steps with by=. seq( from, to ) seq( from, to, by = ) seq( from, to, length.out = ) # potentially return decimal Incremental seq (3, 12) # default increment by=1 seq (3, 12, by = 4) # increment of integer seq (3.25, 12.25, by = 2.25) # increment of decimal ## [1] 3 4 5 6 7 8 9 10 11 12 ## [1] 3 7 11 ## [1] 3.25 5.50 7.75 10.00 12.25 Decremental - from must be larger than to, and by has to be negative. seq (12, 3) # default decrement by=-1 seq (12, 3, by = -4) # decrement of integer seq (12.25, 3.25, by = -2.25) # decrement of decimal ## [1] 12 11 10 9 8 7 6 5 4 3 ## [1] 12 8 4 ## [1] 12.25 10.00 7.75 5.50 3.25 Equal Spreading - with length.out= Equal Spreading of Integer seq (10, 50, length.out = 9) # incremental spreding of integer seq (50, 10, length.out = 9) # decremental spreading of integer ## [1] 10 15 20 25 30 35 40 45 50 ## [1] 50 45 40 35 30 25 20 15 10 Equal Spreading of Decimal seq (10.33, 50.55, length.out = 9) # incremental spreading of decimal seq (50.55, 10.33, length.out = 9) # decremental spreading of decimal ## [1] 10.3300 15.3575 20.3850 25.4125 30.4400 35.4675 40.4950 45.5225 50.5500 ## [1] 50.5500 45.5225 40.4950 35.4675 30.4400 25.4125 20.3850 15.3575 10.3300 "],
["2-2-random-number.html", "2.2 Random Number", " 2.2 Random Number 2.2.1 Unified Distribution runif( n ) # default min=0, max=1 runif( n, min=, max= ) set.seed(123) runif(5) # geenrate 5 numbers within default min=0, max=1 runif(5, min=3, max=9) ## [1] 0.2875775 0.7883051 0.4089769 0.8830174 0.9404673 ## [1] 3.273339 6.168633 8.354514 6.308610 5.739688 Notice that the numbers generated are uniformly distributed. hist(runif(300, min=3, max=9)) 2.2.2 Normal Distribution rnorm( n ) # default mean=0, sd=1 rnorm( n, mean=, sd= ) set.seed(123) rnorm(5) # geenrate 5 numbers within default min=0, max=1 rnorm(5, mean=3, sd=1.5) ## [1] -0.56047565 -0.23017749 1.55870831 0.07050839 0.12928774 ## [1] 5.572597 3.691374 1.102408 1.969721 2.331507 Notice that the numbers generated are uniformly distributed. hist(rnorm(300, mean=3, sd=1.5)) 2.2.3 Binomial Distribution The code below generates ‘n’ number of observations, each observation is the number of success for a number of trials, and there is a specific probability for success of each trial: The numbers generated has the below characteristic: Discrete number Binomial distribution often well approximated by a Normal distribution, where: mean = n * prob variance = n * prob (1-prob) Theoritically, when n approaches infitity, a binomial is a normal distribution rbinorm( n, size, prob ) rbinorm( n=, size=, prob= ) \\(\\quad\\) n = number of observations \\(\\quad\\) size = number of trials per observations \\(\\quad\\) prob = probability of success for each trial rbinom(100, 10, 0.4) ## [1] 6 6 5 3 1 3 3 6 3 3 5 6 4 5 2 3 7 5 5 4 4 5 6 4 2 4 5 2 3 4 5 6 4 4 5 ## [36] 6 6 2 4 3 6 5 2 5 3 2 6 2 5 3 1 4 2 3 4 4 5 3 4 4 5 5 5 6 7 4 4 3 4 3 ## [71] 1 5 4 2 3 4 2 2 4 3 6 3 1 6 3 4 4 3 3 4 4 4 5 5 4 4 1 0 8 2 hist(rbinom(100, 10, 0.4)) 2.2.4 Beta Distribution Balanced Skewness hist(rbeta(1000, 1, 1)) # no left and right skewness = uniform hist(rbeta(1000, 10, 10)) # balanced left and right skew hist(rbeta(1000, 100, 100)) # more balanced left and right skew Left or Right Skew hist(rbeta(1000, 1000, 10)) # left skewed more hist(rbeta(1000, 100, 1000)) # right skew more 2.2.5 Drawing From A Bag A bag has been occupied with vector x (produced using : or any other vector) sample() will draw from this bag Specifying replace=T emulate that the drwan sample will be put back to the bag for next draw R will generate error if no enough samples to draw from ( size &gt; length(x) ) sample( x=, size= ) sample( x=, size=, replace=T) # recycle number even though it is not exhausted set.seed (123) sample (10, 5) # choose 5 numbers from the &#39;bag&#39; containing 1:10 sample (3:10, 5) # choose 5 numbers from the &#39;bag&#39; containing 3:10 bag = runif(8, min=3, max=9) # define the content of the bag sample (bag, 5, replace=T) # draw from the bag, recycling numbers ## [1] 3 8 4 7 6 ## [1] 3 6 8 5 4 ## [1] 7.065424 3.252357 3.252357 8.398950 8.398950 "],
["2-3-factor.html", "2.3 Factor", " 2.3 Factor gl generates elements based on given levels. However, it is not randomized. gl ( n, k, ordered=FALSE ) gl ( n, k, length = n*k, ordered=FALSE ) \\(quad\\) n = number of levels \\(quad\\) k = number of elements to be created per level \\(quad\\) length = number of elements to be generated gl( 3, 5, 9, c(&#39;Aaa&#39;,&#39;Bbb&#39;,&#39;Ccc&#39;), ordered=TRUE ) ## [1] Aaa Aaa Aaa Aaa Aaa Bbb Bbb Bbb Bbb ## Levels: Aaa &lt; Bbb &lt; Ccc str( gl(3,5, 9, c(&#39;Aaa&#39;,&#39;Bbb&#39;,&#39;Ccc&#39;)) ) ## Factor w/ 3 levels &quot;Aaa&quot;,&quot;Bbb&quot;,&quot;Ccc&quot;: 1 1 1 1 1 2 2 2 2 -->"],
["3-data-simulation.html", "Chapter 3 Data Simulation ", " Chapter 3 Data Simulation "],
["3-1-linear-simulation.html", "3.1 Linear Simulation", " 3.1 Linear Simulation gen_slinear = function(n = 50, start = 0, intercept = 0, coef = 1, mean = 0, sd = 1, visual=TRUE) { par(mfrow = c(2, 2)) if (start == -1) my.df = data.frame(x = rnorm(n)) # x is normally distributed random number else my.df = data.frame(x = start:n) # x from 0 to 50 my.df$f = my.df$x * coef + intercept # y = coef * x my.df$residuals = rnorm(length(my.df$x), mean = mean, sd = sd) my.df$y = my.df$f + my.df$residuals # introduce errors if (visual){ plot(my.df$x, my.df$f, main = paste(&#39;Perfect Fit Line:\\nf = &#39;, coef, &#39;x + &#39;, intercept)) plot(my.df$x, my.df$y, main = &#39;Constant Normal Errors Introduced&#39;) hist(my.df$y, main = &#39;Y Distribution&#39;) hist(my.df$residuals, main = &#39;Residuals Distribution&#39;) } my.df } my.slinear = gen_slinear(n = 250, start=-1, intercept = 30, coef = 2, mean = 0, sd = 10) 3.1.1 Example of Random Normal X my.slinear = gen_slinear(n = 250, start=-1, intercept = 30, coef = 2, mean = 0, sd = 10) 3.1.2 Example of Sequantial X (non-random) my.slinear = gen_slinear(n = 250, start=0, intercept = 30, coef = 2, mean = 0, sd = 10) "],
["3-2-logarithm-simulation.html", "3.2 Logarithm Simulation", " 3.2 Logarithm Simulation gen_log = function(n = 50, start = -1, root = 0, mean = 0, sd = 10) { par(mfrow = c(2, 2)) if (start == -1) my.df = data.frame(x = rnorm(n)) # x is normally distributed random number else my.df = data.frame(x = start:n) # x from 0 to 50 my.df$f = log((my.df$x + root)) # y=log(x+root my.df$residuals = rnorm(length(my.df$x), mean = mean, sd = sd) my.df$y = my.df$f + my.df$residuals # introduce errors plot(my.df$x, my.df$f, main = paste(&#39;Perfect Fit Line:\\nf=log(x+&#39;, root, &#39;)&#39;)) plot(my.df$x, my.df$y, main = &#39;Constant Normal Errors Introduced&#39;) hist(my.df$y, main = &#39;Y Distribution&#39;) hist(my.df$residuals, main = &#39;Residuals Distribution&#39;) my.df } gen_log(n=100, start=1, mean=10, sd=0.2) ## x f residuals y ## 1 1 0.0000000 10.356622 10.35662 ## 2 2 0.6931472 9.715658 10.40880 ## 3 3 1.0986123 10.328437 11.42705 ## 4 4 1.3862944 10.024847 11.41114 ## 5 5 1.6094379 10.000225 11.60966 ## 6 6 1.7917595 9.734205 11.52596 ## 7 7 1.9459101 10.155945 12.10186 ## 8 8 2.0794415 10.258089 12.33753 ## 9 9 2.1972246 9.795886 11.99311 ## 10 10 2.3025851 10.140941 12.44353 ## 11 11 2.3978953 10.265750 12.66365 ## 12 12 2.4849066 10.343330 12.82824 ## 13 13 2.5649494 9.828083 12.39303 ## 14 14 2.6390573 9.797290 12.43635 ## 15 15 2.7080502 9.955951 12.66400 ## 16 16 2.7725887 10.031765 12.80435 ## 17 17 2.8332133 9.826145 12.65936 ## 18 18 2.8903718 9.853872 12.74424 ## 19 19 2.9444390 10.222327 13.16677 ## 20 20 2.9957323 9.966724 12.96246 ## 21 21 3.0445224 10.193539 13.23806 ## 22 22 3.0910425 9.900336 12.99138 ## 23 23 3.1354942 10.185377 13.32087 ## 24 24 3.1780538 10.016229 13.19428 ## 25 25 3.2188758 9.977108 13.19598 ## 26 26 3.2580965 9.627000 12.88510 ## 27 27 3.2958369 9.715609 13.01145 ## 28 28 3.3322045 10.142168 13.47437 ## 29 29 3.3672958 9.741037 13.10833 ## 30 30 3.4011974 9.786793 13.18799 ## 31 31 3.4339872 10.032686 13.46667 ## 32 32 3.4657359 10.183092 13.64883 ## 33 33 3.4965076 9.941247 13.43775 ## 34 34 3.5263605 10.165998 13.69236 ## 35 35 3.5553481 9.748136 13.30348 ## 36 36 3.5835189 10.200634 13.78415 ## 37 37 3.6109179 10.152595 13.76351 ## 38 38 3.6375862 9.893352 13.53094 ## 39 39 3.6635616 10.061284 13.72485 ## 40 40 3.6888795 10.241077 13.92996 ## 41 41 3.7135721 9.799455 13.51303 ## 42 42 3.7376696 9.821551 13.55922 ## 43 43 3.7612001 10.006030 13.76723 ## 44 44 3.7841896 9.915600 13.69979 ## 45 45 3.8066625 9.950515 13.75718 ## 46 46 3.8286414 10.128830 13.95747 ## 47 47 3.8501476 10.043703 13.89385 ## 48 48 3.8712010 10.304981 14.17618 ## 49 49 3.8918203 9.795191 13.68701 ## 50 50 3.9120230 10.103900 14.01592 ## 51 51 3.9318256 10.004948 13.93677 ## 52 52 3.9512437 9.805870 13.75711 ## 53 53 3.9702919 9.660796 13.63109 ## 54 54 3.9889840 10.132564 14.12155 ## 55 55 4.0073332 9.983414 13.99075 ## 56 56 4.0253517 9.741701 13.76705 ## 57 57 4.0430513 10.058553 14.10160 ## 58 58 4.0604430 9.980418 14.04086 ## 59 59 4.0775374 10.319549 14.39709 ## 60 60 4.0943446 9.814869 13.90921 ## 61 61 4.1108739 10.168137 14.27901 ## 62 62 4.1271344 9.871789 13.99892 ## 63 63 4.1431347 9.708217 13.85135 ## 64 64 4.1588831 10.235564 14.39445 ## 65 65 4.1743873 9.544156 13.71854 ## 66 66 4.1896547 10.054442 14.24410 ## 67 67 4.2046926 10.110215 14.31491 ## 68 68 4.2195077 10.080951 14.30046 ## 69 69 4.2341065 9.879233 14.11334 ## 70 70 4.2484952 9.815594 14.06409 ## 71 71 4.2626799 10.095982 14.35866 ## 72 72 4.2766661 10.021176 14.29784 ## 73 73 4.2904594 9.771924 14.06238 ## 74 74 4.3040651 9.805703 14.10977 ## 75 75 4.3174881 9.786070 14.10356 ## 76 76 4.3307333 9.619565 13.95030 ## 77 77 4.3438054 10.210773 14.55458 ## 78 78 4.3567088 9.972054 14.32876 ## 79 79 4.3694479 9.923880 14.29333 ## 80 80 4.3820266 10.295239 14.67727 ## 81 81 4.3944492 10.122145 14.51659 ## 82 82 4.4067192 10.311371 14.71809 ## 83 83 4.4188406 10.455816 14.87466 ## 84 84 4.4308168 9.905639 14.33646 ## 85 85 4.4426513 10.042537 14.48519 ## 86 86 4.4543473 10.533253 14.98760 ## 87 87 4.4659081 10.180419 14.64633 ## 88 88 4.4773368 10.437282 14.91462 ## 89 89 4.4886364 10.119064 14.60770 ## 90 90 4.4998097 10.066552 14.56636 ## 91 91 4.5108595 10.279845 14.79070 ## 92 92 4.5217886 10.212606 14.73439 ## 93 93 4.5325995 9.958137 14.49074 ## 94 94 4.5432948 9.999681 14.54298 ## 95 95 4.5538769 9.919399 14.47328 ## 96 96 4.5643482 10.264941 14.82929 ## 97 97 4.5747110 9.893774 14.46848 ## 98 98 4.5849675 10.033555 14.61852 ## 99 99 4.5951199 9.749212 14.34433 ## 100 100 4.6051702 10.274920 14.88009 "],
["3-3-parabola-simulation.html", "3.3 Parabola Simulation", " 3.3 Parabola Simulation gen_parabola = function(n = 50, start = -1, root1 = -0.5, root2 = 0.5, mean = 0, sd = 1) { par(mfrow = c(2, 2)) if (start == -1) my.df = data.frame(x = rnorm(n)) # x is normally distributed random number else my.df = data.frame(x = start:n) # x from 0 to 50 my.df$f = (my.df$x - root1) * (my.df$x - root2) # y=(x-20)(x-30), root at x=20 and x=30 my.df$residuals = rnorm(length(my.df$x), mean = mean, sd = sd) my.df$y = my.df$f + my.df$residuals # introduce errors plot(my.df$x, my.df$f, main = paste(&#39;Perfect Fit Line:\\nf=(x-&#39;, root1, &#39;)(x-&#39;, root2, &#39;)&#39;)) plot(my.df$x, my.df$y, main = &#39;Constant Normal Errors Introduced&#39;) hist(my.df$y, main = &#39;Y Distribution&#39;) hist(my.df$residuals, main = &#39;Residuals Distribution&#39;) my.df } 3.3.1 Example of Random Normal X my.parabola = gen_parabola(n = 250, start=-1, root1 = 0.25, root2 = .5, sd = 1000) 3.3.2 Example of Sequantial X (non-random) my.parabola = gen_parabola(n = 50, start=0, root1 = 20, root2 = 30, sd = 1000) "],
["3-4-polynomial-simulation.html", "3.4 Polynomial Simulation", " 3.4 Polynomial Simulation \\[ y = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\beta_3 x^3 + ...+ \\beta_k x^k \\] -->"],
["4-data-summarization.html", "Chapter 4 Data Summarization ", " Chapter 4 Data Summarization "],
["4-1-library.html", "4.1 Library", " 4.1 Library "],
["4-2-sample-data-1.html", "4.2 Sample Data", " 4.2 Sample Data Sample data used simulate two categorical-alike feature, and two numeric value feature: dept is random character between ‘D1’, ‘D2’ and ‘D3’ grp is random character with randomly generated ‘G1’, ‘G2’ value1 represents numeric value, normally distributed at mean 50 value2 is numeric value, normally distributed at mean 25 set.seed(1234) my.df = data.frame( com = paste(&#39;C&#39;,sample(1:2, 100, replace = T),sep=&#39;&#39;), dept = paste(&#39;D&#39;,sample(1:3, 100, replace = T),sep=&#39;&#39;), grp = paste(&#39;G&#39;,sample(1:2, 100, replace = T),sep=&#39;&#39;), team = paste(&#39;T&#39;,sample(1:2, 100, replace = T),sep=&#39;&#39;), value1 = rnorm(1:100, mean = 50, sd = 5), value2 = rnorm(1:100, mean = 20, sd = 3), value3 = rnorm(1:100, mean = 5, sd = 1), stringsAsFactors = F ) head(my.df) ## com dept grp team value1 value2 value3 ## 1 C1 D1 G2 T1 52.42613 18.26013 3.773185 ## 2 C2 D2 G2 T2 53.48384 17.14016 5.036153 ## 3 C2 D1 G1 T2 50.92757 19.46171 4.578607 ## 4 C2 D1 G2 T1 53.50367 23.02942 4.100636 ## 5 C2 D1 G2 T2 51.55841 20.07088 5.417441 ## 6 C2 D1 G2 T1 53.80231 18.05292 5.153445 "],
["4-3-frequency-table.html", "4.3 Frequency Table", " 4.3 Frequency Table table return table object (which is also array) that report frequency count base of categorical-alike data provided. table has the below data type characteristics. Note that only 2-dimensional table object is a matrix Dimension is.atomic is.vector is.matrix is.array is.table t1 T F F T T t2 T F T T T t3 T F F T T t4 T F F T T ftable is technically a matrix with two dimensional data (it flatten multiple dimension data). It has below data type characteristic. Dimension is.atomic is.vector is.matrix is.array is.table 1 T F T T F 2 T F T T F 3 T F T T F 4 T F T T F 4.3.1 Single Dimension Data t1 = table( my.df$com ) t1 ## ## C1 C2 ## 55 45 str(t1) ## &#39;table&#39; int [1:2(1d)] 55 45 ## - attr(*, &quot;dimnames&quot;)=List of 1 ## ..$ : chr [1:2] &quot;C1&quot; &quot;C2&quot; 4.3.2 Two Dimension Data t2 = table( my.df$com, my.df$dept ) t2 ## ## D1 D2 D3 ## C1 16 18 21 ## C2 15 18 12 str(t2) ## &#39;table&#39; int [1:2, 1:3] 16 15 18 18 21 12 ## - attr(*, &quot;dimnames&quot;)=List of 2 ## ..$ : chr [1:2] &quot;C1&quot; &quot;C2&quot; ## ..$ : chr [1:3] &quot;D1&quot; &quot;D2&quot; &quot;D3&quot; 4.3.3 Three Dimension Data When table contain three or more dimension, use ftable (flat table) to put multi dimension table into one flat output t3 = table( my.df$com, my.df$dept, my.df$grp ) t3 ## , , = G1 ## ## ## D1 D2 D3 ## C1 10 7 11 ## C2 7 9 4 ## ## , , = G2 ## ## ## D1 D2 D3 ## C1 6 11 10 ## C2 8 9 8 str(t3) ## &#39;table&#39; int [1:2, 1:3, 1:2] 10 7 7 9 11 4 6 8 11 9 ... ## - attr(*, &quot;dimnames&quot;)=List of 3 ## ..$ : chr [1:2] &quot;C1&quot; &quot;C2&quot; ## ..$ : chr [1:3] &quot;D1&quot; &quot;D2&quot; &quot;D3&quot; ## ..$ : chr [1:2] &quot;G1&quot; &quot;G2&quot; f3 = ftable( t3 ) f3 ## G1 G2 ## ## C1 D1 10 6 ## D2 7 11 ## D3 11 10 ## C2 D1 7 8 ## D2 9 9 ## D3 4 8 str(f3) ## &#39;ftable&#39; int [1:6, 1:2] 10 7 11 7 9 4 6 11 10 8 ... ## - attr(*, &quot;row.vars&quot;)=List of 2 ## ..$ : chr [1:2] &quot;C1&quot; &quot;C2&quot; ## ..$ : chr [1:3] &quot;D1&quot; &quot;D2&quot; &quot;D3&quot; ## - attr(*, &quot;col.vars&quot;)=List of 1 ## ..$ : chr [1:2] &quot;G1&quot; &quot;G2&quot; 4.3.4 Four Dimension Data When table contain three or more dimension, use ftable (flat table) to put multi dimension table into one flat output t4 = table( my.df$com, my.df$dept, my.df$grp, my.df$team ) t4 ## , , = G1, = T1 ## ## ## D1 D2 D3 ## C1 4 5 4 ## C2 4 5 1 ## ## , , = G2, = T1 ## ## ## D1 D2 D3 ## C1 3 6 6 ## C2 5 5 4 ## ## , , = G1, = T2 ## ## ## D1 D2 D3 ## C1 6 2 7 ## C2 3 4 3 ## ## , , = G2, = T2 ## ## ## D1 D2 D3 ## C1 3 5 4 ## C2 3 4 4 str(t4) ## &#39;table&#39; int [1:2, 1:3, 1:2, 1:2] 4 4 5 5 4 1 3 5 6 5 ... ## - attr(*, &quot;dimnames&quot;)=List of 4 ## ..$ : chr [1:2] &quot;C1&quot; &quot;C2&quot; ## ..$ : chr [1:3] &quot;D1&quot; &quot;D2&quot; &quot;D3&quot; ## ..$ : chr [1:2] &quot;G1&quot; &quot;G2&quot; ## ..$ : chr [1:2] &quot;T1&quot; &quot;T2&quot; f4 = ftable( t4 ) f4 ## T1 T2 ## ## C1 D1 G1 4 6 ## G2 3 3 ## D2 G1 5 2 ## G2 6 5 ## D3 G1 4 7 ## G2 6 4 ## C2 D1 G1 4 3 ## G2 5 3 ## D2 G1 5 4 ## G2 5 4 ## D3 G1 1 3 ## G2 4 4 str(f4) ## &#39;ftable&#39; int [1:12, 1:2] 4 3 5 6 4 6 4 5 5 5 ... ## - attr(*, &quot;row.vars&quot;)=List of 3 ## ..$ : chr [1:2] &quot;C1&quot; &quot;C2&quot; ## ..$ : chr [1:3] &quot;D1&quot; &quot;D2&quot; &quot;D3&quot; ## ..$ : chr [1:2] &quot;G1&quot; &quot;G2&quot; ## - attr(*, &quot;col.vars&quot;)=List of 1 ## ..$ : chr [1:2] &quot;T1&quot; &quot;T2&quot; 4.3.5 Making Table Proportion prop.table converts table or ftable object into proportion. It can calculate table-wise, column-wise or row-wise proportion. prop.table (x,margin=NULL) \\(\\quad\\) x = table object \\(\\quad\\) margin = NULL: proportion table-wise, 1-row_wise, 2-column_wise 4.3.5.1 Proportion Table on ‘table’ object prop.table( t1 ) ## ## C1 C2 ## 0.55 0.45 prop.table( t2 ) ## ## D1 D2 D3 ## C1 0.16 0.18 0.21 ## C2 0.15 0.18 0.12 prop.table( t2, margin=1 ) ## ## D1 D2 D3 ## C1 0.2909091 0.3272727 0.3818182 ## C2 0.3333333 0.4000000 0.2666667 prop.table( t2, margin=2 ) ## ## D1 D2 D3 ## C1 0.5161290 0.5000000 0.6363636 ## C2 0.4838710 0.5000000 0.3636364 4.3.5.2 Proportion Table on ‘ftable’ object prop.table( f3 ) ## G1 G2 ## ## C1 D1 0.10 0.06 ## D2 0.07 0.11 ## D3 0.11 0.10 ## C2 D1 0.07 0.08 ## D2 0.09 0.09 ## D3 0.04 0.08 prop.table( f4 ) ## T1 T2 ## ## C1 D1 G1 0.04 0.06 ## G2 0.03 0.03 ## D2 G1 0.05 0.02 ## G2 0.06 0.05 ## D3 G1 0.04 0.07 ## G2 0.06 0.04 ## C2 D1 G1 0.04 0.03 ## G2 0.05 0.03 ## D2 G1 0.05 0.04 ## G2 0.05 0.04 ## D3 G1 0.01 0.03 ## G2 0.04 0.04 prop.table( f3, margin=1 ) ## G1 G2 ## ## C1 D1 0.6250000 0.3750000 ## D2 0.3888889 0.6111111 ## D3 0.5238095 0.4761905 ## C2 D1 0.4666667 0.5333333 ## D2 0.5000000 0.5000000 ## D3 0.3333333 0.6666667 prop.table( f4, margin=1 ) ## T1 T2 ## ## C1 D1 G1 0.4000000 0.6000000 ## G2 0.5000000 0.5000000 ## D2 G1 0.7142857 0.2857143 ## G2 0.5454545 0.4545455 ## D3 G1 0.3636364 0.6363636 ## G2 0.6000000 0.4000000 ## C2 D1 G1 0.5714286 0.4285714 ## G2 0.6250000 0.3750000 ## D2 G1 0.5555556 0.4444444 ## G2 0.5555556 0.4444444 ## D3 G1 0.2500000 0.7500000 ## G2 0.5000000 0.5000000 prop.table( f3, margin=2 ) ## G1 G2 ## ## C1 D1 0.20833333 0.11538462 ## D2 0.14583333 0.21153846 ## D3 0.22916667 0.19230769 ## C2 D1 0.14583333 0.15384615 ## D2 0.18750000 0.17307692 ## D3 0.08333333 0.15384615 prop.table( f4, margin=2 ) ## T1 T2 ## ## C1 D1 G1 0.07692308 0.12500000 ## G2 0.05769231 0.06250000 ## D2 G1 0.09615385 0.04166667 ## G2 0.11538462 0.10416667 ## D3 G1 0.07692308 0.14583333 ## G2 0.11538462 0.08333333 ## C2 D1 G1 0.07692308 0.06250000 ## G2 0.09615385 0.06250000 ## D2 G1 0.09615385 0.08333333 ## G2 0.09615385 0.08333333 ## D3 G1 0.01923077 0.06250000 ## G2 0.07692308 0.08333333 4.3.6 Adding Margin Info To Table addmargins (x, margin=NULL) \\(\\quad\\) x = table or ftable object \\(\\quad\\) margin = NULL: row and column-sum, 1-col_sum, 2-row_sum 4.3.6.1 Margin Info on ‘table’ object addmargins( t2) ## ## D1 D2 D3 Sum ## C1 16 18 21 55 ## C2 15 18 12 45 ## Sum 31 36 33 100 addmargins( t2,margin=1 ) ## ## D1 D2 D3 ## C1 16 18 21 ## C2 15 18 12 ## Sum 31 36 33 addmargins( t2,margin=2 ) ## ## D1 D2 D3 Sum ## C1 16 18 21 55 ## C2 15 18 12 45 addmargins( t3 ) ## , , = G1 ## ## ## D1 D2 D3 Sum ## C1 10 7 11 28 ## C2 7 9 4 20 ## Sum 17 16 15 48 ## ## , , = G2 ## ## ## D1 D2 D3 Sum ## C1 6 11 10 27 ## C2 8 9 8 25 ## Sum 14 20 18 52 ## ## , , = Sum ## ## ## D1 D2 D3 Sum ## C1 16 18 21 55 ## C2 15 18 12 45 ## Sum 31 36 33 100 addmargins( t4 ) ## , , = G1, = T1 ## ## ## D1 D2 D3 Sum ## C1 4 5 4 13 ## C2 4 5 1 10 ## Sum 8 10 5 23 ## ## , , = G2, = T1 ## ## ## D1 D2 D3 Sum ## C1 3 6 6 15 ## C2 5 5 4 14 ## Sum 8 11 10 29 ## ## , , = Sum, = T1 ## ## ## D1 D2 D3 Sum ## C1 7 11 10 28 ## C2 9 10 5 24 ## Sum 16 21 15 52 ## ## , , = G1, = T2 ## ## ## D1 D2 D3 Sum ## C1 6 2 7 15 ## C2 3 4 3 10 ## Sum 9 6 10 25 ## ## , , = G2, = T2 ## ## ## D1 D2 D3 Sum ## C1 3 5 4 12 ## C2 3 4 4 11 ## Sum 6 9 8 23 ## ## , , = Sum, = T2 ## ## ## D1 D2 D3 Sum ## C1 9 7 11 27 ## C2 6 8 7 21 ## Sum 15 15 18 48 ## ## , , = G1, = Sum ## ## ## D1 D2 D3 Sum ## C1 10 7 11 28 ## C2 7 9 4 20 ## Sum 17 16 15 48 ## ## , , = G2, = Sum ## ## ## D1 D2 D3 Sum ## C1 6 11 10 27 ## C2 8 9 8 25 ## Sum 14 20 18 52 ## ## , , = Sum, = Sum ## ## ## D1 D2 D3 Sum ## C1 16 18 21 55 ## C2 15 18 12 45 ## Sum 31 36 33 100 4.3.6.2 Margin Info on ‘ftable’ object addmargins( f3 ) ## Sum ## 10 6 16 ## 7 11 18 ## 11 10 21 ## 7 8 15 ## 9 9 18 ## 4 8 12 ## Sum 48 52 100 addmargins( f4 ) ## Sum ## 4 6 10 ## 3 3 6 ## 5 2 7 ## 6 5 11 ## 4 7 11 ## 6 4 10 ## 4 3 7 ## 5 3 8 ## 5 4 9 ## 5 4 9 ## 1 3 4 ## 4 4 8 ## Sum 52 48 100 4.3.7 Proportion Table with Margin First to obtain the proportion table, then only add the margin. addmargins( prop.table( t2 )) # add both column and row margin ## ## D1 D2 D3 Sum ## C1 0.16 0.18 0.21 0.55 ## C2 0.15 0.18 0.12 0.45 ## Sum 0.31 0.36 0.33 1.00 addmargins( prop.table( t2 ), 1) # add column margin ## ## D1 D2 D3 ## C1 0.16 0.18 0.21 ## C2 0.15 0.18 0.12 ## Sum 0.31 0.36 0.33 addmargins( prop.table( t2 ), 2) # add row margin ## ## D1 D2 D3 Sum ## C1 0.16 0.18 0.21 0.55 ## C2 0.15 0.18 0.12 0.45 addmargins( prop.table( t3 )) ## , , = G1 ## ## ## D1 D2 D3 Sum ## C1 0.10 0.07 0.11 0.28 ## C2 0.07 0.09 0.04 0.20 ## Sum 0.17 0.16 0.15 0.48 ## ## , , = G2 ## ## ## D1 D2 D3 Sum ## C1 0.06 0.11 0.10 0.27 ## C2 0.08 0.09 0.08 0.25 ## Sum 0.14 0.20 0.18 0.52 ## ## , , = Sum ## ## ## D1 D2 D3 Sum ## C1 0.16 0.18 0.21 0.55 ## C2 0.15 0.18 0.12 0.45 ## Sum 0.31 0.36 0.33 1.00 addmargins( prop.table( t4 )) ## , , = G1, = T1 ## ## ## D1 D2 D3 Sum ## C1 0.04 0.05 0.04 0.13 ## C2 0.04 0.05 0.01 0.10 ## Sum 0.08 0.10 0.05 0.23 ## ## , , = G2, = T1 ## ## ## D1 D2 D3 Sum ## C1 0.03 0.06 0.06 0.15 ## C2 0.05 0.05 0.04 0.14 ## Sum 0.08 0.11 0.10 0.29 ## ## , , = Sum, = T1 ## ## ## D1 D2 D3 Sum ## C1 0.07 0.11 0.10 0.28 ## C2 0.09 0.10 0.05 0.24 ## Sum 0.16 0.21 0.15 0.52 ## ## , , = G1, = T2 ## ## ## D1 D2 D3 Sum ## C1 0.06 0.02 0.07 0.15 ## C2 0.03 0.04 0.03 0.10 ## Sum 0.09 0.06 0.10 0.25 ## ## , , = G2, = T2 ## ## ## D1 D2 D3 Sum ## C1 0.03 0.05 0.04 0.12 ## C2 0.03 0.04 0.04 0.11 ## Sum 0.06 0.09 0.08 0.23 ## ## , , = Sum, = T2 ## ## ## D1 D2 D3 Sum ## C1 0.09 0.07 0.11 0.27 ## C2 0.06 0.08 0.07 0.21 ## Sum 0.15 0.15 0.18 0.48 ## ## , , = G1, = Sum ## ## ## D1 D2 D3 Sum ## C1 0.10 0.07 0.11 0.28 ## C2 0.07 0.09 0.04 0.20 ## Sum 0.17 0.16 0.15 0.48 ## ## , , = G2, = Sum ## ## ## D1 D2 D3 Sum ## C1 0.06 0.11 0.10 0.27 ## C2 0.08 0.09 0.08 0.25 ## Sum 0.14 0.20 0.18 0.52 ## ## , , = Sum, = Sum ## ## ## D1 D2 D3 Sum ## C1 0.16 0.18 0.21 0.55 ## C2 0.15 0.18 0.12 0.45 ## Sum 0.31 0.36 0.33 1.00 addmargins( prop.table( f3 )) ## Sum ## 0.10 0.06 0.16 ## 0.07 0.11 0.18 ## 0.11 0.10 0.21 ## 0.07 0.08 0.15 ## 0.09 0.09 0.18 ## 0.04 0.08 0.12 ## Sum 0.48 0.52 1.00 addmargins( prop.table( f4 )) ## Sum ## 0.04 0.06 0.10 ## 0.03 0.03 0.06 ## 0.05 0.02 0.07 ## 0.06 0.05 0.11 ## 0.04 0.07 0.11 ## 0.06 0.04 0.10 ## 0.04 0.03 0.07 ## 0.05 0.03 0.08 ## 0.05 0.04 0.09 ## 0.05 0.04 0.09 ## 0.01 0.03 0.04 ## 0.04 0.04 0.08 ## Sum 0.52 0.48 1.00 "],
["4-4-data-aggregation.html", "4.4 Data Aggregation", " 4.4 Data Aggregation This chapter explore multiple methods to group data columns and computes value within groups: tapply aggregate (10x slower than apply) 4.4.1 tapply tapply is quick and fast way to produce aggregation with ONE level of grouping. tapply ( X, INDEX, FUN, na.rm = FALSE ) $quad$ X = value vector $quad$ INDEX = groups data, can be factor, number of string $quad$ FUN = function to apply to elements in X according to group specified in INDEX $quad$ na.rm = ignore &lt;NA&gt; values tapply will divide the vector (X) into groups (base on index INDEX), and perform computation (FUN) on each group. If there are &lt;NA&gt; in the vector X, some FUN may fail to return value, such as mean, sum. So it is essential to specify na.rm = TRUE in these cases. Group identifier will be used as column name (accessible through names()). 4.4.1.1 FUN That Returns Vector t1 = tapply(my.df$value1, my.df$com, FUN=mean) t1 ## C1 C2 ## 50.48732 51.12221 names( t1 ) ## [1] &quot;C1&quot; &quot;C2&quot; 4.4.1.2 FUN That Returns Non-Vector - output is list t2 = tapply(my.df$value1, my.df$com, FUN=summary) t2 ## $C1 ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 33.83 48.04 51.29 50.49 53.47 61.37 ## ## $C2 ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 41.96 48.15 51.60 51.12 53.39 64.60 names( t2 ) ## [1] &quot;C1&quot; &quot;C2&quot; 4.4.2 aggretate - base R Aggregate is a very useful base R function and provides quick way to group data and values: Input in list/data.frame, computes and output new data.frame. It groups categorical variable(s) and compute value variable(s) based on function FUN. FUN can be min, max, mean, sd, sum or length (frequency count). ONLY ONE function is supported, and it applies to all value variables !!! 4.4.2.1 Basic Syntax (formula method) - data source is data.frame The formula method use ‘data’ parameter and therefore apply for single data source only. The objective is simplicy and without flexibility to customize column names aggregate (data = df, formula, FUN = function) Formula in the form: value~categorical one value variable ~ one categorical variable aggregate (data = my.df, value1 ~ grp, FUN = length) ## grp value1 ## 1 G1 48 ## 2 G2 52 one value variable ~ multiple categorical variables aggregate (data = my.df, value1 ~ grp + dept, FUN = length) ## grp dept value1 ## 1 G1 D1 17 ## 2 G2 D1 14 ## 3 G1 D2 16 ## 4 G2 D2 20 ## 5 G1 D3 15 ## 6 G2 D3 18 multiple value variables ~ one categorical variable, use cbind() aggregate (data = my.df, cbind(value1,value2) ~ grp, FUN = length) ## grp value1 value2 ## 1 G1 48 48 ## 2 G2 52 52 multiple value variables ~ multiple categorical variable aggregate (data = my.df, cbind(value1,value2) ~ grp + dept, FUN = length) ## grp dept value1 value2 ## 1 G1 D1 17 17 ## 2 G2 D1 14 14 ## 3 G1 D2 16 16 ## 4 G2 D2 20 20 ## 5 G1 D3 15 15 ## 6 G2 D3 18 18 ALL value variables ~ multiple categorical variable, use dot notation Change from FUN=length to sum results in error because sum() cannot be applied to non-numerical variable ‘team’ aggregate (data = my.df, . ~ grp + dept, FUN = length) ## grp dept com team value1 value2 value3 ## 1 G1 D1 17 17 17 17 17 ## 2 G2 D1 14 14 14 14 14 ## 3 G1 D2 16 16 16 16 16 ## 4 G2 D2 20 20 20 20 20 ## 5 G1 D3 15 15 15 15 15 ## 6 G2 D3 18 18 18 18 18 4.4.2.2 Advance Syntax (by method) - data source is either list or data.frame The advantage of ’by method’ are: Can use list/data.frame subset method to choose column to display, hence flexible Can customize output column names (list subset method only) Flexibility to use multiple data sources, hence ‘data’ is not used and has no effect if specified Using list subseting: column name is not preserved, hence must specify meaningful column names. If not supplied, generic names and undesirable column names derived from data value will be used as column name aggregate (x = list(…value_variables…), by = list(…categorical_variables…), FUN = function) aggregate (x = list( v1_mean = my.df$value1, my.df$value2 ), by = list( my.df$grp, DEPT = my.df$dept), FUN=mean) ## Group.1 DEPT v1_mean ## 1 G1 D1 49.19603 ## 2 G2 D1 50.23397 ## 3 G1 D2 50.64011 ## 4 G2 D2 52.20157 ## 5 G1 D3 51.81631 ## 6 G2 D3 50.34312 ## c.18.2601290329418..17.1401638941593..19.4617142391631..23.0294246448329.. ## 1 19.74277 ## 2 20.99601 ## 3 19.87320 ## 4 19.65301 ## 5 19.13836 ## 6 20.54948 Using data.frame subseting: column names are preserved and no option to change. Notice attempt below to change the column name does not succeed aggregate( x = df[,c(…)], by = df[,c(…)]), FUN = function) aggregate( x = df[, p:q], by = df[,s:t]), FUN = function) aggregate(x=my.df[, c(v1_mean=&#39;value1&#39;, &#39;value2&#39;)], by=my.df[,c(GRP=&#39;grp&#39;, &#39;dept&#39;)], FUN=mean) # aggregate(x = my.df[, 4:5], by = my.df[, 1:2], FUN = mean) # produce similar result as above ## grp dept value1 value2 ## 1 G1 D1 49.19603 19.74277 ## 2 G2 D1 50.23397 20.99601 ## 3 G1 D2 50.64011 19.87320 ## 4 G2 D2 52.20157 19.65301 ## 5 G1 D3 51.81631 19.13836 ## 6 G2 D3 50.34312 20.54948 "],
["4-5-r-with-sql-emulation.html", "4.5 R with SQL Emulation", " 4.5 R with SQL Emulation Running SQL statement on existing data.frame are useful to derive summarization and aggregation for someone who are familiar with SQL. 4.5.1 Library sqldf library is required. It has dependency on gsubfn, proto and RSQLite packages. library(sqldf) 4.5.2 Run The Code R data.frame variable is specified in ‘FROM’ clause. Note that . is a SQL operator, any variable with ‘.’ must be contained within single quote. sqldf(&quot; SELECT com, dept, count(*) AS qty, AVG(value1) AS v1_mean, SUM(value2) AS v2_sum FROM &#39;my.df&#39; GROUP BY com, dept &quot;) ## com dept qty v1_mean v2_sum ## 1 C1 D1 16 48.57577 312.0188 ## 2 C1 D2 18 51.93834 351.8293 ## 3 C1 D3 21 50.69999 421.0995 ## 4 C2 D1 15 50.82638 317.5524 ## 5 C2 D2 18 51.07683 359.2020 ## 6 C2 D3 12 51.56008 235.8664 -->"],
["5-data-preprocessing.html", "Chapter 5 Data Preprocessing ", " Chapter 5 Data Preprocessing "],
["5-1-library-2.html", "5.1 Library", " 5.1 Library library(&#39;lubridate&#39;) library(&#39;caTools&#39;) Below summarizes all packages, their functions and purposes used in this Data Preprocessing chapter. Package Function Purpose 1 Base R (factor) as.factor Convert a vector to a new factor droplevels Return a new factor with unused levels removed relevel Return a new factor with new reference level 2 Base R (missing data) is.na Return TRUE if is found in vector na.omit Return a new vector/dataframe with ALL removed, with attribute containing info ro rows removed complete.cases Return TRUE if vector/data.frame row does not contain any 3 Base R (numeric) scale Centre numeric data (-mean), and optionally normalize it (/sd) cut Cut numeric vector into breakpoints, and return grouping factor 4 Base R (date) as.Date Parse a factor / string to date lubridate ymd,mdy,dmy Parse a string to date interval Return interval object betwen two given dates. Can be further used for deriving days/months in between ceiling.date Return next nearest date according to unit (day / month / year) 5 Base R (dataset) subset Return a new data based on input logical vector merge Merge two data.frame together based on common key nrow, ncol How many columns / rows in the dataset ? length How many elements in a vector ? caTools sample.split Split vector into two with a ratio, return result in logical vector "],
["5-2-sample-data-2.html", "5.2 Sample Data", " 5.2 Sample Data my.df &lt;- read.csv ( file=&quot;./datasets/import_sample.csv&quot;, na.strings=c(&#39;NA&#39;,&#39;NULL&#39;,&#39;&#39;)) # stringsAsFactors = FALSE ) my.df ## X X.1 dept gender weight height date_birth amount date_last ## 1 1 ID101 D1 Male 35.00000 173 1/7/1973 100 2/29/2016 ## 2 2 ID102 D2 Female 37.10000 164 28/2/1980 121 4/1/2017 ## 3 3 ID103 D3 Female 43.12000 178 31/12/1978 152 10/31/2015 ## 4 4 ID104 D1 Male 38.12300 182 12/1/1997 133 11/1/2016 ## 5 5 ID105 D1 Male 54.12340 159 2/1/1982 143 9/30/2016 ## 6 6 ID106 D3 Female 34.12345 166 26/7/1973 155 11/27/2015 ## 7 7 ID107 D2 Male 49.12346 153 21/8/1985 117 3/31/2017 ## 8 8 ID108 D1 Male 50.20000 159 2/1/1982 143 9/30/2016 ## 9 9 ID109 D3 Female 59.10000 166 13/7/1975 155 11/1/2017 ## 10 10 ID110 D2 Male 63.20000 163 24/8/1982 117 3/12/2016 ## 11 11 ID111 D3 Female 75.10000 170 9/8/1979 135 2/1/2015 ## 12 12 ID112 D2 Male 52.10000 169 &lt;NA&gt; 128 &lt;NA&gt; ## 13 13 ID113 D3 &lt;NA&gt; 88.80000 171 &lt;NA&gt; 141 &lt;NA&gt; ## date.first ## 1 2013-07-31 ## 2 2013-08-31 ## 3 2014-12-31 ## 4 2015-02-28 ## 5 2012-06-15 ## 6 2013-04-28 ## 7 2014-03-01 ## 8 2011-06-15 ## 9 2012-04-02 ## 10 2013-03-12 ## 11 &lt;NA&gt; ## 12 &lt;NA&gt; ## 13 &lt;NA&gt; str(my.df) ## &#39;data.frame&#39;: 13 obs. of 10 variables: ## $ X : int 1 2 3 4 5 6 7 8 9 10 ... ## $ X.1 : Factor w/ 13 levels &quot;ID101&quot;,&quot;ID102&quot;,..: 1 2 3 4 5 6 7 8 9 10 ... ## $ dept : Factor w/ 3 levels &quot;D1&quot;,&quot;D2&quot;,&quot;D3&quot;: 1 2 3 1 1 3 2 1 3 2 ... ## $ gender : Factor w/ 2 levels &quot;Female&quot;,&quot;Male&quot;: 2 1 1 2 2 1 2 2 1 2 ... ## $ weight : num 35 37.1 43.1 38.1 54.1 ... ## $ height : int 173 164 178 182 159 166 153 159 166 163 ... ## $ date_birth: Factor w/ 10 levels &quot;1/7/1973&quot;,&quot;12/1/1997&quot;,..: 1 8 9 2 4 7 5 4 3 6 ... ## $ amount : int 100 121 152 133 143 155 117 143 155 117 ... ## $ date_last : Factor w/ 10 levels &quot;10/31/2015&quot;,&quot;11/1/2016&quot;,..: 6 9 1 2 10 4 8 10 3 7 ... ## $ date.first: Factor w/ 10 levels &quot;2011-06-15&quot;,&quot;2012-04-02&quot;,..: 6 7 9 10 3 5 8 1 2 4 ... "],
["5-3-column-manipulation.html", "5.3 Column Manipulation", " 5.3 Column Manipulation 5.3.1 Duplicating Columns Duplicate single column using $ selector my.df$Z1 = my.df$X my.df$Z2 = my.df$X str(my.df) ## &#39;data.frame&#39;: 13 obs. of 12 variables: ## $ X : int 1 2 3 4 5 6 7 8 9 10 ... ## $ X.1 : Factor w/ 13 levels &quot;ID101&quot;,&quot;ID102&quot;,..: 1 2 3 4 5 6 7 8 9 10 ... ## $ dept : Factor w/ 3 levels &quot;D1&quot;,&quot;D2&quot;,&quot;D3&quot;: 1 2 3 1 1 3 2 1 3 2 ... ## $ gender : Factor w/ 2 levels &quot;Female&quot;,&quot;Male&quot;: 2 1 1 2 2 1 2 2 1 2 ... ## $ weight : num 35 37.1 43.1 38.1 54.1 ... ## $ height : int 173 164 178 182 159 166 153 159 166 163 ... ## $ date_birth: Factor w/ 10 levels &quot;1/7/1973&quot;,&quot;12/1/1997&quot;,..: 1 8 9 2 4 7 5 4 3 6 ... ## $ amount : int 100 121 152 133 143 155 117 143 155 117 ... ## $ date_last : Factor w/ 10 levels &quot;10/31/2015&quot;,&quot;11/1/2016&quot;,..: 6 9 1 2 10 4 8 10 3 7 ... ## $ date.first: Factor w/ 10 levels &quot;2011-06-15&quot;,&quot;2012-04-02&quot;,..: 6 7 9 10 3 5 8 1 2 4 ... ## $ Z1 : int 1 2 3 4 5 6 7 8 9 10 ... ## $ Z2 : int 1 2 3 4 5 6 7 8 9 10 ... 5.3.2 Delete Columns 5.3.2.1 Delete One Column Use $ column selector, setting to NULL. my.df$Z1 = NULL 5.3.2.2 Delete Multiple Columns Use multiple columns selector [,vector], with vector containing column numbers or names, setting to NULL. my.df[,c(&#39;X&#39;,&#39;Z2&#39;)] = NULL str(my.df) ## &#39;data.frame&#39;: 13 obs. of 9 variables: ## $ X.1 : Factor w/ 13 levels &quot;ID101&quot;,&quot;ID102&quot;,..: 1 2 3 4 5 6 7 8 9 10 ... ## $ dept : Factor w/ 3 levels &quot;D1&quot;,&quot;D2&quot;,&quot;D3&quot;: 1 2 3 1 1 3 2 1 3 2 ... ## $ gender : Factor w/ 2 levels &quot;Female&quot;,&quot;Male&quot;: 2 1 1 2 2 1 2 2 1 2 ... ## $ weight : num 35 37.1 43.1 38.1 54.1 ... ## $ height : int 173 164 178 182 159 166 153 159 166 163 ... ## $ date_birth: Factor w/ 10 levels &quot;1/7/1973&quot;,&quot;12/1/1997&quot;,..: 1 8 9 2 4 7 5 4 3 6 ... ## $ amount : int 100 121 152 133 143 155 117 143 155 117 ... ## $ date_last : Factor w/ 10 levels &quot;10/31/2015&quot;,&quot;11/1/2016&quot;,..: 6 9 1 2 10 4 8 10 3 7 ... ## $ date.first: Factor w/ 10 levels &quot;2011-06-15&quot;,&quot;2012-04-02&quot;,..: 6 7 9 10 3 5 8 1 2 4 ... 5.3.3 Renaming Columns 5.3.3.1 What Are The Column Names colnames returns vector of names attribute of data.frame. colnames(my.df) ## [1] &quot;X.1&quot; &quot;dept&quot; &quot;gender&quot; &quot;weight&quot; &quot;height&quot; ## [6] &quot;date_birth&quot; &quot;amount&quot; &quot;date_last&quot; &quot;date.first&quot; 5.3.3.2 Set the Column Name(s) Use colnames to rename single or multiple columns. Use [] to select the specific column(s). colnames(my.df)[c(1,9)] = c(&#39;id&#39;,&#39;date_first&#39;) colnames(my.df) ## [1] &quot;id&quot; &quot;dept&quot; &quot;gender&quot; &quot;weight&quot; &quot;height&quot; ## [6] &quot;date_birth&quot; &quot;amount&quot; &quot;date_last&quot; &quot;date_first&quot; "],
["5-4-missing-data.html", "5.4 Missing Data", " 5.4 Missing Data 5.4.1 Detecting Complete/Incomplete Vector/Row complete.cases returns logical vector for elements that doesn’t contain , with TRUE. It can be applied to both vector or data.frame. complete.cases(my.df$date_birth) # vector example complete.cases(my.df) # data.frame example ## [1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE ## [12] FALSE FALSE ## [1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE FALSE ## [12] FALSE FALSE Negate complete.cases resulting TRUE for rows containing . !complete.cases(my.df$date_birth) # vector example !complete.cases(my.df) # data.frame example ## [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [12] TRUE TRUE ## [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE ## [12] TRUE TRUE Result from complete.cases is useful to retrieve incomplete rows for further investigation. my.df[!complete.cases(my.df),] ## id dept gender weight height date_birth amount date_last date_first ## 11 ID111 D3 Female 75.1 170 9/8/1979 135 2/1/2015 &lt;NA&gt; ## 12 ID112 D2 Male 52.1 169 &lt;NA&gt; 128 &lt;NA&gt; &lt;NA&gt; ## 13 ID113 D3 &lt;NA&gt; 88.8 171 &lt;NA&gt; 141 &lt;NA&gt; &lt;NA&gt; 5.4.2 Removing Missing Data na.omit returns data with removed. The advantage of this method compare to complete.cases are: - simpler syntax, filtering using [rows,] not require - additonal attribute, accessible through na.action(), providing information on element number removed 5.4.2.1 Remove Elements with Missing Data In Vector na.omit(my.df$date_birth) # vector example str(na.omit(my.df$date_birth)) ## [1] 1/7/1973 28/2/1980 31/12/1978 12/1/1997 2/1/1982 26/7/1973 ## [7] 21/8/1985 2/1/1982 13/7/1975 24/8/1982 9/8/1979 ## attr(,&quot;na.action&quot;) ## [1] 12 13 ## attr(,&quot;class&quot;) ## [1] &quot;omit&quot; ## 10 Levels: 1/7/1973 12/1/1997 13/7/1975 2/1/1982 21/8/1985 ... 9/8/1979 ## Factor w/ 10 levels &quot;1/7/1973&quot;,&quot;12/1/1997&quot;,..: 1 8 9 2 4 7 5 4 3 6 ... ## - attr(*, &quot;na.action&quot;)=Class &#39;omit&#39; int [1:2] 12 13 5.4.2.2 Remove Rows with Missing Data In Data Frame # my.df[complete.cases(my.df),] # longer method which is less elegant my.df = na.omit(my.df) # data.frame example na.action(my.df) # number of rows removed ## 11 12 13 ## 11 12 13 ## attr(,&quot;class&quot;) ## [1] &quot;omit&quot; "],
["5-5-merging-data.html", "5.5 Merging Data", " 5.5 Merging Data "],
["5-6-categorical-data.html", "5.6 Categorical Data", " 5.6 Categorical Data 5.6.1 Converting to Factor as.factor converts string, discrete integer and logical data to factor. Each unique value will make up the levels for the factor, (sorted alphabatically during creation time). Convert below string to factor, and the first element of the sorted unique value - D1 is the reference level. x = as.factor( c(&#39;D2&#39;, &#39;D1&#39;, &#39;D3&#39;, &#39;D3&#39;, &#39;D2&#39;, &#39;D1&#39;, &#39;D3&#39;) ) x ## [1] D2 D1 D3 D3 D2 D1 D3 ## Levels: D1 D2 D3 5.6.2 Releveling Factor relevel return a new factor with base reference changed. Notice current factor x has ’D1` as the reference. levels(x) ## [1] &quot;D1&quot; &quot;D2&quot; &quot;D3&quot; Change the reference to ‘D3’, return a new factor y = relevel(x, &#39;D3&#39;) levels(y) ## [1] &quot;D3&quot; &quot;D1&quot; &quot;D2&quot; 5.6.3 Dropping Levels Levels can be ‘squeezed’ if it contain levels that no longer have data in it. Original data below contain three levels, D1 D2 D3. After removing all ‘D2’ value from the factor, all three original levels still exist ! z = x[x!=&#39;D2&#39;] str(z) ## Factor w/ 3 levels &quot;D1&quot;,&quot;D2&quot;,&quot;D3&quot;: 1 3 3 1 3 Use droplevels to remove all unused levels. All value will be renumbered. z = droplevels(z) str(z) ## Factor w/ 2 levels &quot;D1&quot;,&quot;D3&quot;: 1 2 2 1 2 "],
["5-7-string-manipulation.html", "5.7 String Manipulation", " 5.7 String Manipulation 5.7.1 Extration 5.7.2 Removal 5.7.3 Concatenation "],
["5-8-date-manipulation.html", "5.8 Date Manipulation", " 5.8 Date Manipulation 5.8.1 Parsing Date When import date date from text file using read(), often date column are imported as string or factor. Note that lubridate::dmy,ymd can convert from string only, whereas as.Date can convert from both string and factor. Before Conversion, verify that all dates column are actually chr. str(my.df) ## &#39;data.frame&#39;: 10 obs. of 9 variables: ## $ id : Factor w/ 13 levels &quot;ID101&quot;,&quot;ID102&quot;,..: 1 2 3 4 5 6 7 8 9 10 ## $ dept : Factor w/ 3 levels &quot;D1&quot;,&quot;D2&quot;,&quot;D3&quot;: 1 2 3 1 1 3 2 1 3 2 ## $ gender : Factor w/ 2 levels &quot;Female&quot;,&quot;Male&quot;: 2 1 1 2 2 1 2 2 1 2 ## $ weight : num 35 37.1 43.1 38.1 54.1 ... ## $ height : int 173 164 178 182 159 166 153 159 166 163 ## $ date_birth: Factor w/ 10 levels &quot;1/7/1973&quot;,&quot;12/1/1997&quot;,..: 1 8 9 2 4 7 5 4 3 6 ## $ amount : int 100 121 152 133 143 155 117 143 155 117 ## $ date_last : Factor w/ 10 levels &quot;10/31/2015&quot;,&quot;11/1/2016&quot;,..: 6 9 1 2 10 4 8 10 3 7 ## $ date_first: Factor w/ 10 levels &quot;2011-06-15&quot;,&quot;2012-04-02&quot;,..: 6 7 9 10 3 5 8 1 2 4 ## - attr(*, &quot;na.action&quot;)=Class &#39;omit&#39; Named int [1:3] 11 12 13 ## .. ..- attr(*, &quot;names&quot;)= chr [1:3] &quot;11&quot; &quot;12&quot; &quot;13&quot; Two ways to convert string to date: as.Date - convert vector into date data type. A format parameter has to be specified to match the source format. Otherwise as.Date will try default format of “%Y-%m-%d” then “%Y/%m/%d” lubridate::dmy, mdy, ymd - this is more elegant because manual format is not required. However, it converts only string data. If the data source is factor, convert it to string first In both methods, unmatched rows with unrecognized format will throw an error. my.df$date_birth = as.Date( my.df$date_birth, format = &quot;%d/%m/%Y&quot; ) # base R my.df$date_last = mdy( my.df$date_last ) # lubridate my.df$date_first = ymd( my.df$date_first ) # lubridate str(my.df) ## &#39;data.frame&#39;: 10 obs. of 9 variables: ## $ id : Factor w/ 13 levels &quot;ID101&quot;,&quot;ID102&quot;,..: 1 2 3 4 5 6 7 8 9 10 ## $ dept : Factor w/ 3 levels &quot;D1&quot;,&quot;D2&quot;,&quot;D3&quot;: 1 2 3 1 1 3 2 1 3 2 ## $ gender : Factor w/ 2 levels &quot;Female&quot;,&quot;Male&quot;: 2 1 1 2 2 1 2 2 1 2 ## $ weight : num 35 37.1 43.1 38.1 54.1 ... ## $ height : int 173 164 178 182 159 166 153 159 166 163 ## $ date_birth: Date, format: &quot;1973-07-01&quot; &quot;1980-02-28&quot; ... ## $ amount : int 100 121 152 133 143 155 117 143 155 117 ## $ date_last : Date, format: &quot;2016-02-29&quot; &quot;2017-04-01&quot; ... ## $ date_first: Date, format: &quot;2013-07-31&quot; &quot;2013-08-31&quot; ... ## - attr(*, &quot;na.action&quot;)=Class &#39;omit&#39; Named int [1:3] 11 12 13 ## .. ..- attr(*, &quot;names&quot;)= chr [1:3] &quot;11&quot; &quot;12&quot; &quot;13&quot; 5.8.2 What Day/Month/Year lubridate has useful functions to return numeric day, month, year from date object. 5.8.2.1 What Day d1 = as.Date(&#39;2017-03-31&#39;) lubridate::day(d1) ## [1] 31 5.8.2.2 What Month lubridate::month(d1) ## [1] 3 5.8.2.3 What Year lubridate::year(d1) ## [1] 2017 5.8.3 Days/Months/Year Difference 5.8.3.1 Days Difference my.df$date_last my.df$date_first my.df$date_last - my.df$date_first ## [1] &quot;2016-02-29&quot; &quot;2017-04-01&quot; &quot;2015-10-31&quot; &quot;2016-11-01&quot; &quot;2016-09-30&quot; ## [6] &quot;2015-11-27&quot; &quot;2017-03-31&quot; &quot;2016-09-30&quot; &quot;2017-11-01&quot; &quot;2016-03-12&quot; ## [1] &quot;2013-07-31&quot; &quot;2013-08-31&quot; &quot;2014-12-31&quot; &quot;2015-02-28&quot; &quot;2012-06-15&quot; ## [6] &quot;2013-04-28&quot; &quot;2014-03-01&quot; &quot;2011-06-15&quot; &quot;2012-04-02&quot; &quot;2013-03-12&quot; ## Time differences in days ## [1] 943 1309 304 612 1568 943 1126 1934 2039 1096 5.8.3.2 Months Difference Combination of lubridate::interval and lubridate operator provides a integer vector of months elapsed between two date: - The result can be negative if first date parameter &gt; second date parameter - lubridate ‘intelligently’ knows interval is from end of month to end of month interval interval( ymd(&#39;2016-03-30&#39;), ymd(&#39;2016-04-29&#39;) ) %/% months(1) # end day no. &lt; begining day no. interval( ymd(&#39;2016-03-30&#39;), ymd(&#39;2016-04-30&#39;) ) %/% months(1) # end day no. == beginning day no. interval( ymd(&#39;2016-03-31&#39;), ymd(&#39;2016-04-30&#39;) ) %/% months(1) # end of month to end of month ## [1] 0 ## [1] 1 ## [1] 1 Apply this to data frame / vector. my.df$date_first my.df$date_last interval( my.df$date_first, my.df$date_last ) %/% months(1) ## [1] &quot;2013-07-31&quot; &quot;2013-08-31&quot; &quot;2014-12-31&quot; &quot;2015-02-28&quot; &quot;2012-06-15&quot; ## [6] &quot;2013-04-28&quot; &quot;2014-03-01&quot; &quot;2011-06-15&quot; &quot;2012-04-02&quot; &quot;2013-03-12&quot; ## [1] &quot;2016-02-29&quot; &quot;2017-04-01&quot; &quot;2015-10-31&quot; &quot;2016-11-01&quot; &quot;2016-09-30&quot; ## [6] &quot;2015-11-27&quot; &quot;2017-03-31&quot; &quot;2016-09-30&quot; &quot;2017-11-01&quot; &quot;2016-03-12&quot; ## [1] 31 43 10 20 51 30 36 63 66 36 5.8.3.3 Years Difference Use lubridate::year function to find the year (integer) of a given date. Difference of the year() results from the birthdate and current date is the Age. year(now()) - year(my.df$date_birth) ## [1] 44 37 39 20 35 44 32 35 42 35 However in insurance industry, only a full year is considred for age. interval( ymd(&#39;2016-02-29&#39;), ymd(&#39;2017-02-27&#39;) ) %/% years(1) # a day short for a year interval( ymd(&#39;2016-02-29&#39;), ymd(&#39;2017-02-28&#39;) ) %/% years(1) # EoM to EoM ## [1] 0 ## [1] 1 Apply this to data frame / vector. my.df$date_first my.df$date_last interval( my.df$date_first, my.df$date_last ) %/% years(1) ## [1] &quot;2013-07-31&quot; &quot;2013-08-31&quot; &quot;2014-12-31&quot; &quot;2015-02-28&quot; &quot;2012-06-15&quot; ## [6] &quot;2013-04-28&quot; &quot;2014-03-01&quot; &quot;2011-06-15&quot; &quot;2012-04-02&quot; &quot;2013-03-12&quot; ## [1] &quot;2016-02-29&quot; &quot;2017-04-01&quot; &quot;2015-10-31&quot; &quot;2016-11-01&quot; &quot;2016-09-30&quot; ## [6] &quot;2015-11-27&quot; &quot;2017-03-31&quot; &quot;2016-09-30&quot; &quot;2017-11-01&quot; &quot;2016-03-12&quot; ## [1] 2 3 0 1 4 2 3 5 5 3 5.8.4 Days/Monhts/Years Later Adding days to a date is simple. Just + days(123) for 123 days later. However, adding months and years is tricky, because certain months are shorter. Good solution is to use lubridate::%m+% operator. 5.8.4.1 Days Later Adding days is as simple as add number of days to the date variables. my.df$date_last ## [1] &quot;2016-02-29&quot; &quot;2017-04-01&quot; &quot;2015-10-31&quot; &quot;2016-11-01&quot; &quot;2016-09-30&quot; ## [6] &quot;2015-11-27&quot; &quot;2017-03-31&quot; &quot;2016-09-30&quot; &quot;2017-11-01&quot; &quot;2016-03-12&quot; 5.8.4.2 Months Later Adding month(s) is tricky on the last day of month. Use lubridate operator for correct solution, because it takes cares of last day of month including February of leap years. my.df$date_last my.df$date_last + months(1) # 1 month later, bad solution, can result in &lt;NA&gt; my.df$date_last %m+% months(1) # 1 month later, good solution ## [1] &quot;2016-02-29&quot; &quot;2017-04-01&quot; &quot;2015-10-31&quot; &quot;2016-11-01&quot; &quot;2016-09-30&quot; ## [6] &quot;2015-11-27&quot; &quot;2017-03-31&quot; &quot;2016-09-30&quot; &quot;2017-11-01&quot; &quot;2016-03-12&quot; ## [1] &quot;2016-03-29&quot; &quot;2017-05-01&quot; NA &quot;2016-12-01&quot; &quot;2016-10-30&quot; ## [6] &quot;2015-12-27&quot; NA &quot;2016-10-30&quot; &quot;2017-12-01&quot; &quot;2016-04-12&quot; ## [1] &quot;2016-03-29&quot; &quot;2017-05-01&quot; &quot;2015-11-30&quot; &quot;2016-12-01&quot; &quot;2016-10-30&quot; ## [6] &quot;2015-12-27&quot; &quot;2017-04-30&quot; &quot;2016-10-30&quot; &quot;2017-12-01&quot; &quot;2016-04-12&quot; 5.8.4.3 Years Later Adding year(s) is similar to adding month(s) above. my.df$date_last my.df$date_last + years(1) # 1 year later, bad solution my.df$date_last %m+% years(1) # 1 year later, good solution ## [1] &quot;2016-02-29&quot; &quot;2017-04-01&quot; &quot;2015-10-31&quot; &quot;2016-11-01&quot; &quot;2016-09-30&quot; ## [6] &quot;2015-11-27&quot; &quot;2017-03-31&quot; &quot;2016-09-30&quot; &quot;2017-11-01&quot; &quot;2016-03-12&quot; ## [1] NA &quot;2018-04-01&quot; &quot;2016-10-31&quot; &quot;2017-11-01&quot; &quot;2017-09-30&quot; ## [6] &quot;2016-11-27&quot; &quot;2018-03-31&quot; &quot;2017-09-30&quot; &quot;2018-11-01&quot; &quot;2017-03-12&quot; ## [1] &quot;2017-02-28&quot; &quot;2018-04-01&quot; &quot;2016-10-31&quot; &quot;2017-11-01&quot; &quot;2017-09-30&quot; ## [6] &quot;2016-11-27&quot; &quot;2018-03-31&quot; &quot;2017-09-30&quot; &quot;2018-11-01&quot; &quot;2017-03-12&quot; 5.8.5 Last Day of Month lubridate::ceiling_date rounds up the date to the nearest unit. When rounding up a date to the **next nearest month*, it results the first day of next month. my.df$date_last ceiling_date(my.df$date_last, &quot;month&quot;) ## [1] &quot;2016-02-29&quot; &quot;2017-04-01&quot; &quot;2015-10-31&quot; &quot;2016-11-01&quot; &quot;2016-09-30&quot; ## [6] &quot;2015-11-27&quot; &quot;2017-03-31&quot; &quot;2016-09-30&quot; &quot;2017-11-01&quot; &quot;2016-03-12&quot; ## [1] &quot;2016-03-01&quot; &quot;2017-05-01&quot; &quot;2015-11-01&quot; &quot;2016-12-01&quot; &quot;2016-10-01&quot; ## [6] &quot;2015-12-01&quot; &quot;2017-04-01&quot; &quot;2016-10-01&quot; &quot;2017-12-01&quot; &quot;2016-04-01&quot; Substracting this date by one will return last day of the month. ceiling_date(my.df$date_last, &quot;month&quot;) - days(1) ## [1] &quot;2016-02-29&quot; &quot;2017-04-30&quot; &quot;2015-10-31&quot; &quot;2016-11-30&quot; &quot;2016-09-30&quot; ## [6] &quot;2015-11-30&quot; &quot;2017-03-31&quot; &quot;2016-09-30&quot; &quot;2017-11-30&quot; &quot;2016-03-31&quot; "],
["5-9-number-manipulation.html", "5.9 Number Manipulation", " 5.9 Number Manipulation 5.9.1 Sample Data Scaling section will use sample data generated as below: set.seed(1234) num.df = data.frame( id = paste(&#39;ID_&#39;, 1:5, sep = &#39;&#39;), value1 = sample(50:100, 5), value2 = sample(10:50, 5), stringsAsFactors = F ) num.df ## id value1 value2 ## 1 ID_1 55 36 ## 2 ID_2 81 10 ## 3 ID_3 79 19 ## 4 ID_4 98 35 ## 5 ID_5 90 29 5.9.2 Z-Score Scaling scale apply transformation column-wise, for columns within matrix or dataframe scale return a matrix scale (x, center=T, scale=T) # default S-Score transformation \\(\\quad\\) center = T (default) means value minus with mean \\(\\quad\\) scale = T (default) means value divide by sd \\(\\quad\\) \\(\\quad\\) output scaled:center --&gt; mean \\(\\quad\\) \\(\\quad\\) output scaled:scale --&gt; sd scale( num.df[,2:3] ) ## value1 value2 ## [1,] -1.58066858 0.9170971 ## [2,] 0.02469795 -1.4206014 ## [3,] -0.09879179 -0.6113981 ## [4,] 1.07436067 0.8271856 ## [5,] 0.58040174 0.2877167 ## attr(,&quot;scaled:center&quot;) ## value1 value2 ## 80.6 25.8 ## attr(,&quot;scaled:scale&quot;) ## value1 value2 ## 16.19568 11.12205 scale( num.df[,2:3], scale=F ) ## value1 value2 ## [1,] -25.6 10.2 ## [2,] 0.4 -15.8 ## [3,] -1.6 -6.8 ## [4,] 17.4 9.2 ## [5,] 9.4 3.2 ## attr(,&quot;scaled:center&quot;) ## value1 value2 ## 80.6 25.8 5.9.3 Min Max Scaling Min-Max scaling will transform all numbers between 0 and 1. Easiest way to perform this transformation is to write a function then apply. min_max = function(x){(x-min(x))/(max(x)-min(x))} apply( num.df[,2:3], MARGIN = 2, FUN = min_max ) ## 2 means column-wise ## value1 value2 ## [1,] 0.0000000 1.0000000 ## [2,] 0.6046512 0.0000000 ## [3,] 0.5581395 0.3461538 ## [4,] 1.0000000 0.9615385 ## [5,] 0.8139535 0.7307692 "],
["5-10-artificial-grouping.html", "5.10 Artificial Grouping", " 5.10 Artificial Grouping Artificial group can be created based on existing numeric data. Such as age_group based on age. 5.10.1 Grouping with Numeric Breakpoints Simulate data with x,y,z variables. p simulates priority scoring. x = rnorm(10, mean = 10) y = rnorm(10, mean = 10) p = x * y Articial groups is created first by identifying the number of groups, generate the break points vector, then cut the data base on break points and return factor as output. Automatically calculate breakpoints by distributing numbers into the min-max range, in low to high order: num_groups = 4 breakPoints = seq(min(p), max(p), length.out = num_groups + 1) breakPoints ## [1] 85.59917 92.47633 99.35348 106.23063 113.10779 cut ( x, breaks, right = TRUE, include.lowest = FALSE) \\(\\quad\\) x: numeric vector to be cutted \\(\\quad\\) breaks: numeric vector ranging from low to high (in order) \\(\\quad\\) include.lowest: FALSE - ommit element matching lowest number in breaks \\(\\quad\\) right: TRUE - close end on right;open end on left The result from cut is factor based on order from breakPoints. Therefore, once convert into numeric, the group number is in order of low to high accoriding to breakPoints. Verify that group (g) has been assigned for each priority (p). g = as.numeric( cut( p, breakPoints, include.lowest=TRUE)) data.frame(p,g) ## p g ## 1 103.90189 3 ## 2 89.43620 1 ## 3 85.91982 1 ## 4 86.45631 1 ## 5 113.10779 4 ## 6 96.50497 2 ## 7 85.59917 1 ## 8 88.17396 1 ## 9 105.27011 3 ## 10 101.99212 3 5.10.2 Grouping based on Custom Criteria creates a logical vector, indicating. Ratio specified in SplitRatio will be have value ‘TRUE’ "],
["5-11-radom-dataset-splitting.html", "5.11 Radom Dataset Splitting", " 5.11 Radom Dataset Splitting When we have only ONE dataset, we can split them into training and testing, example 0.7/0.3 split. 5.11.1 Simple Random Split 5.11.1.1 Verify Dataset Verify the total number of rows before splitting. nrow(my.df) ## [1] 10 5.11.1.2 Create Randomized Row Numbers Vector Randomly pick 60% of elements from a bag of all row numbers, with no replacement. Verify that 60% of full observations are picked. set.seed(8034) train.rows = sample( 1:nrow(my.df), 0.6 * nrow(my.df) ) length(train.rows) train.rows ## [1] 6 ## [1] 5 7 4 3 1 10 5.11.1.3 Split Data Into Two Sets Subseting for the training data, using [,] row selection method. my.df [train.rows, ] # Training Data (0.6) ## dept gender weight height date_birth amount date_last date_first ## 5 D1 Male 54.12340 159 1982-01-02 143 2016-09-30 2012-06-15 ## 7 D2 Male 49.12346 153 1985-08-21 117 2017-03-31 2014-03-01 ## 4 D1 Male 38.12300 182 1997-01-12 133 2016-11-01 2015-02-28 ## 3 D3 Female 43.12000 178 1978-12-31 152 2015-10-31 2014-12-31 ## 1 D1 Male 35.00000 173 1973-07-01 100 2016-02-29 2013-07-31 ## 10 D2 Male 63.20000 163 1982-08-24 117 2016-03-12 2013-03-12 Subsetting the test data by negating the splitting row numbers. my.df [-train.rows, ] # Training Data (0.4) ## dept gender weight height date_birth amount date_last date_first ## 2 D2 Female 37.10000 164 1980-02-28 121 2017-04-01 2013-08-31 ## 6 D3 Female 34.12345 166 1973-07-26 155 2015-11-27 2013-04-28 ## 8 D1 Male 50.20000 159 1982-01-02 143 2016-09-30 2011-06-15 ## 9 D3 Female 59.10000 166 1975-07-13 155 2017-11-01 2012-04-02 5.11.2 Random Split (Maintaining Data Value Ratio) Sometimes we want to maintain the original data value ratio. For example, if original factor variable contains 40%A, 30%B and 30%C, and we want the splitted sample to maintain 40-30-30 ratio. For prediction on classification/clustering, usually the dependent variable (categorical/binary) is used as spliting parameter For regression prediction of continuous data, it doesn’t make sense to use dependent variable for spliting, as the ratio cannot be determined with continuous data Maintaining the ratio is key difference to sample approach which does not care about the original balance 5.11.2.1 Verify Data Value Ratio We are going to split the data frame, while maintaining the ratio of dept variable. Here is distribution dept variable. table(my.df[,c(&#39;dept&#39;)]) ## ## D1 D2 D3 ## 4 3 3 5.11.2.2 Create The Spliting Logical Vector caTools::sample.split is the right tool for this job: It takes a vector (or a column from a data.frame), randomly assign TRUE (training data) and FALSE (test data) according to a specified ratio While doing so, it automatically maintaining the data value ratio within the split It returns a logical vector for data subsetting purpose sample.split (Y, SplitRatio = 2/3, group = NULL) \\(\\quad\\) Y = vector, of which ratio of value to be maintained \\(\\quad\\) SplitRatio = ratio of split set.seed(8034) spl = sample.split(my.df$dept, 0.6) # randomly 0.6 set to TRUE, remaining FALSE spl ## [1] FALSE TRUE TRUE FALSE TRUE FALSE TRUE TRUE TRUE FALSE 5.11.2.3 Split Data Into Two Sets Subseting for the training data, using subset or [,] row selection method # my.df[split.data,] # alternative s1 = subset( my.df, spl ) # Training Data (0.6) table(s1[,&#39;dept&#39;]) ## ## D1 D2 D3 ## 2 2 2 Subsetting the test data by negating the splitting logical vector. # my.df[!spl,] # alternative s2 = subset( my.df, !spl ) # Testing Data (0.4) table(s2[,&#39;dept&#39;]) ## ## D1 D2 D3 ## 2 1 1 -->"],
["6-find-order-and-filter-data.html", "Chapter 6 Find, Order and Filter Data ", " Chapter 6 Find, Order and Filter Data "],
["6-1-library-3.html", "6.1 Library", " 6.1 Library Functions and libraries used in this chapter are as below: Package Function Purpose 1 Base R match return the position(s) of first match order return the positions for each vector element with order which return the positions of value TRUE of a logical vector "],
["6-2-sample-data-4.html", "6.2 Sample Data", " 6.2 Sample Data 6.2.1 Sample Data Frame str( my.df ) head( my.df ) ## &#39;data.frame&#39;: 10 obs. of 2 variables: ## $ dept : chr &quot;D1&quot; &quot;D2&quot; &quot;D2&quot; &quot;D2&quot; ... ## $ value1: num 52.5 47.1 47.3 47.2 45.5 ... ## dept value1 ## 1 D1 52.53028 ## 2 D2 47.12630 ## 3 D2 47.26684 ## 4 D2 47.17774 ## 5 D3 45.54981 ## 6 D2 47.61404 6.2.2 Sample Vector str( vector ) ## function (mode = &quot;logical&quot;, length = 0L) head( my.vector ) ## [1] 46 27 31 51 19 76 "],
["6-3-finding-data-in-vector.html", "6.3 Finding Data in Vector", " 6.3 Finding Data in Vector Find the first match position number(s) of specific element(s). match ( x, y ) \\(\\quad\\) x = vector of criteria \\(\\quad\\) y = vector of elements to look in Let’s look at the sample vector. my.df$dept ## [1] &quot;D1&quot; &quot;D2&quot; &quot;D2&quot; &quot;D2&quot; &quot;D3&quot; &quot;D2&quot; &quot;D1&quot; &quot;D1&quot; &quot;D2&quot; &quot;D2&quot; Find the position of one criteria. match( &#39;D2&#39;, my.df$dept ) ## [1] 2 Find the positions of multiple criterias. match( c(&#39;D1&#39;,&#39;D2&#39;,&#39;D3&#39;), my.df$dept ) ## [1] 1 2 5 "],
["6-4-ordering-data.html", "6.4 Ordering Data", " 6.4 Ordering Data The key idea of ordering data is to produce an ‘order’ vector representing the position of the elements. Then apply the order list to vector/data.frame to produce sorted result. order make ordering based on a numeric vector. The order result can be applied to vector or data.frame. order ( x, x must be numeric number \\(\\quad\\) decreasing = FALSE, ascending or descending \\(\\quad\\) na.last = TRUE) if TRUE, NA value are put last 6.4.1 Ordering Vector Let’s look at the sample vector. my.vector ## [1] 46 27 31 51 19 76 21 26 100 81 56 65 32 63 33 51 68 Create the order. the.order = order (my.vector, decreasing=T) the.order ## [1] 9 10 6 17 12 14 11 4 16 1 15 13 3 2 8 7 5 Apply the order on vector. my.vector[ the.order ] ## [1] 100 81 76 68 65 63 56 51 51 46 33 32 31 27 26 21 19 6.4.2 Ordering Data Frame We want to order a data.frame based on the values in one or more columns. Here is the data.frame example. head(my.df) ## dept value1 ## 1 D1 52.53028 ## 2 D2 47.12630 ## 3 D2 47.26684 ## 4 D2 47.17774 ## 5 D3 45.54981 ## 6 D2 47.61404 6.4.2.1 One Level Ordering Order the data.frame based on one column - dept. the.order = order(my.df$dept, decreasing=TRUE) head( my.df[the.order,] ) ## dept value1 ## 5 D3 45.54981 ## 2 D2 47.12630 ## 3 D2 47.26684 ## 4 D2 47.17774 ## 6 D2 47.61404 ## 9 D2 50.32229 6.4.2.2 Multi Levels Ordering Order the data.frame based on two columns. However, all columns follow the same decreasing (ascending/desceding). attach(my.df) the.order = order(dept, value1, decreasing=TRUE) head( my.df[the.order,] ) detach(my.df) ## dept value1 ## 5 D3 45.54981 ## 10 D2 54.79747 ## 9 D2 50.32229 ## 6 D2 47.61404 ## 3 D2 47.26684 ## 4 D2 47.17774 "],
["6-5-filtering-data.html", "6.5 Filtering Data", " 6.5 Filtering Data There are two methods of filtering data: Using logical vector Using subset function Using row numbers: which() With these methods, row.names are retained in the output vector/dataframe. subset() is a general function that can be used to filter vector, matrix and data.frame logical vector method - derive a conditional criteria that produce a logical vector, then apply to element selection which() takes logical vector and return actual indices of TRUE elements. The output from which can be use for subsetting 6.5.1 Subseting Vector All methods show in below section has similar results. 6.5.1.1 Using subset() my.vector = 1:100 subset( my.vector, my.vector&gt;10 &amp; my.vector&lt;20) ## [1] 11 12 13 14 15 16 17 18 19 6.5.1.2 Using Logical Vector First, create a logical vector. my.vector = 1:100 lvector = my.vector&gt;10 &amp; my.vector&lt;20 head( lvector, 24 ) ## [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE ## [12] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE FALSE FALSE FALSE ## [23] FALSE FALSE Then apply the logical vector with selection. my.vector[ lvector ] ## [1] 11 12 13 14 15 16 17 18 19 6.5.1.3 Using Row Numbers which returns the indices of TRUE value, of a logical vector. Create a vector containing row numbers for all TRUE values. my.vector = 1:100 w = which (my.vector&gt;10 &amp; my.vector&lt;20) w ## [1] 11 12 13 14 15 16 17 18 19 Then apply the row numbers with selection. my.vector[ w ] ## [1] 11 12 13 14 15 16 17 18 19 ‘which’ has a useful form to return position of max or min value. my.vector = 1:100 which.min ( my.vector ) my.vector [ which.min(my.vector) ] ## [1] 1 ## [1] 1 which.max ( my.vector ) my.vector [ which.max(my.vector) ] ## [1] 100 ## [1] 100 6.5.2 Subseting Data Frame Subsetting data.frame is generally similar to subsetting vector, except that it uses [rows,cols] selectors. 6.5.2.1 Using subset() my.df = data.frame( x = 1:100, y = 300:201 ) head( my.df ) subset( my.df, x&lt;10 &amp; y&lt;297 ) ## x y ## 1 1 300 ## 2 2 299 ## 3 3 298 ## 4 4 297 ## 5 5 296 ## 6 6 295 ## x y ## 5 5 296 ## 6 6 295 ## 7 7 294 ## 8 8 293 ## 9 9 292 6.5.2.2 Using Logical Vector lvector = my.df$x&lt;10 &amp; my.df$y&lt;297 my.df [ lvector, ] ## x y ## 5 5 296 ## 6 6 295 ## 7 7 294 ## 8 8 293 ## 9 9 292 6.5.2.3 Using Row Numbers which returns the positions of value TRUE of a logical vector. z = which (my.df$x&lt;10 &amp; my.df$y&lt;297) my.df [ z, ] ## x y ## 5 5 296 ## 6 6 295 ## 7 7 294 ## 8 8 293 ## 9 9 292 -->"],
["7-graphic-visualization.html", "Chapter 7 Graphic Visualization", " Chapter 7 Graphic Visualization This chapter compares various method to plotting using base-R and ggplot. "],
["7-1-library-used.html", "7.1 Library used", " 7.1 Library used Loading necessary library as below: Base R library already included functions: ** hist, plot, barplot, boxplot** library(ggplot2) ## ggplot, qplot "],
["7-2-sample-data-5.html", "7.2 Sample Data", " 7.2 Sample Data This chapter uses the sample data generate with below code. The idea is to simulate two categorical-alike feature, and two numeric value feature: dept is random character between ‘D1’, ‘D2’, ‘D3’, ‘D4’ and ‘D5’ grp is random character with randomly generated ‘G1’, ‘G2’ value1 represents numeric value, normally distributed at mean 50 value2 is numeric value, normally distributed at mean 25 set.seed(1234) my.df = data.frame( dept = paste(&#39;D&#39;,sample(1:5, 100, replace = T),sep=&#39;&#39;), grp = paste(&#39;G&#39;,sample(1:2, 100, replace = T),sep=&#39;&#39;), value1 = rnorm(1:100, mean = 50, sd = 5), value2 = rnorm(1:100, mean = 20, sd = 3), stringsAsFactors = F ) head(my.df) ## dept grp value1 value2 ## 1 D1 G1 52.07262 21.45568 ## 2 D4 G2 47.62641 22.09031 ## 3 D4 G1 50.32997 20.55654 ## 4 D4 G1 47.48761 22.10220 ## 5 D5 G1 45.87001 20.93504 ## 6 D4 G1 50.83495 22.28139 "],
["7-3-histogram.html", "7.3 Histogram", " 7.3 Histogram 7.3.1 Single Dimension Data Require x as numerical data In hist, binwidth setting is not available, only breaks (number of bins) can be specified Default hist/ggplot/qplot number of bins is 30 In qplot, single x numerical variable default to histogram You can’t specify both bins/breaks and bindwidth together, as it implies each other par(mfrow=c(1,2)) hist (my.df$value1) # default breaks = 30 hist (my.df$value1, breaks=3) qplot (data = my.df, x=value1) qplot (data = my.df, x=value1, geom=&#39;histogram&#39;) qplot (data = my.df, x=value1, bins=15) ggplot(data = my.df, aes(x=value1)) + geom_histogram() # default bins = 30 ggplot(data = my.df, aes(x=value1)) + geom_histogram(bins = 15) ggplot(data = my.df, aes(x=value1)) + geom_histogram(binwidth = 10) 7.3.2 Two Dimension Data x = numerical data fill = categorica-alike data qplot (data = my.df, x=value1, fill=grp, geom=&#39;histogram&#39;) ggplot(data = my.df, aes(x=value1, fill=grp)) + geom_histogram() "],
["7-4-scatter-plot.html", "7.4 Scatter Plot", " 7.4 Scatter Plot 7.4.1 Two Dimension Data Use scatter plot to represent correlation between two numeric variables x = number, y = number qplot default to geom_point when two numerical value is supplied for x and y plot (my.df$value1, my.df$value2) qplot (data = my.df, x = value1, y = value2) qplot (data = my.df, x = value1, y = value2, geom=&#39;point&#39;) ggplot(data = my.df, aes(x=value1, y=value2)) + geom_point() 7.4.2 Two + One Dimension Data Base-R does not support extra dimension visualization In qplot/ggplot, the third dimension of data can be represented by assigning color parameter to the third variable Note that fill has not effect on scatter plot. fill should only be used for bar like chart eg. geom_hist or gem_bar plot (my.df$value1, my.df$value2) qplot (data = my.df, x = value1, y = value2, color = grp, geom=&#39;point&#39;) ggplot(data = my.df, aes(x=value1, y=value2, color = grp)) + geom_point() ggplot(data = my.df, aes(x=value1, y=value2, fill = grp)) + geom_point() "],
["7-5-bar-chart.html", "7.5 Bar Chart", " 7.5 Bar Chart 7.5.1 Single Dimension Data Use bar to repfresent frequency chart plot requre a factor to plot frequency chart barplot require conversion of vector into table for plotting qplot default to geom_bar when single categorical-alike feature is used par(mfrow=c(1,2)) plot(as.factor(my.df$dept)) barplot(table(my.df$dept)) qplot (data = my.df, x=dept) qplot (data = my.df, x=dept, geom=&#39;bar&#39;) ggplot(data = my.df, aes(x=dept)) + geom_bar() 7.5.2 Two + One Dimension Data Use fill to introduce extra variable visualizion in filling the bar Use color to have the extra variable represented with border color qplot (data = my.df, dept, fill = grp) qplot (data = my.df, x = dept, fill = grp, geom=&#39;bar&#39;) ggplot(data = my.df, aes(x = dept, fill = grp)) + geom_bar() ggplot(data = my.df, aes(x = dept, color= grp)) + geom_bar() 7.5.3 Reordering qplot (data = my.df, x=dept) qplot (data = my.df, x=dept, geom=&#39;bar&#39;) ggplot(data = my.df, aes(x=dept)) + geom_bar() 7.5.4 Positioning qplot does not support positioning For ggplot/qplot, default position is stack position = ‘dodge’ similar to position = position_dodge(), however the later is more flexible with ability to adjust overlapping level between sub-bar (default is 0.9) g = ggplot(data = my.df, aes(x=dept, fill=grp)) g + geom_bar(position=&#39;stack&#39;) # default position g + geom_bar(position=&#39;dodge&#39;) g + geom_bar(position=position_dodge()) # default 0.9 g + geom_bar(position=position_dodge(0.5)) g + geom_bar(position=position_dodge(1.0)) 7.5.5 In-Bar Text Labeling "],
["7-6-box-plot.html", "7.6 Box Plot", " 7.6 Box Plot 7.6.1 One Dimension Data In boxplot(), only single variable need to be supplied In ggplot/qplot, variable x and y is required. Variable y is the actual value, variable x is the group variable. Case of one dimension, use x=’’ when no grouping is desired boxplot(my.df$value1) qplot (data = my.df, x = &#39;&#39; , y = value1, geom=&#39;boxplot&#39;) ggplot (data = my.df, aes( x= &#39;&#39;, y=value1 )) + geom_boxplot() 7.6.2 Two Dimension Data In boxplot, use ~ to specify y~x, where x is grouping variable boxplot(data = my.df, value1~grp) qplot (data = my.df, x = grp , y = value1, geom=&#39;boxplot&#39;) ggplot (data = my.df, aes(x=grp, y=value1)) + geom_boxplot() 7.6.3 Two + One Dimension Data Extra dimension can be included in for x-axis In boxplot, use + to specify extra dimension In qplot/ggplot, use interaction to specify extra dimension boxplot(data = my.df, value1~grp+dept) qplot (data = my.df, x=interaction(grp,dept) , y=value1, geom=&#39;boxplot&#39;) ggplot (data = my.df, aes(x=interaction(grp,dept) , y=value1)) + geom_boxplot() "],
["7-7-pie-chart.html", "7.7 Pie Chart", " 7.7 Pie Chart "],
["7-8-advance.html", "7.8 Advance", " 7.8 Advance 7.8.1 Double Y-Axis Scale ## set up some fake test data time &lt;- seq(0, 72, 12) betagal.abs &lt;- c(0.05, 0.18, 0.25, 0.31, 0.32, 0.34, 0.35) cell.density &lt;- c(0, 1000, 2000, 3000, 4000, 5000, 6000) ## add extra space to right margin of plot within frame par(mar = c(5, 4, 4, 6) + 0.5) ## Plot first set of data and draw its axis plot(time, betagal.abs, pch = 16, axes = FALSE, ylim = c(0, 1), xlab = &quot;&quot;, ylab = &quot;&quot;, type = &quot;b&quot;, col = &quot;black&quot;, main = &quot;Mike&#39;s test data&quot;) axis(2, ylim = c(0, 1), col = &quot;black&quot;, las = 1) ## las=1 makes horizontal labels mtext(&quot;Beta Gal Absorbance&quot;, side = 2, line = 2.5) box() ## Allow a second plot on the same graph par(new = TRUE) ## Plot the second plot and put axis scale on right plot(time, cell.density, pch = 15, xlab = &quot;&quot;, ylab = &quot;&quot;, ylim = c(0, 7000), axes = FALSE, type = &quot;b&quot;, col = &quot;red&quot;) ## a little farther out (line=4) to make room for labels mtext(&quot;Cell Density&quot;, side = 4, col = &quot;red&quot;, line = 4) axis(4, ylim = c(0, 7000), col = &quot;red&quot;, col.axis = &quot;red&quot;, las = 1) ## Draw the time axis axis(1, pretty(range(time), 10)) mtext(&quot;Time (Hours)&quot;, side = 1, col = &quot;black&quot;, line = 2.5) ## Add Legend legend(&quot;topleft&quot;, legend = c(&quot;Beta Gal&quot;, &quot;Cell Density&quot;), text.col = c(&quot;black&quot;, &quot;red&quot;), pch = c(16, 15), col = c(&quot;black&quot;, &quot;red&quot;)) -->"],
["8-statistics.html", "Chapter 8 Statistics ", " Chapter 8 Statistics "],
["8-1-sample-data-6.html", "8.1 Sample Data", " 8.1 Sample Data This chapter uses the sample data generate with below code. The idea is to simulate two categorical-alike feature, and two numeric value feature: dept is random character between ‘D1’, ‘D2’, ‘D3’, ‘D4’ and ‘D5’ grp is random character with randomly generated ‘G1’, ‘G2’ value1 represents numeric value, normally distributed at mean 50 value2 is numeric value, normally distributed at mean 25 is.matrix(state.x77) str(state.x77) dimnames(state.x77)[1] dimnames(state.x77)[2] head(state.x77) ## [1] TRUE ## num [1:50, 1:8] 3615 365 2212 2110 21198 ... ## - attr(*, &quot;dimnames&quot;)=List of 2 ## ..$ : chr [1:50] &quot;Alabama&quot; &quot;Alaska&quot; &quot;Arizona&quot; &quot;Arkansas&quot; ... ## ..$ : chr [1:8] &quot;Population&quot; &quot;Income&quot; &quot;Illiteracy&quot; &quot;Life Exp&quot; ... ## [[1]] ## [1] &quot;Alabama&quot; &quot;Alaska&quot; &quot;Arizona&quot; &quot;Arkansas&quot; ## [5] &quot;California&quot; &quot;Colorado&quot; &quot;Connecticut&quot; &quot;Delaware&quot; ## [9] &quot;Florida&quot; &quot;Georgia&quot; &quot;Hawaii&quot; &quot;Idaho&quot; ## [13] &quot;Illinois&quot; &quot;Indiana&quot; &quot;Iowa&quot; &quot;Kansas&quot; ## [17] &quot;Kentucky&quot; &quot;Louisiana&quot; &quot;Maine&quot; &quot;Maryland&quot; ## [21] &quot;Massachusetts&quot; &quot;Michigan&quot; &quot;Minnesota&quot; &quot;Mississippi&quot; ## [25] &quot;Missouri&quot; &quot;Montana&quot; &quot;Nebraska&quot; &quot;Nevada&quot; ## [29] &quot;New Hampshire&quot; &quot;New Jersey&quot; &quot;New Mexico&quot; &quot;New York&quot; ## [33] &quot;North Carolina&quot; &quot;North Dakota&quot; &quot;Ohio&quot; &quot;Oklahoma&quot; ## [37] &quot;Oregon&quot; &quot;Pennsylvania&quot; &quot;Rhode Island&quot; &quot;South Carolina&quot; ## [41] &quot;South Dakota&quot; &quot;Tennessee&quot; &quot;Texas&quot; &quot;Utah&quot; ## [45] &quot;Vermont&quot; &quot;Virginia&quot; &quot;Washington&quot; &quot;West Virginia&quot; ## [49] &quot;Wisconsin&quot; &quot;Wyoming&quot; ## ## [[1]] ## [1] &quot;Population&quot; &quot;Income&quot; &quot;Illiteracy&quot; &quot;Life Exp&quot; &quot;Murder&quot; ## [6] &quot;HS Grad&quot; &quot;Frost&quot; &quot;Area&quot; ## ## Population Income Illiteracy Life Exp Murder HS Grad Frost ## Alabama 3615 3624 2.1 69.05 15.1 41.3 20 ## Alaska 365 6315 1.5 69.31 11.3 66.7 152 ## Arizona 2212 4530 1.8 70.55 7.8 58.1 15 ## Arkansas 2110 3378 1.9 70.66 10.1 39.9 65 ## California 21198 5114 1.1 71.71 10.3 62.6 20 ## Colorado 2541 4884 0.7 72.06 6.8 63.9 166 ## Area ## Alabama 50708 ## Alaska 566432 ## Arizona 113417 ## Arkansas 51945 ## California 156361 ## Colorado 103766 set.seed(1234) my.df = data.frame( dept = paste(&#39;D&#39;,sample(1:5, 100, replace = T),sep=&#39;&#39;), grp = paste(&#39;G&#39;,sample(1:2, 100, replace = T),sep=&#39;&#39;), value1 = rnorm(1:100, mean = 50, sd = 5), value2 = rnorm(1:100, mean = 20, sd = 3), stringsAsFactors = T ) head(my.df) ## dept grp value1 value2 ## 1 D1 G1 52.07262 21.45568 ## 2 D4 G2 47.62641 22.09031 ## 3 D4 G1 50.32997 20.55654 ## 4 D4 G1 47.48761 22.10220 ## 5 D5 G1 45.87001 20.93504 ## 6 D4 G1 50.83495 22.28139 "],
["8-2-descriptive-summary.html", "8.2 Descriptive Summary", " 8.2 Descriptive Summary 8.2.1 Single Vector summary provides min, max, quantiles, mean for numerical vector. But it doesn’t provide standard deviation. Other functions below take single vector as input, and output a single value summary(my.df$value1) mean (my.df$value1) max (my.df$value1) median (my.df$value1) sd (my.df$value1) # standard deviation var (my.df$value1) # variance length (my.df$value1) # number of elements ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 35.72 47.20 50.16 50.21 53.14 65.22 ## [1] 50.20622 ## [1] 65.21883 ## [1] 50.16402 ## [1] 5.160937 ## [1] 26.63527 ## [1] 100 8.2.2 Multiple Vectors summary can be used for multiple columns in a data frame, which each columns is evaluated For factor data, summary provides frequency count For individual functions (mean, max, min, sd, var) that take only single vecotr and output single value, use sapply to provide calculation for multiple columns of a dataframe summary (my.df) sapply (my.df[,3:4], min) sapply (my.df[,3:4], max) sapply (my.df[,3:4], median) sapply (my.df[,3:4], sd) sapply (my.df[,3:4], var) ## dept grp value1 value2 ## D1:25 G1:48 Min. :35.72 Min. :10.30 ## D2:26 G2:52 1st Qu.:47.20 1st Qu.:18.87 ## D3:17 Median :50.16 Median :20.83 ## D4:17 Mean :50.21 Mean :20.46 ## D5:15 3rd Qu.:53.14 3rd Qu.:22.05 ## Max. :65.22 Max. :28.76 ## value1 value2 ## 35.72121 10.30054 ## value1 value2 ## 65.21883 28.75742 ## value1 value2 ## 50.16402 20.83363 ## value1 value2 ## 5.160937 2.880463 ## value1 value2 ## 26.635268 8.297068 8.2.3 Custom Function Custom function can be built to accept single vector and return single vector Use sapply with the custom function to sweep through multiple columns in a dataframe and return a matrix (with row and col names) as a result mystats = function(x, na.omit=FALSE){ if (na.omit) x =x[!is.na(x)] m = mean(x) med = median(x) v = var(x) s = sd(x) n = length(x) skew = sum((x-m)^3/s^3)/n kurt = sum((x-m)^4/s^4)/n - 3 return(c(length=n, mean=m, median=med, stdev=s, skew=skew, kurtosis=kurt)) } sapply(my.df[,3:4], mystats) ## value1 value2 ## length 100.00000000 100.0000000 ## mean 50.20621590 20.4638110 ## median 50.16401641 20.8336304 ## stdev 5.16093675 2.8804631 ## skew -0.01832526 -0.2217617 ## kurtosis 0.52800386 1.0046773 "],
["8-3-t-test.html", "8.3 T-Test", " 8.3 T-Test "],
["8-4-covariance-correlation.html", "8.4 Covariance / Correlation", " 8.4 Covariance / Correlation If two variables are independent, their covariance/correlation is 0. But, having a covariance/correlation of 0 does not imply the variables are independent. 8.4.1 Covariance \\[Pearson - Cov(X,Y)= \\frac{\\sum_{i=1}^n (X_i-\\bar{X})*(Y_i-\\bar{Y})}{n-1}\\] Covariance doesn’t really tell you about the strength of the relationship between the two variables. - A large covariance can simply means the variables are made of large numbers, doesn’t means that the relation are strong. Hence correlation (scaled covariance) is a better indicator of the relation strenght. 8.4.2 Correlation \\[Pearson-Cor(X,Y)= \\frac{Cov(X,Y)}{sd(X)sd(Y)}\\] Correlation is a scaled version of covariance that takes on values between -1 and 1 Correlation are used to measure the strength of relationship among linearly related quntitative variables (numerical) 0 indicates no correlation. +1 and -1 indicates perfect correlation cor(x, y, use= , method= ) \\(\\quad\\) x= matrix or dataframe \\(\\quad\\) y= matrix or dataframe, default = x \\(\\quad\\) method= pearson, spearman, kendall, default is pearson \\(\\quad\\) use= everthing:missing value will set to missing, complete.obs:listwise deletion, pairwise.complete.obs:pairwise deletion If y is not specified, you get cross matrices by default (all variables crossed with all other variables). states = state.x77[,1:5] cor(states) cor(states, method = &#39;kendall&#39;) ## Population Income Illiteracy Life Exp Murder ## Population 1.00000000 0.2082276 0.1076224 -0.06805195 0.3436428 ## Income 0.20822756 1.0000000 -0.4370752 0.34025534 -0.2300776 ## Illiteracy 0.10762237 -0.4370752 1.0000000 -0.58847793 0.7029752 ## Life Exp -0.06805195 0.3402553 -0.5884779 1.00000000 -0.7808458 ## Murder 0.34364275 -0.2300776 0.7029752 -0.78084575 1.0000000 ## Population Income Illiteracy Life Exp Murder ## Population 1.00000000 0.08408163 0.2123063 -0.06865555 0.2364983 ## Income 0.08408163 1.00000000 -0.1970811 0.21904389 -0.1448450 ## Illiteracy 0.21230629 -0.19708113 1.0000000 -0.42852098 0.5155359 ## Life Exp -0.06865555 0.21904389 -0.4285210 1.00000000 -0.5997547 ## Murder 0.23649826 -0.14484495 0.5155359 -0.59975465 1.0000000 If x and y are specified, you can produce non squared correlation matrices with only the variables specified for both x and y axis cor(states[,1:5], states[,3:5]) ## Illiteracy Life Exp Murder ## Population 0.1076224 -0.06805195 0.3436428 ## Income -0.4370752 0.34025534 -0.2300776 ## Illiteracy 1.0000000 -0.58847793 0.7029752 ## Life Exp -0.5884779 1.00000000 -0.7808458 ## Murder 0.7029752 -0.78084575 1.0000000 8.4.3 Correlation Test for significance From the cor function, we know that Murder rate and Illiteracy are highly correlated (&gt;0.7). However, is this merely by chance, or it is statistically significant that there are indeed correlated ? To answer this question, we need to perform hypotesis testing: \\(H_0\\) : (population) correlation betwen Murder rate and Illiteracy is zero \\(H_1\\) : (sample) correlation between Murder rate and Illiteracy is not zero We then test our sample data using cor.test to find out the p-value: If p-value &lt; 0.025 (two sided test), means \\(H_1\\) is significant, therefore reject \\(H_0\\). If p-value &gt; 0.025 (two sided test), means \\(H_0\\) is significant, therefore accept \\(H_0\\). cor.test(x, y, method = , alternative= , conf.level= ) \\(\\quad\\) x= vector 1 \\(\\quad\\) y= vector 2 \\(\\quad\\) method= pearson (default), spearman, kendall \\(\\quad\\) alternative= two.sided (default), less, more \\(\\quad\\) conf.level = 0.95(default), any value between 0 to 1 cor.test (states[,&#39;Murder&#39;], states[,&#39;Illiteracy&#39;]) ## ## Pearson&#39;s product-moment correlation ## ## data: states[, &quot;Murder&quot;] and states[, &quot;Illiteracy&quot;] ## t = 6.8479, df = 48, p-value = 1.258e-08 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.5279280 0.8207295 ## sample estimates: ## cor ## 0.7029752 If \\(H_0\\) is true, then chance of observing the sample data (with correlation of 0.7) is 1.258e-08 (too low to be true). Hence we reject \\(H_0\\), and accept \\(H_1\\) that there is indeed a significant correlation between the variables. -->"],
["9-clustering-analysis.html", "Chapter 9 Clustering Analysis", " Chapter 9 Clustering Analysis Cluster analysis is a data-reduction technique designed to uncover subgroups of observations within a dataset It reduce a large number of observations to a much smaller number of clusters or types A cluster is defined as a group of observations that are more similar to each other than they are to the observations in other groups "],
["9-1-library-4.html", "9.1 Library", " 9.1 Library Below summarizes all packages, their functions and purposes used in this Clustering Analysis chapter. Package Function Purpose 1 Base - R dist Calculate distance between data points with methods: euclidean, maximum, cenberra, minkowski, manhattan scale Scale data (minus mean, div by SD) hclust Build hirerchical cluster model (no cutting) kmeans Build k-means cluster model 2 factoextra fviz_nbclust Optimum number of cluster (K) visual analysis, methods: wss, silhoutte hcut Build hirerchical cluster model (with cutting) fviz_dend Visualize h-cluster model in dendrogram graph as ggplot object fviz_cluster Visualize data points with cluster grouping as ggplot object 3 NbClust NbClust 30 indices to analyze optimal number of cluster, K 4 flexclust randIndex Agreement measure for two cluster results "],
["9-2-application.html", "9.2 Application", " 9.2 Application Finding groups within data and distinguishing observation in your data set that are similar from those that are different. The objective is to find the right balance between similarities and differences. On the one hand we want to treat similar cases in a similar way to benefit from economies of scale. Finding groups within data will allow you to allocate your effort more efficiently: Leveraging synergies between cases that are similar Allocating specific case houses to different cases if needed in order to maximize your effectiveness Example Usages are: Business : researchers use cluster analysis for customer segmentation. Customers are arranged into clusters based on the similarity of their demographics and buying behaviours. Marketing campaings are then tailored to appeal to the groups Human Resource : you’re concerned by the number of employees leaving the firm. As an HR manager, you want to retain your best employees within the company but you cannot follow-up with each one of them too often as that would be very time consuming. Instead, you may rely on an HR analytic solution to understand what are the most frequent situations explaining why an employee decides to leave, with variables like number of completed projects for the past 12 months/utilization, age, last project evaluation, time spent in company, position, number of newborn, etc. Psychological: researchers cluster data on the symptoms and demographics of depressed patients, seeking to uncover subtypes of depression, with the hope of finding more effective targeted treatments and a better understanding of the disorder. Medical: researchers use cluster analysis to help catalog gene-expression patterns obtained from DNA microarray data. This can help them to understand normal growth and development and the underlying causes of many human diseases Information Retrieval: The world wide web consists of billions of Web pages, and the results of a query to a search engine can return thousands of pages. Clustering can be used to group these search results into a small number of clusters, each of which captures a particular aspect of the query. For example, a query of “movie” might return Web pages grouped into categories such as reviews, trailers, starts and theaters. Each category (Cluster) can be bnorken into subcategories (sub-clusters), producing a hierachical structure that further assists a user’s exploration of the query results. "],
["9-3-sample-data-7.html", "9.3 Sample Data", " 9.3 Sample Data Sample data used in this chapter emulate two dimensional data points with three groups with clear grouping when visualize. set.seed(1234) my.df = data.frame( id = paste(&#39;ID_&#39;, 1:15, sep = &#39;&#39;), grp = c(rep(&#39;G1&#39;, 5), rep(&#39;G2&#39;, 5), rep(&#39;G3&#39;, 5)), value1 = c( round(rnorm(5, mean = 10, sd = 3)), round(rnorm(5, mean = 10, sd = 3)), round(rnorm(5, mean = 30, sd = 3))), value2 = c( round(rnorm(5, mean = 10, sd = 3)), round(rnorm(5, mean = 20, sd = 3)), round(rnorm(5, mean = 20, sd = 3))), stringsAsFactors = F ) rownames(my.df) = my.df[,1] str(my.df) plot(my.df$value1, my.df$value2) ## &#39;data.frame&#39;: 15 obs. of 4 variables: ## $ id : chr &quot;ID_1&quot; &quot;ID_2&quot; &quot;ID_3&quot; &quot;ID_4&quot; ... ## $ grp : chr &quot;G1&quot; &quot;G1&quot; &quot;G1&quot; &quot;G1&quot; ... ## $ value1: num 6 11 13 3 11 12 8 8 8 7 ... ## $ value2: num 10 8 7 7 17 20 19 19 21 18 ... "],
["9-4-general-steps.html", "9.4 General Steps", " 9.4 General Steps Choose appropriate attributes This is the most important steps. Choose attributes that that actions can be taken upon A sohisticated cluster analysis can’t compensate for a poor choice of variables Scale Data When NOT to scale If you have attributes with a well-defined meaning. Say, latitude and longitude, then you should not scale your data, because this will cause distortion When To Scale If you have mixed numerical data, where each attribute is something entirely different (say, shoe size and weight), has different units attached (lb, tons, m, kg …) then these values aren’t really comparable anyway; z-standardizing them is a best-practise to give equal weight to them If variables vary in range, then the variable with the largest value will have the greatest impact on result. This is undesirable Therefore data must be scaled so that they can be compared fairly Methods of Scaling Popular scaling methods are: Normalize to mean=0 and sd=1 Divide by Max Minus min, divide by Min-Max range Screen for Outliers Outliers can distort results. Screen to remove them Calculate Data Point Distances Popular measure of distance between two data point is Euclidean distance Others are Manhattan, Canberra, Asymmetric Binary, Maximum and Minkowski also available Chosoe a Clustering Alrorithm, and Inter-Custer Distance Method Try few Clustering Solutions Decide the best clustering algorithm, cluster distance method and number of cluster, K Use NbClus as a tool to guide choosing K (number of cluster) Visualize the result Visualization can help you determine the meaning and usefulness of the cluster solution Hierarchical clustering are usually presented as a dendrogram Partitioning results are typically visualized using a bivariate cluster plot Intepret the Cluster Once a cluster solution has been obtained, you must interpret (and possibly name) the clusters What do the observations in a cluster have in common? How do they differ from the observations in other clusters? This step is typically accomplished by obtaining summary statistics for each variable by cluster For continuous data, the mean or median for each variable within each cluster is calculated. For mixed data (data that contain categorical variables), the summary statistics will also include modes or category distributions Validate Result Validating the cluster solution involves asking the question: “Are these groupings in some sense real, and not a manifestation of unique aspects of this dataset or statistical technique?” If a different cluster method or different sample is employed, would the same clusters be obtained? If actual grouping data is known, run randIndex to measure the degree of agreement The fpc, clv, and clValid packages each contain functions for evaluating the stability of a clustering solution (not discussed here) "],
["9-5-distance-algorithm.html", "9.5 Distance Algorithm", " 9.5 Distance Algorithm The choice of an appropriate metric will influence the shape of the clusters, as some elements may be close to one another according to one distance and farther away according to another. For example, in a two dimensional data, the distance between the point (1,1) and the origin (0,0) can be 2 under Manhattan distance, \\(\\sqrt{2}\\) under Euclidean distance, or 1 under Maximum distance. dist is used to measure distance for all numeric elements in dataframe or matrix. Supplying non-numeric columns for dist will incur warning. dist( x, method = ) default method = 'euclidean' \\(\\quad\\) method = 'euclidean', &quot;maximum&quot;, &quot;manhattan&quot;, &quot;canberra&quot;, &quot;binary&quot; or &quot;minkowski&quot; 9.5.1 Euclidean Distance \\[Euclidean-d(p,q) = \\sqrt{\\sum_{i=1}^n (p_i-q_i)^2} \\quad,n = dimension\\] The Euclidean distance is a distance measure between two points or or vectors in a two- or multidimensional (Euclidean) space based on Pythagoras’ theorem The distance is calculated by taking the square root of the sum of the squared pair-wise distances of every dimension Below command measures distance for numeric columns of all data points in my.df, using euclidean algorithmn. data.scaled = scale(my.df[,3:4]) # Z-Score scaling d.euclidean = dist( data.scaled ) # Euclidean distance round (d.euclidean,1) ## ID_1 ID_2 ID_3 ID_4 ID_5 ID_6 ID_7 ID_8 ID_9 ID_10 ID_11 ID_12 ID_13 ## ID_2 0.6 ## ID_3 0.9 0.3 ## ID_4 0.6 0.8 1.0 ## ID_5 1.4 1.7 1.9 2.1 ## ID_6 2.0 2.3 2.5 2.6 0.6 ## ID_7 1.7 2.1 2.4 2.4 0.5 0.4 ## ID_8 1.7 2.1 2.4 2.4 0.5 0.4 0.0 ## ID_9 2.1 2.5 2.7 2.7 0.8 0.4 0.4 0.4 ## ID_10 1.5 2.0 2.2 2.1 0.4 0.6 0.2 0.2 0.6 ## ID_11 2.5 2.3 2.3 3.0 1.7 1.8 2.1 2.1 2.2 2.1 ## ID_12 3.1 3.1 3.2 3.7 1.8 1.5 1.9 1.9 1.8 2.1 1.2 ## ID_13 2.5 2.4 2.4 3.1 1.6 1.6 1.9 1.9 2.1 2.0 0.2 1.0 ## ID_14 3.0 2.9 3.0 3.6 1.9 1.7 2.1 2.1 2.1 2.2 0.8 0.5 0.6 ## ID_15 2.9 2.7 2.7 3.5 2.1 2.1 2.4 2.4 2.5 2.5 0.4 1.1 0.5 ## ID_14 ## ID_2 ## ID_3 ## ID_4 ## ID_5 ## ID_6 ## ID_7 ## ID_8 ## ID_9 ## ID_10 ## ID_11 ## ID_12 ## ID_13 ## ID_14 ## ID_15 0.6 9.5.2 Manhattan Distance \\[Manhattan - d(p,q) = \\sum_{i=1}^n |p_i-q_i| \\quad,n = dimension\\] The Manhattan distance (sometimes also called Taxicab distance) metric is related to the Euclidean distance But instead of calculating the shortest diagonal path (“beeline”) between two points, it calculates the distance based on gridlines Below command measures distance for numeric columns of all data points in my.df, using manhattan algorithm. data.scaled = scale(my.df[,3:4]) # Z-Score scaling d.manhattan = dist( data.scaled, method=&#39;manhattan&#39;) round (d.manhattan, 1) ## ID_1 ID_2 ID_3 ID_4 ID_5 ID_6 ID_7 ID_8 ID_9 ID_10 ID_11 ID_12 ID_13 ## ID_2 0.9 ## ID_3 1.2 0.4 ## ID_4 0.9 1.0 1.0 ## ID_5 1.8 1.7 2.1 2.7 ## ID_6 2.5 2.4 2.6 3.4 0.7 ## ID_7 1.9 2.4 2.8 2.8 0.7 0.6 ## ID_8 1.9 2.4 2.8 2.8 0.7 0.6 0.0 ## ID_9 2.3 2.8 3.2 3.2 1.1 0.6 0.4 0.4 ## ID_10 1.6 2.3 2.7 2.5 0.6 0.9 0.3 0.3 0.7 ## ID_11 3.3 3.3 3.3 4.2 1.9 2.4 2.6 2.6 3.0 2.5 ## ID_12 4.3 4.2 4.2 5.2 2.5 1.8 2.4 2.4 2.0 2.7 1.3 ## ID_13 3.4 3.4 3.4 4.3 1.6 2.1 2.3 2.3 2.7 2.2 0.3 1.1 ## ID_14 4.2 4.1 4.1 5.1 2.4 1.7 2.3 2.3 2.3 2.6 0.9 0.7 0.8 ## ID_15 3.9 3.8 3.8 4.8 2.1 2.6 2.8 2.8 3.2 2.7 0.6 1.5 0.5 ## ID_14 ## ID_2 ## ID_3 ## ID_4 ## ID_5 ## ID_6 ## ID_7 ## ID_8 ## ID_9 ## ID_10 ## ID_11 ## ID_12 ## ID_13 ## ID_14 ## ID_15 0.9 9.5.3 Maximum Distance \\[d(x,y)= sup|x_j - y_j|, 1≤ j ≤ d\\] 9.5.4 Canberra Distance \\[\\sum_{j=1}^{d}|x_j - y_j|) / (|x_j|+|y_j|)\\] 9.5.5 Minkowski Distance The Minkowski distance is a generalized form of the Euclidean distance (if m=2) and the Manhattan distance (if m=1). \\[\\left(\\sum_{i=1}^n |p_i-q_i|^p\\right)^{1/m}\\] "],
["9-6-optimum-number-of-clusters-k.html", "9.6 Optimum Number of Clusters (K)", " 9.6 Optimum Number of Clusters (K) There are three (3) popular methods for determining the optimal number of clusters. Elbow Method Applicable for partioning clustering, such as k-means Average Silhoutte Method Gap Statistics (not discussed here) There is no guarantee that they will agree with each other. In fact, they probably won’t. However, use this as a guidine and test few highest criteria score to determinee final number of cluster. 9.6.1 Elbow Method 9.6.1.1 Elbow Concept The objective of partitioning clustering (such as K-Mean) is to define clusters such that the total intra-cluster variation (known as total within-cluster variation or total within-cluster sum of square, wss) is minimized. 9.6.1.2 Elbow Algorithm Run K-mean clustering algorithm for K=1 to n For each K, calculate the within-cluster-sum-of-square (wss) Plot the curve of wss against the number of clusters K The location of bend (knee) in the plot is generally considered as the indicator of the appropriate number of clusters When the WSS value stop decreasing significantly (at the knee), then the number of clusters probably had reached its optimum. Although this approach is heuristic, it still provide a good guideline for K selection. 9.6.1.3 Elbow Codes (for K-mean) - Do It Yourself! The method presented here does not require any external library ! However, it requires writing a funciton to calculate WSS and plot the results. Define the The Algorithmn # Algorithmn: Compute k-means and plot wss for k=2 to k=15 wssplot = function(data, nc=15, seed=1234){ wss &lt;- (nrow(data)-1)*sum(apply(data,2,var)) for (i in 2:nc) { set.seed(seed) wss[i] &lt;- sum(kmeans(data, centers=i)$withinss) } plot(1:nc, wss, type=&quot;b&quot;, xlab=&quot;Number of Clusters (K)&quot;, ylab=&quot;Total Within Groups Sum of Squares&quot;) wss } Run The Code If number of observations &lt;=nc(default 15), specify smaller nc. wssplot(data.scaled, nc=8) abline(v=3, lty=2) # mark the optimum K after facts ## [1] 28.000000 12.865178 2.467904 2.191659 2.038777 0.785887 1.396947 ## [8] 1.030988 The wssplot above indicates that there is a distinct drop in the within-groups sum of squares when moving from 1 to 3 clusters. After three clusters, this decrease drops off, suggestign that a three-cluster solution may be a good fit to the data. 9.6.1.4 Elbow Codes - using factoextra::fviz_nbclust,hcut factoextra combined functions to calculate ‘silhoutte’ and output ggplot object For k-mean wss analysis, kmeans helper function from base-R is required For pam wss analysis, cluster:pam helper function is required For h-cluster wss analysis, hcut helper function by its own library is used. Somehow base-R hclust is not supproted library(factoextra) library(cluster) fviz_nbclust(data.scaled, kmeans, method = &quot;wss&quot;) + labs(subtitle=&#39;kmeans&#39;) fviz_nbclust(data.scaled, pam, method = &quot;wss&quot;) + labs(subtitle=&#39;pam&#39;) fviz_nbclust(data.scaled, hcut, method = &quot;wss&quot;) + labs(subtitle=&#39;hcut&#39;) + geom_vline(xintercept = 3, linetype = 2) 9.6.2 Average Silhoutte Method 9.6.2.1 Average Silhoutte Concept Average silhouette method computes the average silhouette of observations for different values of k. The optimal number of clusters k is the one that maximize the average silhouette over a range of possible values for k (Kaufman and Rousseeuw [1990]). Silhouette analysis can be used to study the separation distance between the resulting clusters. The silhouette plot displays a measure of how close each point in one cluster is to points in the neighboring clusters and thus provides a way to assess parameters like number of clusters visually. This measure has a range of [-1, 1]. Silhouette coefficients (as these values are referred to as) near +1 indicate that the sample is far away from the neighboring clusters. A value of 0 indicates that the sample is on or very close to the decision boundary between two neighboring clusters and negative values indicate that those samples might have been assigned to the wrong cluster. 9.6.2.2 Average Silhoutte Algorithm Compute clustering algorithm (e.g., k-means clustering) for different values of k For each k, calculate the average silhouette of observations (avg.sil) Plot the curve of avg.sil according to the number of clusters k The location of the maximum is considered as the appropriate number of clusters 9.6.2.3 Average Silhoutte Code - factoextra:fviz_nbclust Example code below shows silhoute analysis for kmeans, pam and h-cluster: factoextra combined functions to calculate ‘silhoutte’ and output ggplot object For k-mean silhoutte analysis, kmeans helper function from base-R is required For pam silhoutte analysis, cluster:pam helper function is required For h-cluster silhoutte analysis, hcut helper function by its own library is used. Somehow base-R hclust is not supproted library(factoextra) library(cluster) fviz_nbclust(data.scaled, kmeans, method = &quot;silhouette&quot;) + labs(subtitle=&#39;kmeans&#39;) fviz_nbclust(data.scaled, pam, method = &quot;silhouette&quot;) + labs(subtitle=&#39;pam&#39;) fviz_nbclust(data.scaled, hcut, method = &quot;silhouette&quot;) + labs(subtitle=&#39;hcut&#39;) 9.6.3 NbClust Package (with 30 Indices) NbClust package offers numerous 26 indices for determining the best number of clusters in a cluster analysis. library(&#39;NbClust&#39;) Multiple indices are computed simultaneously - a clear advantage Paramter index='all' will utilize all indices to evaluate the optimum number of clusters Nbclust returns a list that contains all evaluation statistic based on the indices used Results of the evaluation is stored in Best.nc vector Using table and barplot is best way to visualize the result of best K Supported Indices are kl, ch, hartigan, ccc, scott, marriot, trcovw, tracew, friedman, rubin, cindex, db, silhouette, duda, pseudot2, beale, ratkowsky, ball, ptbiserial, gap, frey, mcclain, gamma, gplus, tau, dunn, hubert, sdindex, dindex, sdbw ‘all’ (all indices except GAP, Gamma, Gplus and Tau) ‘alllong’ (all indices with Gap, Gamma, Gplus and Tau included) NbClust( data=, diss=NULL, distance='euclidean', min.nc=2, max.nc=15, method=NULL, index='all', alphaBeale=0.1) \\(\\quad\\) data = matrix or dataframe \\(\\quad\\) diss = dissimilarity matrix, if not NULL, then distance should be NULL \\(\\quad\\) distance = &quot;euclidean&quot;, &quot;maximum&quot;, &quot;manhattan&quot;, &quot;canberra&quot;, &quot;binary&quot;, &quot;minkowski&quot; or &quot;NULL&quot; \\(\\quad\\) min.nc = minimum number of clusters \\(\\quad\\) max.nc = maximum number of clusters \\(\\quad\\) method = &quot;ward.D&quot;, &quot;ward.D2&quot;, &quot;single&quot;, &quot;complete&quot;, &quot;average&quot;, &quot;mcquitty&quot;, &quot;median&quot;, &quot;centroid&quot;, &quot;kmeans&quot; \\(\\quad\\) index = 'all' to use all indices for evaluation NbClust output an object with below values: Best.nc : Best number of clusters proposed by each index and the corresponding index value Best.partition: vector of cluster group for every observation 9.6.3.1 Run The Code Run NbClust for average (h-clustering) and kmeans method. nbc.hclust = NbClust(data.scaled, distance=&quot;euclidean&quot;, min.nc=2, max.nc=8, method=&quot;average&quot;) nbc.kmeans = NbClust(data.scaled, min.nc=2, max.nc=8, method=&quot;kmeans&quot;) 9.6.3.2 Visualize The Result Visualize using Base-R As output Best.nc[1,] shows, majority indices favor three (3) clusters. table( nbc.hclust$Best.n[1,] ) barplot( table(nbc.hclust$Best.n[1,] ), xlab=&quot;Numer of Clusters&quot;, ylab=&quot;Number of Criteria&quot;, main=&quot;Number of Clusters Chosen by 26 Criteria\\nh-cluster&quot;) table( nbc.kmeans$Best.nc[1,] ) barplot( table(nbc.kmeans$Best.nc[1,] ), xlab=&quot;Numer of Clusters&quot;, ylab=&quot;Number of Criteria&quot;, main=&quot;Number of Clusters Chosen by 26 Criteria\\nkmeans&quot;) ## ## 0 1 2 3 5 6 7 8 ## 2 1 2 14 2 1 2 2 ## ## 0 1 2 3 5 7 8 ## 2 1 2 15 1 1 4 Visualize using factoextra::fviz_nbclust() Single function fviz_nbclust() from factoextra library will use value in NbClust object to visualize the optimal cluster number. fviz_nbclus output ggplot object, hence can be easily customized. library(&#39;factoextra&#39;) fviz_nbclust(nbc.hclust) + labs(subtitle=&#39;H-Cluster&#39;) fviz_nbclust(nbc.kmeans) + labs(subtitle=&#39;K-Means&#39;) "],
["9-7-clustering-algorithm-compared.html", "9.7 Clustering Algorithm Compared", " 9.7 Clustering Algorithm Compared Description h-cluster k-means 1 Computation Time Fast. Linear to number of observation Slow: Quadradric to number of observation 2 Initial K needed No Yes 3 Fine Tuning Experiment with different method of Linkage Experiment with different K centroids 4 Perform Well in Hierachical Nature Data Set Spherical Data Points 5 Perform Bad in Large data sets U-Shape, Outliers 6 Unique Advantages Good for hirechical discovery 7 R Library Base R, factoextra Base R "],
["9-8-hierarchical-clustering.html", "9.8 Hierarchical Clustering", " 9.8 Hierarchical Clustering Hierarchical clustering is a widely used data analysis tool The idea is to build a binary tree of the data that successively merges similar groups of points Number of clusters (K) is required as import It is an unsupervised learning 9.8.1 Clustering Algorithm This is how Hierarchical Clustering works: 1. Initially, put each data point in its own cluster 2. Calucate the distances between each cluster and all other clusters (inter-cluster distance) 3. Combine the two clusters with the smallest distance - This reduce cluster number by one 4. Repeat step (2) and (3) until all clusters have been merged into single cluster 9.8.2 Inter Cluster Distance Method Once distance for all data points has been measured, decide which of the five (5) methods below to measure distance between clusters: Single Linkage: Shortest distance among all data points betweentwo clusters Complete Linkage (common): Longest distance among all data points between two clusters Average Linkage (common): Average distance of all points between two clusters Centroid: Find the centroid of each cluster and calculate the distance between centroids between two clusters Please note that the Inter Cluster Distance Method above uses Distance Algorithmn such as ‘euclidean’, “maximum”, “manhattan”, “canberra”, “binary” or “minkowski” to calculate actual distance value. 9.8.3 Run The Code Generally, step (A), (B) and (C) are followed for H-clustering analysis. A. Before running H-Clustering Fiter Data (choose only numeric columns) Scale Data (if required) Calculate distance, using B. Performing H-Clustering Build H-Cluster model, require input of inter-cluster distance method Derive cluster by **cutting* into K number of clusters C. Visualize Display frequency, number of observations for each cluster group Plot dendrogram, superimpose cluster group into the plot 9.8.3.1 Using Base-R Utilities hclust (d, method ='complete') \\(\\quad\\) d = distance calculated using dist() \\(\\quad\\) method = 'single', 'complete', 'average', 'centroid' as cluster distance method Filter, Scale, Calculate Distance data.scaled = scale(my.df[,3:4]) # scale data d.euclidean = dist( data.scaled ) # calculate distance Build H-Cluster Model, Cutting into clusters fit.average = hclust (d.euclidean, method=&#39;average&#39;) # build the model clusters = cutree (fit.average, k=3) # derive the clusters clusters ## ID_1 ID_2 ID_3 ID_4 ID_5 ID_6 ID_7 ID_8 ID_9 ID_10 ID_11 ID_12 ## 1 1 1 1 2 2 2 2 2 2 3 3 ## ID_13 ID_14 ID_15 ## 3 3 3 Display frequency table, Visualize with dendogram, superimpose cluster group barplot( table(clusters), xlab=&#39;Cluster Group Number&#39;, ylab=&#39;No. of Observations&#39; ) plot( fit.average, main=&#39;HCluster, Average-Linkage\\n3 Clusters Solution&#39; ) rect.hclust( fit.average, k=3, border = 2:5 ) 9.8.3.2 Using factoextra Package factoextra provides single function hcut to scale, calcuate distance and cutting into cluster groups. Which is handy. library(&quot;factoextra&quot;) hcut(x, k = 2, hc_method = &quot;ward.D2&quot;, hc_metric = &quot;euclidean&quot;, stand = FALSE, graph = FALSE) \\(\\quad\\) x = matrix or dataframe \\(\\quad\\) k = number of clusters to cut \\(\\quad\\) hc_method = inter-cluster distance method: ward.D, ward.D2, single, complete, average \\(\\quad\\) hc_metric = distance calc method: euclidean, manhattan, maximum, canberra, binary, &quot;minkowski \\(\\quad\\) stand = TRUE: scale x with z-score, FALSE: not to scale x hcut output below useful values (not all displayed): \\(\\quad\\) data = original data (if stand=FALSE), scaled data (if stand=TRUE) \\(\\quad\\) nbclust = number of clusters \\(\\quad\\) cluster = cluster group number assigned for each observation \\(\\quad\\) size = frequency vector, number of observations for each cluster \\(\\quad\\) method = inter-cluster distance method applied \\(\\quad\\) dist.method = distance method applied Filter, Scale, Calculate Distance, Build H-Cluster Model, Cutting into Clusters fit.hc = hcut(my.df[,3:4], k=3, hc_method=&#39;average&#39;, hc_metric=&#39;euclidean&#39;, stand = TRUE) Display frequency table, Visualize with dendogram, superimpose cluster group barplot( table(fit.hc$cluster), xlab=&#39;Cluster Group Number&#39;, ylab=&#39;No. of Observations&#39; ) fviz_dend(fit.hc, rect = TRUE, rect_border = &#39;red&#39;, cex = 0.5, lower_rect = -0.5, horiz = T) "],
["9-9-k-mean-clustering.html", "9.9 K-Mean Clustering", " 9.9 K-Mean Clustering K-mean is the most common partitioning clustering algorithm Partitioning means data points need to be initally partioned into few clusters to start the process with The other partitioning clustering method is Medoids 9.9.1 Clustering Algorithm Define K number of centroids (data points) Cluster Assignment Each observation is assigned to the nearest centroid, using euclidean distance Update Centroids After all observations had been assigned to the centroids, a new centroids is calculated Repeat step (2) - (3) until convergence Convergence means none of the observations changed cluster membership 9.9.2 Run The Code Generally, step (A), (B) and (C) are followed for H-clustering analysis. A. Before running H-Clustering Fiter Data (choose only numeric columns) Scale Data (if required) B. Performing H-Clustering Build K-Means Cluster model, require input Number of initial centers (clusters K) K-Means Algorithmn Number of tries to seed random centers, before choosing the best model C. Visualize Display frequency, number of observations for each cluster group Plot graph, superimpose cluster group into the plot 9.9.2.1 kmeans( x, centers, nstart=1, algorithmn='Hartiga-Wong' ) \\(\\quad\\) x = matrix or dataframe \\(\\quad\\) centers = number of centroids \\(\\quad\\) nstart = how many times to randomly try seeding centroids \\(\\quad\\) algorithm = &quot;Hartigan-Wong&quot;-default, &quot;Lloyd&quot;, &quot;Forgy&quot;, &quot;MacQueen&quot; nstart=25 is a good number to use. kmeans output below useful values: cluster : cluster number for all observations centers : Centre values (of each dimensions) for each cluster withinss : Total Sum of Squares Within For Each Cluster size : Number of observations for each cluster number fit.kmeans = kmeans(data.scaled, 3, nstart = 25) fit.kmeans$cluster ## ID_1 ID_2 ID_3 ID_4 ID_5 ID_6 ID_7 ID_8 ID_9 ID_10 ID_11 ID_12 ## 3 3 3 3 1 1 1 1 1 1 2 2 ## ID_13 ID_14 ID_15 ## 2 2 2 fit.kmeans$withinss ## [1] 0.5512567 1.1226046 0.7940430 fit.kmeans$size ## [1] 6 5 4 9.9.3 Visualizing K-Mean Cluster library(factoextra) fviz_cluster( fit.kmeans, data = data.scaled, geom = &quot;point&quot;, stand = FALSE, ellipse.type = &quot;norm&quot;) "],
["9-10-agreement-with-actual-group-data.html", "9.10 Agreement With Actual Group Data", " 9.10 Agreement With Actual Group Data The adjusted Rand index provides a measure of the agreement between two sets of grouping data, adjusted for chance. It ranges from -1 (no agreement) to 1 (perfect agreement). library(&#39;flexclust&#39;) 9.10.1 Compare Cluster with Actual Data If we know the actual grouping data, we can then run this index analysis against the cluster grouping. Construct a table of cluster groups and actual groups. Then run randIndex on it to reveal the agreement measure. # cluster data generated using cutree() table(my.df$grp, clusters) randIndex( table(my.df$grp, clusters) ) ## clusters ## 1 2 3 ## G1 4 1 0 ## G2 0 5 0 ## G3 0 0 5 ## ARI ## 0.7920792 9.10.2 Compare Two Clusters We can also compare cluster data from two different clusters, eg. clusters using different algorithm. # cluster data generated using kmeans and factoextra::hcut() table(fit.kmeans$cluster, fit.hc$cluster) randIndex( table(fit.kmeans$cluster, fit.hc$cluster) ) ## ## 1 2 3 ## 1 0 6 0 ## 2 0 0 5 ## 3 4 0 0 ## ARI ## 1 It this case, both models give similar clustering result. -->"],
["10-regression-analysis.html", "Chapter 10 Regression Analysis ", " Chapter 10 Regression Analysis "],
["10-1-introduction.html", "10.1 Introduction", " 10.1 Introduction Regression is statistical process for estimating the relationships among variables: It includes many techniques for modeling and analyzing the relationship between a dependent variable and one or more independent variables More specifically, regression analysis helps one understand how the typical value of the dependent variable changes when any one of the independent variables is varied, while the other independent variables are held fixed Regression analysis estimates the conditional expectation of the dependent variable given the independent variables – that is, the average value of the dependent variable when the independent variables are fixed Regression analysis is a form of predictive modelling technique which investigates the relationship between a dependent (target) and independent variable 10.1.1 General Equation and Terminilogy A regression model relates Y to a function of \\(X\\) and \\(\\beta\\): \\(\\quad{Y\\approx f(\\mathbf {X} ,{\\boldsymbol {\\beta }})}\\) \\(\\quad \\quad E(Y)\\) = Predicted Value, Expected outcome or response, Expectation \\(\\quad \\quad Y\\) = Dependent variable, criterion, outocme, response \\(\\quad \\quad X\\) = Independent variable, features, predictors The goal of regression analaysis is to derive a model that estimate the paramter (\\(\\beta\\)) which has the least cost. 10.1.2 R Formula Object 10.1.2.1 Notation Symbols Symbol Usage Example ~ Seperate response variables on the left, predictor variables on the right y ~ x + Seperate predictor variables y ~ x + z : Denotes interaction between predictors y ~ x : z * Shortcut denoting all possible interactions. x * z equivalent to x + z + x:z y ~ x * z ^ ^3 measn include these variables and all interactions up to three way y ~ (x + z + w)^2 -1 Suppress the intercept (make intercept to be zero) y ~ x - 1 fits the regression of y on x, forcing the line y = 0 at x = 0 y ~ x -1 . A shortcut placeholder to include all other variables except the dependent variable y ~ . will expand to y ~ x + z + w y ~ . - Exclude specific variable y ~.-z , y~.-w:z I () Elements in parentheses are interpreted arithmetically function Math function can be used in formula, log(y) ~ x + z + w would predict log(y) from x, z and w log(y) ~ x + z + w 10.1.2.2 Example Equation and Notations Example Equation R Formula Notation 1 \\(y=\\beta_0 + \\beta_1 x + \\beta_2 w + \\beta_3 z + e\\) y ~ x + w + z 2 \\(y=\\beta_0 + \\beta_1 x + \\beta_2 wz + e\\) y ~ x + w:z 3 \\(y=\\beta_0 + \\beta_1 x + \\beta_2 wz + \\beta_3 w + \\beta_4 z + e\\) y ~ x + w*z y ~ x + w:z + w + z 4 \\(y = 0 + \\beta_1 x + \\beta_2 w\\) y ~ -1 + x + w 5 \\(y=\\beta_0 + \\beta_1 x + \\beta_2 w + e\\) y ~ .- z 6 \\(y=\\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\beta_3 x^3 + e\\) y ~ x + I(x^2) + I(x^3) In R, there are more than one way to write a notation, consider below two examples: \\(y=\\beta_0 + \\beta_1 x + \\beta_2 w + \\beta_3 z + \\beta_4 xw + \\beta_5 xz + \\beta_6 wz + \\beta_7 xwz + e\\) can be represented by three formulas as below: y ~ (x + w + z)^3 y ~ x * w * z y ~ x + w + z + x:w + x:z + w:z + x:w:z \\(y=\\beta_0 + \\beta_1 x + \\beta_2 w + \\beta_3 z + \\beta_4 xw + \\beta_5 xz + \\beta_6 wz + e\\) can be represented by three formula as below: y ~ (x + w + z)^2 y ~ x * w * z - x:w:z y ~ x + w + z + x:w + x:z + w:z "],
["10-2-application-1.html", "10.2 Application", " 10.2 Application "],
["10-3-types-of-regression.html", "10.3 Types of Regression", " 10.3 Types of Regression 10.3.1 Linear and Non Lienar Linear Regression Simple an d Multiple Regression General Linear Model Heteroscedastic Model Hierarchical Linear Model Erros-in-variables Others Non Linear Regression Logistic Regression Polynomial Regression Stepwise Regression Ridge Regression Lasso Regression ElasticNet Regression Others 10.3.2 Choosing the Regression Algorithm Number of independent variables Shape of the regression line Type of dependent variable Binary outocme : Logistic Regression lm() fits models following the form Y = Xb + e, where e is Normal (0 , s^2). glm() fits models following the form f(Y) = Xb + e. However, in glm both the function f(Y) (the ‘link function’) and the distribution of the error term e can be specified. Hence the name - ‘generalised linear model’. If you are getting the same results using both lm() and glm(), it is because for glm(), f(Y) - the ‘link function’ defaults to Y, and e defaults to Normal (0, s^2). i.e. if you don’t specify the link function and error distribution, the parameters that glm() uses produce the same effect as running lm(). Glm uses normal distribution, lm uses t-distribution, hence the degree of freedom used are different. lm models equation of \\(Y = \\beta_0 X + E\\), where e = Normal(0,s^2) glm models equation of \\(g(Y) = \\beta_0 X + E\\), where distribution of e can be specified function g(Y) is called ‘link function’ . By default parameters, glm fits the same model as lm, with exception that it uses normal distribution instead of t-distribution. "],
["10-4-linear-regression-ols.html", "10.4 Linear Regression (OLS)", " 10.4 Linear Regression (OLS) This section discussed Ordinary Least Square Linear Regression, this includes single and multiple variable OLS Linear Regression. There are other types of linear regression, which are not covered. General Linear Regression Generalized Linear Regression Hierachical Linear Regression Erros-in-variable Linear Regression Others 10.4.1 The Concept Linear Regression establishes a relationship between dependent variable (Y) and one or more independent variables (X) using a best fit straight line (also known as regression line). The objective of linear regression modeling is to find the most optimum equation that best explain the data Optimum equation is defined as the one that has the least cost (error) Once we had derived the optimum equation, we can use the model to predict target \\(Y&#39;\\) base on new variables \\(X\\). 10.4.2 Assumptions Below are conditions for the least-squares estimator - used by linear regression to possess desirable properties; in particular, these assumptions imply that the parameter estimates will be unbiased, consistent, and efficient in the class of linear unbiased estimators. 10.4.2.1 Classical Assumptions The sample is representative of the population for the inference prediction Question how is the data being gathered, is it convincing that it represents the population ? Number of observations must be larger than number of independent variables Check the length of observations &gt;= column length of data 10.4.2.2 Assumptions On Dependent Variable Must not be a categorical data type 10.4.2.3 Assumptions On Independent Variable The independent variables are measured with no error, that is observations must be a set of known constants. (Note: If this is not so, modeling may be done instead using errors-in-variables model techniques) Each independent variable are linearly correclated with outcome, when other independent variables are held constant. Matrix scatter plot and correlation calculation can validate this. Generally correlation of 0.7 and above are considered good. NO Multicollinearity amont predictors - Meaning little or not linear correlationamong the predictors, i.e. it is not possible to express any predictor as a linear combination of the others, if so, we wouldn’t know which predictor actually influene the outcome. 10.4.2.4 Assumptions On Errors (residuals) The errors are random numbers, with means of zero There should not be a pattern in the residuals distribution If the residuals are normally distributed with mean of zero, then it is considered a bonus which we can perform statistical significant testing. \\(e = N(0,\\sigma^2)\\) Normality on redisuals implies that the dependent variable are also normally distributed (if and only if dependent variable is not stochastic) The errors are uncorrelated - that is, the variance–covariance matrix of the errors is diagonal and each non-zero element is the variance of the error Homoscedasticity - The variance of the error is constant across observations. If heteroscedasticity exist, scatter plot of response and predictor will look like below The Goldfeld-Quandt Test can test for heteroscedasticity If homoscedasticity is present, a non-linear correction might fix the problem Otherwise, weighted least squares or other methods might instead be used. 10.4.2.5 Are These Assumptions to be followed strictly ? In real life, actual data rarely satisfies the assumptions, that is: Method is used even though the assumptions are not true Variation from the assumptions can sometimes be used as a measure of how far the model is from being useful Many of these assumptions may be relaxed in more advanced treatments Reports of statistical analyses usually include analyses of tests on the sample data and methodology for the fit and usefulness of the model. 10.4.2.6 Additional Notes On Independent variables Adding more variables to a regression procedure may overfit the model and make things worse. The idea is to pick the best variables Some independent variable(s) are better at predicting the outocme, some contribute little or nothing Because of multicollinearity and overfitting, there is a fair amount of prep-work to be performed BEFORE conducting multiple regression analysis - if one is to do it properly. 10.4.3 Equations 10.4.3.1 Terminology Simple Linear Regression (classical) consists of just on predictor. aka Single Variable Linear Regression. Multiple Linear Regression (classical) consists of multiple predictors. aka. Multiple Variable Linear Regression. Multivariate Regression (aka. General Linear Regression) is linear regression where the outocme is a vector (not scalar). Not the same as multiple variable linear regression. 10.4.3.2 Ordinary Least Square Estimatation Regression Model - Actual Outcome \\(\\quad y_i = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... \\beta_k x_k + e_i\\) \\(\\quad\\)where: \\(\\quad \\quad y_i\\) = actual outcome value \\(\\quad \\quad \\beta_0\\) = intercept, when all independent variables are 0 \\(\\quad \\quad \\beta_k\\) = parameter for independent variable k \\(\\quad \\quad e_i\\) = error for observation i Regression Equation - Predicted Outcome \\(\\quad E(y_i) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... \\beta_k x_k\\) \\(\\quad h_\\theta(X)~=~\\theta_0~+~\\theta_1 \\cdot X\\) , error terms assumed to be zero \\(\\quad\\)where: \\(\\quad \\quad h_\\theta(x)\\) = hypothesis target (dependant variable) \\(\\quad \\quad \\theta_0\\) = intercept \\(\\quad \\quad \\theta_1\\) = slopes or coefficients \\(\\quad \\quad X\\) = independant variables (predictors) \\(\\quad\\)Take note that each \\(\\theta_0\\) and \\(\\theta_1\\) represents multi-variate data in matrix form. 10.4.3.3 Cost Function The goal is to find some values of θ (known as coefficients), so we can minimize the difference between real values \\(y\\) and predicted values (\\(h(x)\\)) Mathematically, this means finding the minimum value of cost function \\(J\\) and derive the optimum value of \\(\\theta_0\\) and \\(\\theta_1\\) Linear regression uses Total Sum Of Square calculation on Error as Cost Function, denoted by \\(J\\) below: \\(\\quad \\quad J(\\theta_0,\\theta_1) = \\frac{1}{2m}\\sum_{i=1}^m ((h_\\theta(x^i)-y^i)^2\\) 10.4.4 OLS Performance 10.4.4.1 Fundamental OLS performance is mainly on error analysis. SST (total sample variability) = SSR (explained variability) + SSE (unexplained variability) : SST Explained 10.4.4.2 Root Mean Square Error (RMSE) RMSE = The square root of the average of the total sum of square error \\(RMSE = \\sqrt{\\frac{SSE}{n}} = \\sqrt \\frac{\\sum^n_{i=1}{(y_i - \\hat y_i)^2}}{n}\\) It measure how close observed data points are to the model’s predicted values It has a unit of Y, therefore cannot used for comparison models with different outcome It can be used to compare different model with the similar outcome but different predictors, however, adjusted \\(R^2\\) is better in this Low RMSE value indicates better fit Compared to the similar Mean Absolute Error, RMSE amplifies and severely punishes large errors SSE is not usable for performance measurement becuase it increases with number of datapoints. RMSE does not suffer this as it is divided by number of datapoints Residual Standard Errror (RSE) is very similar to RMSE, except that RSE use division by degree of freedom RMSE is excellent general measurement to assess the accuracy of a model 10.4.4.3 \\(r\\), \\(R^2\\) and \\(R^2_{adj}\\) r - Correlation Coeeficient Correlation, often measured as a correlation coefficient - indicates the strength and direction of a linear relationship between two variables (for example model output and observed values) The best known is the Pearson product-moment correlation coefficient (also called Pearson correlation coefficient or the sample correlation coefficient) It is a ratio (has no unit) \\(Pearson Correlation, r = \\frac{{}\\sum_{i=1}^{n} (x_i - \\overline{x})(y_i - \\overline{y})} {\\sqrt{\\sum_{i=1}^{n} (x_i - \\overline{x})^2(y_i - \\overline{y})^2}} \\quad, \\quad 0=&lt;r&lt;=1\\) Scatter plot predicted and actual outcome reveal visually the good-fit of the model, good correlation also means tigther the scatter plots with less variability R-Squared - Coefficient Determination \\(R^2 = r^2 = \\frac{SSR}{SST} = 1-\\frac{SSE}{SST}, \\quad 0 &lt;= R^2 &lt;= 1\\) \\(R^2\\) is a ratio (unit-less) indicating how much variations are explained by regression model \\(R^2\\) compares the fit model to a ‘baseline’ model (SST) High \\(R^2\\) value indicates high SSR and low SSE, which means the model is more precise Perfect Case - no errors (SSE=0), \\(R^2\\) will be 1. Worst Case - no improvement over baseline, (coefficient=0, SSR=0, aka horizontally flat line), \\(R^2\\) will be 0. One pitfall of \\(R^2\\) is that it always increases when additional variables are added to the model. The increase can be artificial as it doesn’t improve the model fit - which is called over-fitting. A remediation to this is \\(R^2_{adj}\\) Adjusted R-Squared Adjusted \\(R^2\\) incorporates the number of coefficients and observations into the calculation \\(R_{adj}^2 = 1- \\bigg( \\frac{n-1}{n-p}\\bigg) \\frac{SSE}{SST}\\) \\(\\quad\\) p = number of coefficients (including intercept) \\(\\quad\\) n = number of observations Adjusted \\(R^2\\) will decrease when adding predictors that doesn’t increase the model fit that make up for the loss of degrees of freedom Likewise, it will increase as predictors are added if the increase in model fit is worthwhile \\(R^2_{adj}\\) is useful to compare models with a different number of predictors, hence good for feature selection Training Data and Test Data (out of sample) A built model based on training data always has \\(R^2\\) between 0 &lt;= \\(R^2\\) &lt;= 1 However, if the model is underfit, test data may reveal \\(R^2\\) &lt;= 0 10.4.5 Feature Selection The strength and importance of an independent variable is not measured by its correlation or coefficient with the dependent variable - It only hold true if there is only single independent variable Multicolinearity has below independent variable diversion and therefore must be removed: It increases the p-value to make it insignificant It divert a coef direction (eg. positive becomes negative) Perform transformation (such as log, quadradric) if the plot of independet vs dependent variables shows Heteroscedasticity 10.4.6 Sample Data set.seed(1234) n=100 my.df = data.frame( id = paste(&#39;ID&#39;, 1:n), x1 = 10:(10 + n - 1) * runif(n, min = 0.5, max = 1.1), x2 = 20:(20 + n - 1) * runif(n, min = 0.5, max = 1.1), x3 = 30:(30 + n - 1) * runif(n, min = 0.5, max = 1.1) ) #my.df = gen_slinear(n=100,start=-1, intercept = 7, coef=3, visual=FALSE) my.df$y = 88 + 0.1 * my.df$x1 + 0.2 * my.df$x2 + 0.3*my.df$x3 10.4.7 Run The Code 10.4.7.1 Build The Model lm build a linear regresson model for this equation: \\(h(x_1,x_2,x_3) = \\theta_0+ \\theta_1x_1 + \\theta_2x_2 + \\theta_3X_3\\) lm ( formula , data ) \\(\\quad\\) formula : y~x1 + x2 + x3 \\(\\quad\\) data: matrix or dataframe fit = lm(formula = y ~ . -id, data = my.df) 10.4.7.2 Evaluate The Model summary provides useful infomration about the model, such as: List of coefficients, and their t-value, p-value R-square Adjusted R-square Residual Standard Error, similar to RMSE except that it divides by degree of freedom, instead of number of observations summary(fit) ## ## Call: ## lm(formula = y ~ . - id, data = my.df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.407e-13 -4.830e-15 5.030e-15 1.338e-14 5.722e-14 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 8.800e+01 1.214e-14 7.250e+15 &lt;2e-16 *** ## x1 1.000e-01 3.518e-16 2.842e+14 &lt;2e-16 *** ## x2 2.000e-01 2.954e-16 6.770e+14 &lt;2e-16 *** ## x3 3.000e-01 3.168e-16 9.469e+14 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.7e-14 on 96 degrees of freedom ## Multiple R-squared: 1, Adjusted R-squared: 1 ## F-statistic: 3.423e+30 on 3 and 96 DF, p-value: &lt; 2.2e-16 However, summary doesn’t show RMSE/SSE/SST/SSR measurement, which requried additional coding as in following section. 10.4.7.3 Additional Evaluation (custom code) The built model (lm object) contain good information as below for us to build additional evaluation: Original training data ($model) Residuals ($residuals) Predicted outocme ($fitted.values) R2, Adjusted R2 (summary$) From these information, we can calculate SSE, SST, SSR, RMSE. Note that R2 and R2-Adj can be derived from summary function, no calculation required. eval_regression = function ( fit ) { # fit: lm object SSE = sum(fit$residuals^2) SSR = sum((fit$fitted.values - mean(fit$model[[1]]))^2) SST = sum((fit$model[1] - mean(fit$model[[1]]))^2) RMSE = sqrt(SSE/nrow(fit$model)) R2 = 1-SSE/SST R2.ADJ = summary(fit)$adj.r.squared result = c( sse=SSE, ssr=SSR, sst=SST, rmse=RMSE, r2=R2, r2.adj = R2.ADJ) } This evaluation function return key statistics in a named vector. eval_regression(fit) "],
["10-5-regression-diagnostic.html", "10.5 Regression Diagnostic", " 10.5 Regression Diagnostic library(&#39;car&#39;) 10.5.1 Outlier Test 10.5.2 Linearity Test 10.5.3 Homoscedasticity Test 10.5.4 Multicollinearity Test Validation method: a. Scatter Plot Matrix for all independent variables b. Correlation Matrix calculation for all independent varaibels. Correlation value 0.2 and below consider week correlation and therefore acceptable c. Tolerance (T) - measures the influence of one independent variable on all other independent variables T &lt;0.01 –&gt; confirm correlation T &lt;0.1 –&gt; might have correlation d. Variance Inflation Factor (VIF) - VIF = 1/T T &gt;100 –&gt; confirm correlation T &gt;10 –&gt; might have correlation e. Condition Index (CI) - calculated using a factor analysis on the independent variables ** Values of 10-30** –&gt; indicate a mediocre multicollinearity ** Values &gt; 30** –&gt; strong multicollinearity 10.5.5 Normality Test -->"],
["11-classification.html", "Chapter 11 Classification ", " Chapter 11 Classification "],
["11-1-introduction-1.html", "11.1 Introduction", " 11.1 Introduction The goal of classification prediction is to estimate binary outcome of TRUE or FALSE. 11.1.1 Application 11.1.2 Types of Classification 11.1.2.1 Probability Outcome Logistic regression falls into this category It predicts the probability of binary outcome Researcher need to determine the cutoff threshold to derive binary outcome Cutoff determines variuos performance metrics, hence usefulness of the model 11.1.2.2 Binary Outcome Here are the list of algorithm produce direct binary prediction: 1.Decision Tree 2.Conditional Forest 3.Ensemble Learning 4.Support Vector Machine (SVM) 5.Neural Networks 6.Tree Ensembles - Random Forest - Gradient Boosted Trees 7.Deep Learning 11.1.3 Comparing Algorithm "],
["11-2-library-5.html", "11.2 Library", " 11.2 Library Summary of Libraries And Functions Package Function Purpose 1 Base - R glm General linear model, use for logistic regression 2 ROCR prediction (logistic regression use only) create prediction object from score and label, simulate confusion table for multiple thresholds performance (logistic regression use only) create performane object from prediction object, and support plotting 3 rpart rpart Create a decision tree model plotcp Visualize decision tree model 4 rpart.plot prp 5 party ctree Create a conditional inference tree model 6 randomForest randomForest Create a random forest model 7 e1071 svm Create a SVM model Load the R libraries library(ROCR) library(rpart) library(rpart.plot) library(party) library(randomForest) library(e1071) "],
["11-3-sample-dataset.html", "11.3 Sample Dataset", " 11.3 Sample Dataset This chapter will use a Human Resource dataset that contain 12,000 training records. The dataset will be splited into training set (70%), and test set (30%). ### Load The Data set.seed(1234) raw = read.csv(&#39;./datasets/hr.csv&#39;) ### Split Data train.rows = sample( 1:nrow(raw), 0.7 * nrow(raw) ) train = raw[train.rows,] test = raw[-train.rows,] 11.3.1 The Variables 11.3.1.1 Dependent Variable We want to predict who are the employees that are likely to leave the company. left: employee had left the company (binary) 11.3.1.2 Independent Variables Here are the variables that are relevant to predicting employees leaving. S: Satisfied Employee (binary, obtained through survey) NP: Number of Project Currently Handled LPE: Last Project Evaluation ANH: Average Number of Monthly Hours TIC: Time in company (in years) Newborn: Had a new born in last 12 months str(train) ## &#39;data.frame&#39;: 8400 obs. of 7 variables: ## $ S : num 0.41 0.57 0.83 0.77 0.64 0.72 0.9 0.16 0.34 0.86 ... ## $ LPE : num 0.52 0.42 0.91 0.96 0.43 0.89 0.96 0.97 0.76 0.74 ... ## $ NP : int 2 2 4 3 5 4 4 6 6 2 ... ## $ ANH : int 147 248 210 232 269 217 258 235 237 178 ... ## $ TIC : int 3 4 4 2 3 3 5 3 5 3 ... ## $ Newborn: int 0 0 0 1 0 0 0 0 0 0 ... ## $ left : int 1 0 0 0 0 0 1 0 0 0 ... 11.3.2 Explore The Data Check the structure and samples. summary(raw) ## S LPE NP ANH ## Min. :0.0900 Min. :0.3600 Min. :2.000 Min. : 96.0 ## 1st Qu.:0.4800 1st Qu.:0.5700 1st Qu.:3.000 1st Qu.:157.0 ## Median :0.6600 Median :0.7200 Median :4.000 Median :199.5 ## Mean :0.6295 Mean :0.7166 Mean :3.802 Mean :200.4 ## 3rd Qu.:0.8200 3rd Qu.:0.8600 3rd Qu.:5.000 3rd Qu.:243.0 ## Max. :1.0000 Max. :1.0000 Max. :7.000 Max. :310.0 ## TIC Newborn left ## Min. :2.000 Min. :0.0000 Min. :0.0000 ## 1st Qu.:2.000 1st Qu.:0.0000 1st Qu.:0.0000 ## Median :3.000 Median :0.0000 Median :0.0000 ## Mean :3.229 Mean :0.1542 Mean :0.1667 ## 3rd Qu.:4.000 3rd Qu.:0.0000 3rd Qu.:0.0000 ## Max. :6.000 Max. :1.0000 Max. :1.0000 head(raw) ## S LPE NP ANH TIC Newborn left ## 1 0.38 0.53 2 157 3 0 1 ## 2 0.80 0.86 5 262 6 0 1 ## 3 0.11 0.88 7 272 4 0 1 ## 4 0.72 0.87 5 223 5 0 1 ## 5 0.37 0.52 2 159 3 0 1 ## 6 0.41 0.50 2 153 3 0 1 Check any class bias (skewness) barplot(table(raw$left), ylab=&#39;left&#39;, xlab=&#39;1=Yes, 0=No&#39;) Calculate the Accuracy baseline - that is, probability of True(1) or False(0). Use whichever is higher. prop.table( table(raw$left) ) ## ## 0 1 ## 0.8333333 0.1666667 "],
["11-4-logistic-regression.html", "11.4 Logistic Regression", " 11.4 Logistic Regression 11.4.1 The Concept Logistic Regression is a actually a classification algorithm. It is used to predict: Binary outcome (1=Yes/Sucess, 0=No/Failure), given a set of independent variables. Multinomial outcome (more than two categories) - however, reference category for comparison must be specified, otehrwise, must run multiple regressions with different refence categories Logistic Regression as a special case of linear regression where: The outcome variable is categorical Ln of odds as dependent variable Linear regression cannot be used for classification because: Binary data does not have a normal distribution, which is a condition for many types of regressions Predicted values can go beyond 0 and 1, which violates the definition of probability Probabilities are often not linear 11.4.2 Assumptions Since logistic regression is related to linear combination of IVs, it share some common asssumptions regarding IVs and error terms: Dependent variable must be 1/0 type eg. ‘sucess/failure’, ‘male/female’, ‘yes/no’. Must not be ordinal and continous Observations must be independent Like OLS, Linearity between logit with all independent variables Like OLS, NO multicollinearity - if found, create interaction term, or drop one of the IVs Like OLS, error terms are assumed uncorrelated Although logit is a linear relation with independent variables, logistic regression (which use MLE) is different from OLS Linear Regression as below, due to the fact that DV is categorical and not continuuous: Can handle categorical independent variables Does not assume normality of DV and IVs: becauae \\(p\\) follow Bernoulli distribution Does not assume linearity between DV and IVs: because DV is categorical Does not assume homoscedasticity Does not assume normal errors 11.4.3 Equations The goal of logistic regression is to estimate \\(p\\) (the probability of ‘Success’) for a linear combination of the independent variables This is done by ‘linking’ the linear combination of independent variables to Bernoulli probability distribution (with domain from 0 to 1), to predict the probability of success The link function is called logit, which is the natural log of odds ratio. It is a linear function against independent variables: \\(logit(p) = ln(odds) = ln\\bigg(\\frac{p}{1-p}\\bigg) = \\beta_0 + \\beta_1 x_1 + ... + \\beta_n x_n\\) Derive Odd ratio by anti-log. It measures the ‘strength’ of IV in affecting the outcome, p: \\(odds = \\frac{p}{1-p} = e^{\\beta_0 + \\beta_1 x_1 + ... + \\beta_n x_n}\\) \\(p\\) can be further derived as below sigmoid function. \\(p\\) is non-linear against independent varibales : \\(p = \\frac{1}{1+e^{-(\\beta_0 + \\beta_1x_1 + ... + \\beta_nx_n)}}\\) The logistic graph below shows P(Y=1) vs \\(\\beta_0 + \\beta_1 x_1 + ... + \\beta_n x_n\\). **Intuitively, larger coefficients and independence varibles values increases the chance of being classified as Y=1*8 Compare the graph below to understand why logistic regression is better than linear regression in binary classification. However, it is simplied to illustrate single independent variable as multiple variables cannot be represented in 2D graph \\(\\quad\\) 11.4.4 High School Formula Some of the high scholl maths are useful for calculating the logistic scores. Remember them by heart. What is log \\(y = 10^a\\) \\(log(y) = a\\) \\(y = e^a\\) \\(ln(y) = a\\) Ln is log base e \\(ln(a) = log_e(a)\\) Basic properties of log \\(ln(1) = log(1) = 0\\) \\(ln(0) = log(0) = Inf\\) Log of division becomes substration of log \\(ln\\bigg(\\frac{a}{b}\\bigg) = ln(a) - ln(b)\\) Exponential of summation becomes multiplication of exponential \\(e^{a+b} = e^a * e^b\\) 11.4.5 Run The Code glm (formula, family=gaussian(link=identity), data) \\(\\quad\\) formula : example y ~ x1 + x2 + x3 \\(\\quad\\) family : binomial, gaussian, poisson, quasi etc $\\quad$link : logit:default for binomial, identity:default for gaussian, log:default for poisson` fit.logit = glm(left ~ ., family = binomial, data = train) pred.logit = predict(fit.logit, type = &#39;response&#39;, newdata = test) # alternative with specifying link function # fit = glm(left ~ ., family = binomial (link=logit), data = train) "],
["11-5-decision-tree.html", "11.5 Decision Tree", " 11.5 Decision Tree 11.5.1 The Concept A decision tree is a machine learning algorithm that partitions the data into subsets It is a form of tree structure, containing three types of nodes: Root Node: no incoming, zero or more outgoing Internal/Branch Nodes: one incoming and two or more outgoing Terminal/Leaf Nodes: at the bottom of the tree, no outgoing edge \\(\\quad\\) The tree building process starts with a split at the root, and continues splitting through branches until no further splits can be made (leaf node reached) N number of splits will create N+1 leafs It classify observations by starting at the root of the tree and moving through it until a leaf node. 11.5.1.1 The Goal The goal of a decision tree is to encapsulate the training data in the smallest possible tree The rationale for minimizing the tree size is the logical rule that the simplest possible explanation for a set of phenomena is preferred over other explanations Small trees produce decisions faster than large trees, and they are much easier to look at and understand There are various methods and techniques to control the depth, or prune, of the tree 11.5.1.2 Advantages Popular among non-statistician, as the model is very easy to interpret None-parametric, therefore does not require normality assumption Support many data types: continuous, categorical, ordinal and binary variable Transformation is not required Useful for detecting important variables, interactions and identifying outliers 11.5.2 Tree Building Construcing a good decision tree involves below three processes: Splitting Criteria for each level of the tree: Selecting the variable to split Choosing the value to split Condition to stop splitting Pruning Tree Selection After a good decision tree model is built, you are ready to use it for prediction. 11.5.3 Splitting Criteria Splitting is the process of partitioning the data set into subsets, from root to leaf nodes. Splits are formed on a particular variable and in a particular location For each split, two determinations are made: Splitting variable - The predictor variable used for the split Split point - The set of values for the predictor variable (which are split between the left child node and the right child node) The goal of splitting each node is to select a combination of splitting variable and splitting point that minimizes the error (or maximizes the purity). Two algorithmns (with impurity measure) can be used to make this decision: The picture below illustrates spliting of simple tree with two variables, X and Y with 3 splits 11.5.3.1 Information Gain (based on Entropy value) \\(Entropy = \\sum_{i=1}^c{-p_i * log_2(p_i)}\\) \\(\\quad\\) c = number of class (1 if only single splitting variable), p = the probability of class \\(Information Gain = Entropy(Parent Node) - Entropy (Child Node)\\) Partition the data into subsets that contain instances with similar values (homogenous) Perfectly classified (completely homogeneous), the entropy is zero Maximum Entropy is 1 for (binary class) Weights probability of class by log(base=2) The parent node will select a variable that maximize information gain (smallest Child Entropy) as its child node 11.5.3.2 Gini (Gini Index) \\(Gini = 1 - \\sum_{i=1}^c (p_i)^2\\) \\(\\quad\\) c = number of class (1 if only single splitting variable), p = the probability of class Partition the data into subsets that contain instances with similar values (homogenous) Perfectly classified, Gini index would be zero (pure) Maximum Gini Index is 1 Split that has lowest Gini index value is chosen (most pure) Weights probability of class by square 11.5.3.3 Gini or Information Gain ? Gini index calculation is faster compared to Information gain becuase it doesn’t use log computation This could be a reason why gini is the default method in some machine learning packages Studies have shown that choice of impurity measures has little effect on the performance of the decision tree. This is because they are quite consistent with each other, as shown below: 11.5.3.4 Splitting Stops When All samples belongs to same class (pure) Most samples belongs to same class. This is generalization of the above approach with some error threshold There are no more variabels with samples to be partitioned There is no more samples for the branch test attributes Additionally, some program will implement extra parameters to control the size of the tree, eg. rpart::rpart.control 11.5.4 Pruning Tree 11.5.4.1 Pruning Benefits A decision tree can expanded until the it perfectly fit the training data (error is zero). However, it will perform poorly in test data (test error will be large). Such model does not generalize well to the test data, also known as overfitting. The solution is called pruning: Prunning a tree has greater impact than choosing the impurity measure The larger the tree (more nodes), the more complex the tree is, the more risk of overfitting The reason and benefits of smaller tree size are: Lower branches may be strongly affected by outliers. Pruning enables you to find the next largest tree and minimize this concern A simpler tree often avoids over-fitting 11.5.4.2 Pruning Process Pruning reduces the size of the tree using bottom-up appraoch: Pruning removes leaf nodes under original branch Pruning turns some branch nodes into leaf nodes (bottom-up approach) 11.5.4.3 When To Prune Pre-Pruning Stop the algorithm before it becomes a fully grown tree Post-Pruning Grow decision tree to its entirety Trim the nodes from bottom-up fashion If generalization error improves after trimming, replace the sub-tree by a leaf noes 11.5.5 Tree Selection The process of finding the smallest tree that fits the data This is the tree that yields the lowest cross-validated error 11.5.6 Run The Code 11.5.6.1 Build The Model rpart (formula, method=, na.action=na.rpart, model=FALSE, x=FALSE, y=FALSE, data=) \\(\\quad\\) formula : example y ~ x1 + x2 + x3 \\(\\quad\\) data : binomial, gaussian, poisson, quasi etc \\(\\quad\\) method : splitting method - 'class' for classification \\(\\quad\\) control : (optional) parameter used for controlling tree growth, default as following \\(\\quad\\) na.action: default action delete all observations with y is missing \\(\\quad\\) model: keep a copy of model in the result \\(\\quad\\) x : keep a copy of x matrix in result \\(\\quad\\) y : keep a copy of dependent variable y in result rpart.control specifies the paramters that limits the growth of tree. rpart.control(minsplit = 20, minbucket = round(minsplit/3), cp = 0.01, maxcompete = 4, maxsurrogate = 5, usesurrogate = 2, xval = 10,surrogatestyle = 0, maxdepth = 30) The default rpart.control parameters specifies belowcriteria to be met before splitting a node: minsplit: minimum number of observations in node to be 20 minbucket: if splitted, both sides must have at least 20/3 observations cp: if splitted, the overall relative Cost of the entire tree must reduce by cp*T (T is number of terminal nodes). Cost is calculated as sum of square errors. Relative cost is compared to baseline (where there is no split, with baseline error is scaled to 1) Low cp value relaxed the error reduction expectation, hence creates larger tree maxdept: tree depth not more than 30 levels fit.dtree &lt;- rpart(left ~ ., method = &quot;class&quot;, control = rpart.control(cp = 0.001), data = train) dtree.pred.train = predict(fit.dtree, newdata= train, type = &quot;class&quot;) eval.binclass(score=dtree.pred.train, label=train$left) ## cutoff accuracy recall precision specificity fscore fpr ## 1 NA 0.9838095 0.9124457 0.9882445 0.9978626 0.9488337 0.002137361 ## tpr_fpr tp fp fn tn ## 1 426.9029 1261 15 121 7003 11.5.6.2 Visualize The Model Visualize with Text fit.dtree ## n= 8400 ## ## node), split, n, loss, yval, (yprob) ## * denotes terminal node ## ## 1) root 8400 1382 0 (0.835476190 0.164523810) ## 2) S&gt;=0.465 6403 410 0 (0.935967515 0.064032485) ## 4) TIC&lt; 4.5 5688 56 0 (0.990154712 0.009845288) ## 8) ANH&lt; 286.5 5681 51 0 (0.991022707 0.008977293) * ## 9) ANH&gt;=286.5 7 2 1 (0.285714286 0.714285714) * ## 5) TIC&gt;=4.5 715 354 0 (0.504895105 0.495104895) ## 10) LPE&lt; 0.805 262 10 0 (0.961832061 0.038167939) * ## 11) LPE&gt;=0.805 453 109 1 (0.240618102 0.759381898) ## 22) ANH&lt; 215.5 71 6 0 (0.915492958 0.084507042) * ## 23) ANH&gt;=215.5 382 44 1 (0.115183246 0.884816754) ## 46) S&lt; 0.715 29 5 0 (0.827586207 0.172413793) * ## 47) S&gt;=0.715 353 20 1 (0.056657224 0.943342776) ## 94) NP&lt; 3.5 10 0 0 (1.000000000 0.000000000) * ## 95) NP&gt;=3.5 343 10 1 (0.029154519 0.970845481) * ## 3) S&lt; 0.465 1997 972 0 (0.513269905 0.486730095) ## 6) NP&gt;=2.5 1264 383 0 (0.696993671 0.303006329) ## 12) S&gt;=0.115 920 39 0 (0.957608696 0.042391304) * ## 13) S&lt; 0.115 344 0 1 (0.000000000 1.000000000) * ## 7) NP&lt; 2.5 733 144 1 (0.196452933 0.803547067) ## 14) LPE&gt;=0.575 92 5 0 (0.945652174 0.054347826) * ## 15) LPE&lt; 0.575 641 57 1 (0.088923557 0.911076443) ## 30) LPE&lt; 0.445 25 0 0 (1.000000000 0.000000000) * ## 31) LPE&gt;=0.445 616 32 1 (0.051948052 0.948051948) ## 62) S&lt; 0.35 18 4 0 (0.777777778 0.222222222) * ## 63) S&gt;=0.35 598 18 1 (0.030100334 0.969899666) ## 126) ANH&lt; 125 8 0 0 (1.000000000 0.000000000) * ## 127) ANH&gt;=125 590 10 1 (0.016949153 0.983050847) ## 254) ANH&gt;=165.5 8 1 0 (0.875000000 0.125000000) * ## 255) ANH&lt; 165.5 582 3 1 (0.005154639 0.994845361) * Visualize with Graph Use rpart::prp to plot rpart model (prp is shortform for ‘plot rpart’). Notice that the graph has the below characteristics: Nodes at the same depth level addes up to 100% probability Leafs nodes are formed at different depth levels All leaf nodes adds up to 100% probability prp(fit.dtree, type = 2, extra = 104, fallen.leaves = TRUE, main=&quot;Decision Tree&quot;) 11.5.6.3 Cross Validation Analysis Yes, rpart has built-in cross validation, calculated when a model is created ! Use rpart::cptable to find out the lowest value of cross validation error (column xerror), and its cross standard deviation (column xstd) Use xerror + xstd as the cutting point for best tree size for optimization (pruning), see plotcp below cptable is stored in the model built with rpart. printcp(fit.dtree) #fit.dtree$cptable # alternative ## ## Classification tree: ## rpart(formula = left ~ ., data = train, method = &quot;class&quot;, control = rpart.control(cp = 0.001)) ## ## Variables actually used in tree construction: ## [1] ANH LPE NP S TIC ## ## Root node error: 1382/8400 = 0.16452 ## ## n= 8400 ## ## CP nsplit rel error xerror xstd ## 1 0.1609986 0 1.000000 1.00000 0.0245874 ## 2 0.0850217 3 0.429088 0.42909 0.0169872 ## 3 0.0593343 5 0.259045 0.26194 0.0134673 ## 4 0.0426918 6 0.199711 0.20260 0.0119045 ## 5 0.0180897 7 0.157019 0.16064 0.0106378 ## 6 0.0137482 8 0.138929 0.14327 0.0100611 ## 7 0.0072359 9 0.125181 0.13025 0.0096034 ## 8 0.0057887 11 0.110709 0.12663 0.0094719 ## 9 0.0043415 12 0.104920 0.11505 0.0090373 ## 10 0.0021708 13 0.100579 0.11216 0.0089251 ## 11 0.0010000 14 0.098408 0.11143 0.0088968 Visualize cptable with rpart::plotcp. size of tree means the number of nodes in the tree, it is the number of splits + 1 Horizontal line is the valued at lowest xerror + its xstd Optimum tree size for pruning is below the horizontal line, use this chart to determine the optimum tree size, and its cp value, prune with this cp value plotcp(fit.dtree) 11.5.6.4 Pruning Tree The decision tree was originally built with some constrains (rpart::control) to limits its tree size The tree size can be further reduced by pruning, with the use the cp value in prunning command Prune the tree size that yields lowest xerror, based on the cp analysis above Pruning creates new, reduced nodes decision tree model For this example, we purposely prune the tree to one size smaller, for the sake of demonstration Bigger tree size (lower complexity parameter, cp) means higher accuracy of the model to training data, hence it always reduce the errors (error in $cptable). However, cross validation error (xerror) may go up and down as the tree size increases To choose pruning cp value, first choose the tree size: From cptable, pruning cp value = sqrt(cpvalue for tree-size N * cpvalue for tree-size N-1) From plotcp, choose a tree size, pruning cp value = corresponding cpvalue at the y-axis Remember, tree size = nsplit + 1 fit.dtree.prune = prune(fit.dtree, cp = 0.005) printcp(fit.dtree.prune) ## ## Classification tree: ## rpart(formula = left ~ ., data = train, method = &quot;class&quot;, control = rpart.control(cp = 0.001)) ## ## Variables actually used in tree construction: ## [1] ANH LPE NP S TIC ## ## Root node error: 1382/8400 = 0.16452 ## ## n= 8400 ## ## CP nsplit rel error xerror xstd ## 1 0.1609986 0 1.00000 1.00000 0.0245874 ## 2 0.0850217 3 0.42909 0.42909 0.0169872 ## 3 0.0593343 5 0.25904 0.26194 0.0134673 ## 4 0.0426918 6 0.19971 0.20260 0.0119045 ## 5 0.0180897 7 0.15702 0.16064 0.0106378 ## 6 0.0137482 8 0.13893 0.14327 0.0100611 ## 7 0.0072359 9 0.12518 0.13025 0.0096034 ## 8 0.0057887 11 0.11071 0.12663 0.0094719 ## 9 0.0050000 12 0.10492 0.11505 0.0090373 plotcp(fit.dtree.prune) 11.5.6.5 Prediction We would like to compare the model performance: a. Original Model with training set b. Pruned Model with training set c. Pruned Model with test set Overall, we can see that the performance dropped slightly after prunning for datset. # original model with training set dtree.pred.train = predict(fit.dtree, newdata=train, type=&quot;class&quot;) eval.binclass(score=dtree.pred.train, label=train$left)[2:5] ## accuracy recall precision specificity ## 1 0.9838095 0.9124457 0.9882445 0.9978626 # original model with test set dtree.pred.test1 = predict(fit.dtree, newdata=test, type=&quot;class&quot;) eval.binclass(score=dtree.pred.test1, label=test$left)[2:5] ## accuracy recall precision specificity ## 1 0.9816667 0.9110032 0.9808362 0.9963112 # pruned model with training set dtree.pred.train.pruned = predict(fit.dtree.pruned, data=train, type=&quot;class&quot;) eval.binclass(score=dtree.pred.train.pruned, label=train$left)[2:5] ## accuracy recall precision specificity ## 1 0.9794048 0.9124457 0.960396 0.9925905 # pruned model with test set dtree.pred.test2 = predict(fit.dtree.pruned, newdata=test, type=&quot;class&quot;) eval.binclass(score=dtree.pred.test2, label=test$left)[2:5] ## accuracy recall precision specificity ## 1 0.9772222 0.9142395 0.9511785 0.990275 "],
["11-6-conditional-inference-tree.html", "11.6 Conditional Inference Tree", " 11.6 Conditional Inference Tree "],
["11-7-random-forest.html", "11.7 Random Forest", " 11.7 Random Forest 11.7.1 The Concept Random forest is a type of Ensemble Method, which enjoy all the ensemble benefits. Random forest model contain large amount of decision trees as oppose to tranditional single decision tree. 11.7.1.1 How Does It Work ? Divides training data into large amount random subsets, each subset will draw samples from training data using bagging method (with replacement). Each subset will contain the same number of observations as training set Create a decision tree for each subset, resulting multiple trees (henced called Forest) Each tree uses different set of random variables for splitting (not all variables are being used in each tree) When doing prediction, all tress will votes and majority wins Random Forest Process Flow 11.7.1.2 The Beliefs Random Forest provides much better accuracy compared to single decision tree. The major belief why this works are: Most of the trees can provide correct prediction for most part of the data The tree are making mistake at different place Hence, major votes for classification result expected to be closer to correct classification 11.7.1.3 Advantages and Disadvantages Advantages A single decision tree suffers from the phenomena of overfitting, as the signle tree model memorize the data Randomforest uses bagging (with replacement) to ‘artificially’ creates different types of dataset, hence reducing the phenomena of memorizing Cross validation is not required, since each subset are already being randomized It is usually more accurate than single tree, but no guarantee Disadvantage Ensemble method such as random forest is not interpretable 11.7.2 Run The Code 11.7.2.1 Determine Class or Probability Outcome randomForest can build both class and probability model, depending on the dependent variable: If the depenedent variable is factor, it will build a model for class prediction If the depenedent variable is number, it will build a model for probability prediction For 0/1 dependent variable, if not factorized, it is treated as number, and therefore for probability prediction. Here we convert all independent variable to factor for class prediction. This step is different from rpart (single decision tree), as rpart does not require this step. rpart can predict both class and probability by just specifying the type=‘class’ or type=‘response’ train$left = as.factor(train$left) test$left = as.factor(test$left) 11.7.2.2 Build The Model library(randomForest) set.seed(1234) fit.forest &lt;- randomForest(left~., data=train) 11.7.2.3 Class Prediction forest.pred &lt;- predict(fit.forest, newdata=test) confusion &lt;- table(test$left, forest.pred, dnn=c(&quot;Actual&quot;, &quot;Predicted&quot;)) confusion ## Predicted ## Actual 0 1 ## 0 2975 7 ## 1 48 570 11.7.2.4 Analyize Vairable Importance What Variables Are Important ? randomForest::importance output matrix of all variable used in the randomForest and their contribution in term of decreasing GiniIndex (decreasing impurity and increasing purity) The higher the value, means the variable is more important in contributing to accuracy of the model forest.imp = importance(fit.forest) forest.imp[order(forest.imp, decreasing=T),, drop=FALSE] ## order descendingly ## MeanDecreaseGini ## S 782.815405 ## NP 419.376853 ## TIC 400.909756 ## ANH 358.655346 ## LPE 289.506732 ## Newborn 9.953228 Visualize The Importance randomForest::varImpPlot comes with handy plotting utility to plot the variable importance data. varImpPlot(fit.forest) 11.7.2.5 Frequent Used Variables Random forest models work by building a large collection of trees. As a result, we lose some of the interpretability that comes with CART in terms of seeing how predictions are made and which variables are important However, one of the way to give us insight into which variables are important, other than the decrease of impurity above, we can calculate how frequent a variable is being used for decision making (splitting) randomForest::varUsed returns a vector of frequency of each variable used. However, it doesn’t comes with the variable name, which can be obtained from $forest$xlevels. randomForest::varUsed(x, by.tree=FALSE, count=TRUE) \\(\\quad\\) x : model object of randomForest \\(\\quad\\) by.tree : FALSE, default display per variable appearance, and not per tree \\(\\quad\\) count : TRUE, default aggregate all varibales appearance into frequency, otherwise display individual appearance vu = varUsed(fit.forest, count = TRUE) vusorted = sort(vu, decreasing = FALSE, index.return = TRUE) dotchart(vusorted$x, names(fit.forest$forest$xlevels[vusorted$ix])) "],
["11-8-svm.html", "11.8 SVM", " 11.8 SVM library(e1071) set.seed(1234) fit.svm &lt;- svm(class~., data=train) fit.svm #By default, the svm() function sets gamma to 1 / (number of predictors) and cost to 1. svm.pred &lt;- predict(fit.svm, na.omit(test)) svm.perf &lt;- table(na.omit(test)$class, svm.pred, dnn=c(&quot;Actual&quot;, &quot;Predicted&quot;)) svm.perf set.seed(1234) tuned &lt;- tune.svm(left~., data=train, gamma=10^(-6:1), cost=10^(-10:10)) tuned fit.svm &lt;- svm(left~., data=train, gamma=.01, cost=1) fit.svm svm.pred &lt;- predict(fit.svm, na.omit(test)) svm.perf &lt;- table(na.omit(test)$class, svm.pred, dnn=c(&quot;Actual&quot;, &quot;Predicted&quot;)) svm.perf 11.8.1 Run The Code "],
["11-9-performance-measurement.html", "11.9 Performance Measurement", " 11.9 Performance Measurement There are many performance measurement used for binary classification. Here are the rules of thumb which one to use: Recall: If you don’t mind getting some inaccurate result, as long as you get as much correct ones Precision: If you demand rate of correctness and willing to reject some correct results F1 Score: For a more balanced measurement, taking into consideraton both recall and precision 11.9.1 Confusion Matrix Confusion Matrix and Performance Measurement 11.9.1.1 Accuracy Accuracy answers the question: From the total samples, how many had been correctly predicted by the model ? \\(Accuracy = \\frac{TP+TN}{TP+TN+FP+FN}\\) This measurement is useful when the both classes are balanced (that is, the number of TP and TN cases are almost balanced) In practice, it seems that the best accuracy is usually achieved when the cutpoint is near the Probability(actual TRUE) Accuracy is completely usessless in highly skewed class. For example, with a disease that only affects 1 in a million people a completely bogus screening test that always reports “negative” will be 99.9999% accurate 11.9.1.2 Recall Recall answers the question: Out of all actual positive samples, how many were correctly predicted by classifiers ? \\(Recall = \\frac{TP}{TP+FN}\\) 11.9.1.3 Precision Precison answers the question: Out of all the samples classifier predicted as positive, what fraction were correct ? \\(Precision = \\frac{TP}{TP+FP}\\) 11.9.1.4 F1 Score F1 score is the harmonic mean of Precision and Recall. Intuitively, F1 Score is the weighted average of Precision and Recall. It takes into account all three measures: TP, FP and FN \\(Precision = 2*\\frac{Recall * Precision}{Recall + Precision}\\) F1 is usually more useful than accuracy, especially if you have an unbalanced class distribution 11.9.2 Performance related to Logistic Regression 11.9.2.1 Area Under ROC (AUROC) ROC curve is basically a graph of TPR vs FPR (some refer to as Recall vs (1-Sensitivity), plotted for different thresholds Comparing two different models, the model with higher AUROC is considered to have higher overall Accuracy AUROC (Area Under ROC) measures : AUC of 0.5: means the model is as good as tossing a coin, worthless AUC of 1.0: means for all cutoff points, TPR=1 and FPR=0. Intuitively it means, all samples had been correctly classified into TP (TPR=1) and TN(FPR=0), and there is no FP and FN. Ultiamtely it means Accuracy is 100% AUROC and Thresholds 11.9.2.2 Cutoff Threshold Impact Cutoff threshold direclty influence the value of TP, FP, TN, FN. If cutoff threshold is lowered (lower probability to classify as Postive), the results are: More linient and hence more samples will be classified as Positive More predicted Positives means more TP and FP, hence TPR and FPR increases However, TPR and FPR increases at different rate: If TPR increases faster than FPR -&gt; this is good, as the lowered threshold generated more TP than FP If FPR increases faster then TPR -&gt; this is not good, as the lowered threhsold generated more FP than TP The cutoff with highest TPR/FPR value is the optimal means optimum point whereby It is possible to discover the optimum cutoff by finding the cutoff with highest TPR/FPR Different threshold produces different performance metrics (Accuracy, Recall, Precision and Specificity and F1-score). As an example, picture below shows how threshold influences the ROC curve. Threshold and ROC The only way to estimate the optimum threshold for each of the performance measurement will be to measure them for a wide range of threshold. 11.9.2.3 Cutoff Visualization Selecting a cutoff threshold depends on the objectives of the researcher. To help understanding the how cutoff changes the performance metircs, try visualize them in below graph: 1.Threshold vs Accuracy 2.Threshold vs Recall (TPR) 3.Threshold vs TPR/FPR 4.Threshold vs Precision 5.Threshold vs F1 Score 6.ROC Curve (TPR vs FPR) 11.9.3 Model Evaluation 11.9.3.1 Compare With Baseline In an highly unbalanced dataset (eg. patients that is diagnosed with cancer skewed towards negative). Hence, it is essential to make a lazy baseline comparison with simply classifying every records with negative (cutoff at 1) or positive (cutoff at 0). Put in mind of the dataset used for comparison: Use data from training set as model baseline, when concerning training performance Use dta from test set as model baseline, when concerning test performance Model Accuracy is not better than ‘lazy’ baseline ! In highly bias class, in term of accuracy, the model usually unable to outperform the baseline by good marign Hence accuracy is not a good measurement in such case Model is still useful if the research objective is towards other measurement usch as recall, precision Adjust the threshold to achieve optimum better objectives 11.9.3.2 Combination of Variables Model AIC to measure the usefullness. It is like R-square in linear regression. Use it to compare model with different combinations of variables. Since logistic regression is a actual linear regression of ln(odds), multicolinearity rule apply. 11.9.4 Run The Code 11.9.4.1 General Function This is a custom general function to evaluate all key metrics for Classification. eval.bincalss ( score=NULL, label=NULL, cutoff=NULL) score: predicted value, either probability or binary label: actual value cutoff: threshold (0-1) if score is probability eval.binclass = function(score = NULL, label = NULL, cutoff = NA) { actual = factor(as.logical(as.integer(as.character(label))), levels = c(TRUE, FALSE)) if (is.na(cutoff)) predicted = factor(as.logical(as.integer(as.character(score))), levels = c(TRUE, FALSE)) # binary else predicted = factor(as.logical(score &gt; cutoff), levels = c(TRUE, FALSE)) # regression ct = table(actual, predicted, useNA = &#39;no&#39;, exclude = c(NA)) #confusion table tp = ct[1] fp = ct[2] fn = ct[3] tn = ct[4] accuracy = (ct[1] + ct[4]) / (sum(ct)) recall = ct[1] / (ct[1] + ct[3]) precision = ct[1] / (ct[1] + ct[2]) fpr = ct[2] / (ct[2] + ct[4]) specificity = ct[4] / (ct[2] + ct[4]) tpr_fpr = recall / (1 - specificity) fscore = 2 * (precision * recall / (precision + recall)) data.frame(cutoff = cutoff, accuracy = accuracy, recall = recall, precision = precision, specificity = specificity, fscore = fscore, fpr = fpr, tpr_fpr = tpr_fpr, tp = tp, fp = fp, fn = fn, tn = tn) } eval.binclass(score=pred.logit, label=test$left, cutoff=0.5) ## cutoff accuracy recall precision specificity fscore fpr ## 1 0.5 0.8147222 0.1957929 0.4158076 0.9429913 0.2662266 0.05700872 ## tpr_fpr tp fp fn tn ## 1 3.434437 121 170 497 2812 11.9.4.2 ROCR::Construct the Performance Metric ROCR::predictionis the main function to crate prediction object that contains key data such as thresholds, TP, FP, TN, FP. With these data points, we can plot and calculate multiple performacne and visualization. It takes only two inputs: 1. Score (probability of being POSITIVE) 2. Label (actual POSITVE) rocr.pred = prediction(pred.logit, test$left) rocr.metrics = data.frame( cutoff = rocr.pred@cutoffs[[1]], accuracy = (rocr.pred@tp[[1]] + rocr.pred@tn[[1]]) / (rocr.pred@tp[[1]] + rocr.pred@tn[[1]] + rocr.pred@fp[[1]] + rocr.pred@fn[[1]]), tpr = rocr.pred@tp[[1]] / (rocr.pred@tp[[1]] + rocr.pred@fn[[1]]), fpr = rocr.pred@fp[[1]] / (rocr.pred@fp[[1]] + rocr.pred@tn[[1]]), ppv = rocr.pred@tp[[1]] / (rocr.pred@tp[[1]] + rocr.pred@fp[[1]]) ) rocr.metrics$fscore = 2 * (rocr.metrics$tpr * rocr.metrics$ppv) / (rocr.metrics$tpr + rocr.metrics$ppv) rocr.metrics$tpr_fpr = rocr.metrics$tpr / rocr.metrics$fpr 11.9.4.3 ROCR::List The Performance Table The code below discover the optimum threshold for few metrices. Threhold for the highest estiamtes is summarized. ## discovery the optimal threshold for various metrics rocr.best = rbind( best.accuracy = c(max = max(rocr.metrics$accuracy, na.rm = TRUE), cutoff = rocr.metrics$cutoff[which.max(rocr.metrics$accuracy)]), best.ppv = c(max = max(rocr.metrics$ppv, na.rm = TRUE), cutoff = rocr.metrics$cutoff[which.max(rocr.metrics$ppv)]), best.recall = c(max = max(rocr.metrics$tpr, na.rm = TRUE), cutoff = rocr.metrics$cutoff[which.max(rocr.metrics$tpr)]), best.fscore = c(max = max(rocr.metrics$fscore, na.rm = TRUE), cutoff = rocr.metrics$cutoff[which.max(rocr.metrics$fscore)]), best.tpr_fpr = c(max = max(rocr.metrics$tpr_fpr, na.rm = TRUE), cutoff = rocr.metrics$cutoff[which.max(rocr.metrics$tpr_fpr)]) ) kable(rocr.best) max cutoff best.accuracy 0.8383333 0.2564513 best.ppv 0.5214797 0.2564513 best.recall 1.0000000 0.0073737 best.fscore 0.6031102 0.2490565 best.tpr_fpr 5.2584316 0.2564513 11.9.4.4 ROCR::Visualize The Data and Performance Plot TP, Tn, FP and FN These data are available in the ROCR::prediction object. plot (rocr.pred@cutoffs[[1]], rocr.pred@tp[[1]],xlim=c(0,1), ylim=c(0,12000), col=&#39;green&#39;) lines(rocr.pred@cutoffs[[1]], rocr.pred@tn[[1]], xlim = c(0, 1), ylim = c(0, 12000),col=&#39;red&#39;) lines(rocr.pred@cutoffs[[1]], rocr.pred@fp[[1]], xlim = c(0, 1), ylim = c(0, 12000), col=&#39;blue&#39;) lines(rocr.pred@cutoffs[[1]], rocr.pred@fn[[1]], xlim = c(0, 1), ylim = c(0, 12000),col=&#39;orange&#39;) legend(&quot;top&quot;, inset = .05, cex = 1, title = &quot;Legend&quot;, c(&quot;TP&quot;, &quot;TN&quot;, &quot;FP&quot;,&quot;FN&quot;), horiz = TRUE, lty = c(1, 1), lwd = c(2, 2), col = c(&quot;green&quot;, &quot;red&quot;, &quot;blue&quot;,&#39;orange&#39;), bg = &quot;grey96&quot;) Plot ROC Curve We can build a ROCR::performance object, and plot it ! AUC is calculated with ROCR::performance as well. rocr.perf = performance(rocr.pred, measure = &quot;tpr&quot;, x.measure = &quot;fpr&quot;) rocr.auc = as.numeric(performance(rocr.pred, &quot;auc&quot;)@y.values) plot(rocr.perf, lwd = 3, colorize = TRUE, print.cutoffs.at = seq(0, 1, by = 0.1), text.adj = c(-0.2, 1.7), main = &#39;ROC Curve&#39;) mtext(paste(&#39;auc : &#39;, round(rocr.auc, 5))) abline(0, 1, col = &quot;red&quot;, lty = 2) Plot Accuracy rocr.perf = performance(rocr.pred, measure = &quot;acc&quot;) best.x = rocr.perf@x.values[[1]][which.max(rocr.perf@y.values[[1]])] best.y = max(rocr.perf@y.values[[1]]) plot(rocr.perf, main = &#39;Accuracy vs Cutoff&#39;, xlim = c(0, 1), ylim = c(0, 1)) abline(v = best.x, col = &#39;red&#39;, lty = 2) abline(h = best.y, col = &#39;red&#39;, lty = 2) text(best.x + 0.1, 0.05, round(best.x, 5), col = &quot;red&quot;) text(0.05, best.y + 0.05, round(best.y, 5), col = &quot;red&quot;) Plot Precesion rocr.perf = performance(rocr.pred, measure = &quot;prec&quot;) best.x = rocr.perf@x.values[[1]][which.max(rocr.perf@y.values[[1]])] best.y = max(rocr.perf@y.values[[1]][is.finite(rocr.perf@y.values[[1]])]) plot(rocr.perf, main = &#39;Precision vs Cutoff&#39;, xlim = c(0, 1), ylim = c(0, 1)) abline(v = best.x, col = &#39;red&#39;, lty = 2) abline(h = best.y, col = &#39;red&#39;, lty = 2) text(best.x + 0.1, 0.05, round(best.x, 5), col = &quot;red&quot;) text(0.05, best.y + 0.05, round(best.y, 5), col = &quot;red&quot;) Plot Multiple Metrices Into One Graph As we can see, ROCR::performance is good to build measure and plot for single y-axis measurement. It unfortunately does not support plotting multiple y-axis into one graph. Threfore, we shall build the plot using metrices table constructed earlier (variable rocr.metrics). ### set initial margin of the plot par(mar = c(5, 5, 4, 6)) ## plot graph on left hand side scale plot(rocr.metrics$cutoff, rocr.metrics$tpr, axes = FALSE,, ylab = &#39;&#39;, xlab = &#39;&#39;, col = &#39;green&#39;, main = &#39;Multiple Performances vs Cutoff&#39;, ylim = c(0, 1)) lines(rocr.metrics$cutoff, rocr.metrics$fpr, col = &#39;red&#39;, ylim = c(0, 1)) axis(1, pretty(range(rocr.metrics$cutoff[is.finite(rocr.metrics$cutoff)]), 10)) axis(2, ylim = c(0, 1), col = &quot;black&quot;, las = 1) ## las=1 makes horizontal labels lines(rocr.metrics$fpr, rocr.metrics$tpr, lwd = 3, col = &#39;black&#39;, ylim = c(0, 1), xlim = c(0, 1)) lines(rocr.metrics$cutoff, rocr.metrics$accuracy, col = &#39;purple&#39;, ylim = c(0, 1), xlim = c(0, 1)) lines(rocr.metrics$cutoff, rocr.metrics$ppv, col = &#39;cyan&#39;, ylim = c(0, 1), xlim = c(0, 1)) lines(rocr.metrics$cutoff, rocr.metrics$fscore, col = &#39;pink&#39;, ylim = c(0, 1), xlim = c(0, 1)) abline(0, 1, lty = 2, col = &#39;black&#39;) ## plot graph on right hand side scale par(new = TRUE) plot(rocr.metrics$cutoff, rocr.metrics$tpr_fpr, axes = FALSE, ylab = &#39;&#39;, xlab = &#39;&#39;, col = &#39;orange&#39;) axis(4, col = &quot;orange&quot;, las = 1) ## las=1 makes horizontal labels best = rocr.metrics$cutoff[which.max(rocr.metrics$tpr_fpr[is.finite(rocr.metrics$tpr_fpr)])] abline(v = best, col = &#39;red&#39;, lty = 2) text(best + 0.075, 0.05, round(best, 5), col = &quot;red&quot;) ## axis labels mtext(&quot;cutoff&quot;, side = 1, col = &quot;black&quot;, line = 2.5) mtext(&quot;tpr, fpr, accuracy, precision, f1-scaore&quot;, side = 2, col = &quot;black&quot;, line = 2.5) mtext(&quot;tpr/fpr&quot;, side = 4, col = &quot;black&quot;, line = 2.5) ## legend legend(&quot;topright&quot;, inset = .05, cex = 1, title = &quot;Legend&quot;, c(&quot;TPR&quot;, &quot;FPR&quot;, &quot;TPR/FPR&quot;, &quot;ROC&quot;, &quot;Accuracy&quot;, &quot;Precision&quot;, &quot;F1-Score&quot;), horiz = FALSE, lty = c(1, 1), lwd = c(2, 2), col = c(&quot;green&quot;, &quot;red&quot;, &quot;orange&quot;, &#39;black&#39;, &#39;purple&#39;, &#39;cyan&#39;, &#39;pink&#39;), bg = &quot;grey96&quot;) -->"],
["12-survival-analysis.html", "Chapter 12 Survival Analysis ", " Chapter 12 Survival Analysis "],
["12-1-library-6.html", "12.1 Library", " 12.1 Library library(survival) "],
["12-2-sample-data-9.html", "12.2 Sample Data", " 12.2 Sample Data main.data = read.csv(&#39;./datasets/surv_maintenance.csv&#39;) str(main.data) ## &#39;data.frame&#39;: 1000 obs. of 7 variables: ## $ lifetime : int 56 81 60 86 34 30 68 65 23 81 ... ## $ broken : int 0 1 0 1 0 0 0 1 0 1 ... ## $ pressureInd : num 92.2 72.1 96.3 94.4 97.8 ... ## $ moistureInd : num 104.2 103.1 77.8 108.5 99.4 ... ## $ temperatureInd: num 96.5 87.3 112.2 72 103.8 ... ## $ team : Factor w/ 3 levels &quot;TeamA&quot;,&quot;TeamB&quot;,..: 1 3 1 3 2 1 2 2 2 3 ... ## $ provider : Factor w/ 4 levels &quot;Provider1&quot;,&quot;Provider2&quot;,..: 4 4 1 2 1 1 2 3 2 4 ... head(main.data) ## lifetime broken pressureInd moistureInd temperatureInd team provider ## 1 56 0 92.17885 104.23020 96.51716 TeamA Provider4 ## 2 81 1 72.07594 103.06570 87.27106 TeamC Provider4 ## 3 60 0 96.27225 77.80138 112.19617 TeamA Provider1 ## 4 86 1 94.40646 108.49361 72.02537 TeamC Provider2 ## 5 34 0 97.75290 99.41349 103.75627 TeamB Provider1 ## 6 30 0 87.67880 115.71226 89.79210 TeamA Provider1 summary(main.data) ## lifetime broken pressureInd moistureInd ## Min. : 1.0 Min. :0.000 Min. : 33.48 Min. : 58.55 ## 1st Qu.:34.0 1st Qu.:0.000 1st Qu.: 85.56 1st Qu.: 92.77 ## Median :60.0 Median :0.000 Median : 97.22 Median : 99.43 ## Mean :55.2 Mean :0.397 Mean : 98.60 Mean : 99.38 ## 3rd Qu.:80.0 3rd Qu.:1.000 3rd Qu.:112.25 3rd Qu.:106.12 ## Max. :93.0 Max. :1.000 Max. :173.28 Max. :128.60 ## temperatureInd team provider ## Min. : 42.28 TeamA:336 Provider1:254 ## 1st Qu.: 87.68 TeamB:356 Provider2:266 ## Median :100.59 TeamC:308 Provider3:242 ## Mean :100.63 Provider4:238 ## 3rd Qu.:113.66 ## Max. :172.54 "],
["12-3-the-code.html", "12.3 The Code", " 12.3 The Code Create the Model Step 1 - Use survival::Surv to create an object that ties time (lifetime) to event (broken). This object simple simply combines both time and event variables together in a matrix. Step 2 - run the modelling with the time-event object create above, specify the parameters and distribution (example below use normal distribution). dependantVar = Surv(main.data$lifetime, main.data$broken) main.fit = survreg(dependantVar~pressureInd+moistureInd+temperatureInd+team+provider, dist=&quot;gaussian&quot;, data=main.data) summary(main.fit) ## ## Call: ## survreg(formula = dependantVar ~ pressureInd + moistureInd + ## temperatureInd + team + provider, data = main.data, dist = &quot;gaussian&quot;) ## Value Std. Error z p ## (Intercept) 8.04e+01 0.29371 273.574 0.00e+00 ## pressureInd -7.14e-04 0.00122 -0.587 5.57e-01 ## moistureInd 6.01e-03 0.00240 2.505 1.22e-02 ## temperatureInd -1.04e-02 0.00121 -8.593 8.49e-18 ## teamTeamB -5.67e-02 0.05882 -0.964 3.35e-01 ## teamTeamC -6.22e+00 0.06132 -101.392 0.00e+00 ## providerProvider2 1.25e+01 0.06665 187.464 0.00e+00 ## providerProvider3 -1.44e+01 0.06275 -229.241 0.00e+00 ## providerProvider4 7.92e+00 0.07056 112.233 0.00e+00 ## Log(scale) -7.43e-01 0.03540 -20.998 6.86e-98 ## ## Scale= 0.476 ## ## Gaussian distribution ## Loglik(model)= -270.1 Loglik(intercept only)= -1557 ## Chisq= 2573.75 on 8 degrees of freedom, p= 0 ## Number of Newton-Raphson Iterations: 12 ## n= 1000 Use the model to predict lifetime before break (labeled as Ebreak below). Then calculate the remaing lifetime (remaingLT) as predicted lifetime before break - current lifetime. Ebreak = predict(main.fit, data=main.data,type=&#39;quantile&#39;,p=0.5) remainingLT = Ebreak - main.data$lifetime main.data.forecast = data.frame ( lifetime = main.data$lifetime, broken = main.data$broken, Ebreak, remainingLT ) Put all into one data.frame. Sort the remainingLT with lowest first, so that the priority to pay attention to are top to down. head(main.data.forecast[order(main.data.forecast$remainingLT, decreasing = FALSE),]) ## lifetime broken Ebreak remainingLT ## 277 61 1 59.67415 -1.325853 ## 668 60 1 58.83450 -1.165500 ## 619 60 1 58.86275 -1.137253 ## 338 60 1 58.87862 -1.121378 ## 303 60 1 58.88567 -1.114328 ## 557 60 1 58.89501 -1.104994 "],
["12-4-performance-evaluation.html", "12.4 Performance Evaluation", " 12.4 Performance Evaluation Performance can be measured by analysis how well the model predict the lifetime of ‘already happened event’, which is the training data. Subset the training data. eval.data = subset ( main.data.forecast, broken==1) Measure correlation of the predicted and actual. cor(eval.data$Ebreak,eval.data$lifetime) ## [1] 0.9989941 Visualize the fitness of the model. plot(eval.data$Ebreak, eval.data$lifetime) -->"],
["13-model-optimization.html", "Chapter 13 Model Optimization ", " Chapter 13 Model Optimization "],
["13-1-occams-razor.html", "13.1 Occam’s Razor", " 13.1 Occam’s Razor Given two models of similar generalization errors, one should prefer the simpler model over the more complex model For complex models, there is a greater chance that it was fitted accidentally by errors in data Therefore, one should include model complexity when evaluating a model -->"]
]
