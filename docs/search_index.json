[
["10-classification.html", "Chapter 10 Classification ", " Chapter 10 Classification "],
["10-1-introduction.html", "10.1 Introduction", " 10.1 Introduction "],
["10-2-application.html", "10.2 Application", " 10.2 Application "],
["10-3-choosing-the-right-algorithm.html", "10.3 Choosing The Right Algorithm", " 10.3 Choosing The Right Algorithm "],
["10-4-logistic-regression.html", "10.4 Logistic Regression", " 10.4 Logistic Regression 10.4.1 The Concept Logistic Regression is a actually a classification algorithm. It is used to predict: Binary outcome (1=Yes/Sucess, 0=No/Failure), given a set of independent variables. Multinomial outcome (more than two categories) - however, reference category for comparison must be specified, otehrwise, must run multiple regressions with different refence categories Like OLS, NO multicollinearity - if found, create interaction term, or drop one of the IVs Like OLS, error terms are assumed uncorrelated Logistic Regression as a special case of linear regression where: The outcome variable is categorical Log of odds as dependent variable In simple words, it predicts the probability (\\(p\\)) of occurrence of an event by fitting data to a logit function. Linear regression cannot be used for classification because: Binary data does not have a normal distribution, which is a condition for many types of regressions Predicted values can go beyond 0 and 1, which violates the definition of probability Probabilities are often not linear 10.4.2 Assumptions Dependent variable must be 1/0 type eg. ‘sucess/failure’, ‘male/female’, ‘yes/no’. Must not be ordinal and continous Observations must be independent Linearity between logit with all independent variables Although logit is a linear relation with independent variables, logistic regression (which use MLE) is differenct from OLS Linear Regression as below: Can handle categorical independent variables Does not assume normality of DV and IVs Does not assume linearity between DV and IVs Does not assume homoscedasticity Does not assume normal errors 10.4.3 Equations The goal of logistic regression is to estimate \\(p\\) for a linear combination of the independent variables. This is done by ‘linking’ the linear combination of independent variables to Bernoulli probability distribution (with domain from 0 to 1), to predict the probability of success. The link function is called logit, which is the natural log of odds ratio. It is a linear function against independent variables: \\(logit(p) = ln(odds) = ln\\bigg(\\frac{p}{1-p}\\bigg) = \\beta_0 + \\beta_1 x_1 + ... + \\beta_n x_n\\) \\(p\\) can be further derived as below sigmoid function. \\(p\\) is non-linear against independent varibales : \\(p = \\frac{e^{\\beta_0 + \\beta_1x_1 + ... + \\beta_nx_n}}{1+e^{\\beta_0 + \\beta_1x_1 + ... + \\beta_nx_n}}\\) 10.4.4 Sample Data 10.4.5 Run The Code 10.4.6 Performance Measurement 10.4.6.1 Confusion Matrix 10.4.6.2 ROC Curve "],
["10-5-decision-tree-regression.html", "10.5 Decision Tree Regression", " 10.5 Decision Tree Regression -->"]
]
