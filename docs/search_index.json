[
["11-classification.html", "Chapter 11 Classification ", " Chapter 11 Classification "],
["11-1-introduction.html", "11.1 Introduction", " 11.1 Introduction The goal of classification prediction is to estimate binary outcome of TRUE or FALSE. 11.1.1 Application 11.1.2 Types of Classification 11.1.2.1 Probability Outcome Logistic regression falls into this category It predicts the probability of binary outcome Researcher need to determine the cutoff threshold to derive binary outcome Cutoff determines variuos performance metrics, hence usefulness of the model 11.1.2.2 Binary Outcome Here are the list of algorithm produce direct binary prediction: 1.Decision Tree 2.Conditional Forest 3.Ensemble Learning 4.Support Vector Machine (SVM) 5.Neural Networks 6.Tree Ensembles - Random Forest - Gradient Boosted Trees 7.Deep Learning 11.1.3 Comparing Algorithm "],
["11-2-library.html", "11.2 Library", " 11.2 Library Summary of Libraries And Functions Package Function Purpose 1 Base - R glm General linear model, use for logistic regression 2 ROCR prediction (logistic regression use only) create prediction object from score and label, simulate confusion table for multiple thresholds performance (logistic regression use only) create performane object from prediction object, and support plotting 3 rpart rpart Create a decision tree model plotcp Visualize decision tree model 4 rpart.plot prp 5 party ctree Create a conditional inference tree model 6 randomForest randomForest Create a random forest model 7 e1071 svm Create a SVM model Load the R libraries library(ROCR) library(rpart) library(rpart.plot) library(party) library(randomForest) library(e1071) "],
["11-3-sample-dataset.html", "11.3 Sample Dataset", " 11.3 Sample Dataset This chapter will use a Human Resource dataset that contain 12,000 training records. The dataset will be splited into training set (70%), and test set (30%). ### Load The Data set.seed(1234) raw = read.csv(&#39;./datasets/hr.csv&#39;) raw$left = as.factor(raw$left) ### Split Data train.rows = sample( 1:nrow(raw), 0.7 * nrow(raw) ) train = raw[train.rows,] test = raw[-train.rows,] 11.3.1 The Variables 11.3.1.1 Dependent Variable We want to predict who are the employees that are likely to leave the company. left: employee had left the company (binary) 11.3.1.2 Independent Variables Here are the variables that are relevant to predicting employees leaving. S: Satisfied Employee (binary, obtained through survey) NP: Number of Project Currently Handled LPE: Last Project Evaluation ANH: Average Number of Monthly Hours TIC: Time in company (in years) Newborn: Had a new born in last 12 months str(train) ## &#39;data.frame&#39;: 8400 obs. of 7 variables: ## $ S : num 0.41 0.57 0.83 0.77 0.64 0.72 0.9 0.16 0.34 0.86 ... ## $ LPE : num 0.52 0.42 0.91 0.96 0.43 0.89 0.96 0.97 0.76 0.74 ... ## $ NP : int 2 2 4 3 5 4 4 6 6 2 ... ## $ ANH : int 147 248 210 232 269 217 258 235 237 178 ... ## $ TIC : int 3 4 4 2 3 3 5 3 5 3 ... ## $ Newborn: int 0 0 0 1 0 0 0 0 0 0 ... ## $ left : Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 2 1 1 1 1 1 2 1 1 1 ... 11.3.2 Explore The Data Check the structure and samples. summary(raw) ## S LPE NP ANH ## Min. :0.0900 Min. :0.3600 Min. :2.000 Min. : 96.0 ## 1st Qu.:0.4800 1st Qu.:0.5700 1st Qu.:3.000 1st Qu.:157.0 ## Median :0.6600 Median :0.7200 Median :4.000 Median :199.5 ## Mean :0.6295 Mean :0.7166 Mean :3.802 Mean :200.4 ## 3rd Qu.:0.8200 3rd Qu.:0.8600 3rd Qu.:5.000 3rd Qu.:243.0 ## Max. :1.0000 Max. :1.0000 Max. :7.000 Max. :310.0 ## TIC Newborn left ## Min. :2.000 Min. :0.0000 0:10000 ## 1st Qu.:2.000 1st Qu.:0.0000 1: 2000 ## Median :3.000 Median :0.0000 ## Mean :3.229 Mean :0.1542 ## 3rd Qu.:4.000 3rd Qu.:0.0000 ## Max. :6.000 Max. :1.0000 head(raw) ## S LPE NP ANH TIC Newborn left ## 1 0.38 0.53 2 157 3 0 1 ## 2 0.80 0.86 5 262 6 0 1 ## 3 0.11 0.88 7 272 4 0 1 ## 4 0.72 0.87 5 223 5 0 1 ## 5 0.37 0.52 2 159 3 0 1 ## 6 0.41 0.50 2 153 3 0 1 Check any class bias (skewness) barplot(table(raw$left), ylab=&#39;left&#39;, xlab=&#39;1=Yes, 0=No&#39;) Calculate the Accuracy baseline - that is, probability of True(1) or False(0). Use whichever is higher. prop.table( table(raw$left) ) ## ## 0 1 ## 0.8333333 0.1666667 "],
["11-4-logistic-regression.html", "11.4 Logistic Regression", " 11.4 Logistic Regression 11.4.1 The Concept Logistic Regression is a actually a classification algorithm. It is used to predict: Binary outcome (1=Yes/Sucess, 0=No/Failure), given a set of independent variables. Multinomial outcome (more than two categories) - however, reference category for comparison must be specified, otehrwise, must run multiple regressions with different refence categories Logistic Regression as a special case of linear regression where: The outcome variable is categorical Ln of odds as dependent variable Linear regression cannot be used for classification because: Binary data does not have a normal distribution, which is a condition for many types of regressions Predicted values can go beyond 0 and 1, which violates the definition of probability Probabilities are often not linear 11.4.2 Assumptions Since logistic regression is related to linear combination of IVs, it share some common asssumptions regarding IVs and error terms: Dependent variable must be 1/0 type eg. ‘sucess/failure’, ‘male/female’, ‘yes/no’. Must not be ordinal and continous Observations must be independent Like OLS, Linearity between logit with all independent variables Like OLS, NO multicollinearity - if found, create interaction term, or drop one of the IVs Like OLS, error terms are assumed uncorrelated Although logit is a linear relation with independent variables, logistic regression (which use MLE) is different from OLS Linear Regression as below, due to the fact that DV is categorical and not continuuous: Can handle categorical independent variables Does not assume normality of DV and IVs: becauae \\(p\\) follow Bernoulli distribution Does not assume linearity between DV and IVs: because DV is categorical Does not assume homoscedasticity Does not assume normal errors 11.4.3 Equations The goal of logistic regression is to estimate \\(p\\) (the probability of ‘Success’) for a linear combination of the independent variables This is done by ‘linking’ the linear combination of independent variables to Bernoulli probability distribution (with domain from 0 to 1), to predict the probability of success The link function is called logit, which is the natural log of odds ratio. It is a linear function against independent variables: \\(logit(p) = ln(odds) = ln\\bigg(\\frac{p}{1-p}\\bigg) = \\beta_0 + \\beta_1 x_1 + ... + \\beta_n x_n\\) Derive Odd ratio by anti-log. It measures the ‘strength’ of IV in affecting the outcome, p: \\(odds = \\frac{p}{1-p} = e^{\\beta_0 + \\beta_1 x_1 + ... + \\beta_n x_n}\\) \\(p\\) can be further derived as below sigmoid function. \\(p\\) is non-linear against independent varibales : \\(p = \\frac{1}{1+e^{-(\\beta_0 + \\beta_1x_1 + ... + \\beta_nx_n)}}\\) The logistic graph below shows P(Y=1) vs \\(\\beta_0 + \\beta_1 x_1 + ... + \\beta_n x_n\\). **Intuitively, larger coefficients and independence varibles values increases the chance of being classified as Y=1*8 Compare the graph below to understand why logistic regression is better than linear regression in binary classification. However, it is simplied to illustrate single independent variable as multiple variables cannot be represented in 2D graph \\(\\quad\\) 11.4.4 High School Formula Some of the high scholl maths are useful for calculating the logistic scores. Remember them by heart. What is log \\(y = 10^a\\) \\(log(y) = a\\) \\(y = e^a\\) \\(ln(y) = a\\) Ln is log base e \\(ln(a) = log_e(a)\\) Basic properties of log \\(ln(1) = log(1) = 0\\) \\(ln(0) = log(0) = Inf\\) Log of division becomes substration of log \\(ln\\bigg(\\frac{a}{b}\\bigg) = ln(a) - ln(b)\\) Exponential of summation becomes multiplication of exponential \\(e^{a+b} = e^a * e^b\\) 11.4.5 Run The Code 11.4.5.1 Build The Model glm (formula, family=gaussian(link=identity), data) \\(\\quad\\) formula : example y ~ x1 + x2 + x3 \\(\\quad\\) family : binomial, gaussian, poisson, quasi etc \\(\\quad\\) link : logit:default binomial, identity:default gaussian, log:default poisson fit.logit = glm(left ~ ., family = binomial, data = train) # alternative: specifying link function # fit = glm(left ~ ., family = binomial (link=logit), data = train) 11.4.5.2 Prediction predict (object, newdata=NULL, type=c('link','response','term') \\(\\quad\\) object : model object from lm, glm, randomForest etc \\(\\quad\\) newdata : dataset to predict on, default use train data stored in model \\(\\quad\\) type : for probability output:'response Logistic regression always returns prediction with probability. Prediction on Test Data Specify newdata parameter to predict result for test data. pred.logit.train = predict(fit.logit, newdata = test, type = &#39;response&#39;) head(pred.logit.train) ## 1 2 5 7 9 11 ## 0.3283974 0.2830817 0.3371913 0.5068831 0.1243384 0.2580546 Prediction on Training Data Omitting newdata parameter, prediction default to training data. pred.logit.test = predict(fit.logit, type = &#39;response&#39;) head(pred.logit.test) ## 1365 7467 7311 7479 10328 7681 ## 0.29490977 0.36711466 0.10946995 0.01352364 0.08279127 0.09231202 "],
["11-5-decision-tree.html", "11.5 Decision Tree", " 11.5 Decision Tree 11.5.1 The Concept A decision tree is a machine learning algorithm that partitions the data into subsets It is a form of tree structure, containing three types of nodes: Root Node: no incoming, zero or more outgoing Internal/Branch Nodes: one incoming and two or more outgoing Terminal/Leaf Nodes: at the bottom of the tree, no outgoing edge \\(\\quad\\) The tree building process starts with a split at the root, and continues splitting through branches until no further splits can be made (leaf node reached) N number of splits will create N+1 leafs It classify observations by starting at the root of the tree and moving through it until a leaf node. 11.5.1.1 The Goal The goal of a decision tree is to encapsulate the training data in the smallest possible tree The rationale for minimizing the tree size is the logical rule that the simplest possible explanation for a set of phenomena is preferred over other explanations Small trees produce decisions faster than large trees, and they are much easier to look at and understand There are various methods and techniques to control the depth, or prune, of the tree 11.5.1.2 Advantages Popular among non-statistician, as the model is very easy to interpret None-parametric, therefore does not require normality assumption Support many data types: continuous, categorical, ordinal and binary variable Transformation is not required Useful for detecting important variables, interactions and identifying outliers 11.5.2 Tree Building Construcing a good decision tree involves below three processes: Splitting Criteria for each level of the tree: Selecting the variable to split Choosing the value to split Condition to stop splitting Pruning Tree Selection After a good decision tree model is built, you are ready to use it for prediction. 11.5.3 Splitting Criteria Splitting is the process of partitioning the data set into subsets, from root to leaf nodes. Splits are formed on a particular variable and in a particular location For each split, two determinations are made: Splitting variable - The predictor variable used for the split Split point - The set of values for the predictor variable (which are split between the left child node and the right child node) The goal of splitting each node is to select a combination of splitting variable and splitting point that minimizes the error (or maximizes the purity). Two algorithmns (with impurity measure) can be used to make this decision: The picture below illustrates spliting of simple tree with two variables, X and Y with 3 splits 11.5.3.1 Information Gain (based on Entropy value) \\(Entropy = \\sum_{i=1}^c{-p_i * log_2(p_i)}\\) \\(\\quad\\) c = number of class (1 if only single splitting variable), p = the probability of class \\(Information Gain = Entropy(Parent Node) - Entropy (Child Node)\\) Partition the data into subsets that contain instances with similar values (homogenous) Perfectly classified (completely homogeneous), the entropy is zero Maximum Entropy is 1 for (binary class) Weights probability of class by log(base=2) The parent node will select a variable that maximize information gain (smallest Child Entropy) as its child node 11.5.3.2 Gini (Gini Index) \\(Gini = 1 - \\sum_{i=1}^c (p_i)^2\\) \\(\\quad\\) c = number of class (1 if only single splitting variable), p = the probability of class Partition the data into subsets that contain instances with similar values (homogenous) Perfectly classified, Gini index would be zero (pure) Maximum Gini Index is 1 Split that has lowest Gini index value is chosen (most pure) Weights probability of class by square 11.5.3.3 Gini or Information Gain ? Gini index calculation is faster compared to Information gain becuase it doesn’t use log computation This could be a reason why gini is the default method in some machine learning packages Studies have shown that choice of impurity measures has little effect on the performance of the decision tree. This is because they are quite consistent with each other, as shown below: 11.5.3.4 Splitting Stops When All samples belongs to same class (pure) Most samples belongs to same class. This is generalization of the above approach with some error threshold There are no more variabels with samples to be partitioned There is no more samples for the branch test attributes Additionally, some program will implement extra parameters to control the size of the tree, eg. rpart::rpart.control 11.5.4 Pruning Tree 11.5.4.1 Pruning Benefits A decision tree can expanded until the it perfectly fit the training data (error is zero). However, it will perform poorly in test data (test error will be large). Such model does not generalize well to the test data, also known as overfitting. The solution is called pruning: Prunning a tree has greater impact than choosing the impurity measure The larger the tree (more nodes), the more complex the tree is, the more risk of overfitting The reason and benefits of smaller tree size are: Lower branches may be strongly affected by outliers. Pruning enables you to find the next largest tree and minimize this concern A simpler tree often avoids over-fitting 11.5.4.2 Pruning Process Pruning reduces the size of the tree using bottom-up appraoch: Pruning removes leaf nodes under original branch Pruning turns some branch nodes into leaf nodes (bottom-up approach) 11.5.4.3 When To Prune Pre-Pruning Stop the algorithm before it becomes a fully grown tree Post-Pruning Grow decision tree to its entirety Trim the nodes from bottom-up fashion If generalization error improves after trimming, replace the sub-tree by a leaf noes 11.5.5 Tree Selection The process of finding the smallest tree that fits the data This is the tree that yields the lowest cross-validated error 11.5.6 Run The Code 11.5.6.1 Build The Model rpart (formula, method=, na.action=na.rpart, model=FALSE, x=FALSE, y=FALSE, data=) \\(\\quad\\) formula : example y ~ x1 + x2 + x3 \\(\\quad\\) data : training data \\(\\quad\\) method : splitting method - 'class' for classification \\(\\quad\\) control : (optional) parameter used for controlling tree growth \\(\\quad\\) na.action: default action delete all observations with y is missing \\(\\quad\\) model: keep a copy of model in the result \\(\\quad\\) x : keep a copy of x matrix in result \\(\\quad\\) y : keep a copy of dependent variable y in result rpart.control specifies the paramters that limits the growth of tree: rpart.control( \\(\\quad\\) minsplit = 20, minbucket = round(minsplit/3), cp = 0.01,$\\quad$maxcompete = 4, maxsurrogate = 5, usesurrogate = 2,$\\quad$xval = 10,surrogatestyle = 0, maxdepth = 30)` The default rpart.control parameters specifies below criteria to be met before splitting a node: minsplit: minimum number of observations in node to be 20 minbucket: if splitted, both sides must have at least 20/3 observations cp: if splitted, the overall relative Cost of the entire tree must reduce by cp*T (T is number of terminal nodes). Cost is calculated as sum of square errors. Relative cost is compared to baseline (where there is no split, with baseline error is scaled to 1) Low cp value relaxed the error reduction expectation, hence creates larger tree maxdept: tree depth not more than 30 levels fit.dtree1 = rpart(left ~ ., method = &quot;class&quot;, data=train, cp = 0.001, minbucket=10) fit.dtree2 = rpart(left ~ ., method = &quot;class&quot;, data=train) 11.5.6.2 Prediction predict (object, newdata=NULL, type=c('response','class') \\(\\quad\\) object : model object from lm, glm, randomForest etc \\(\\quad\\) newdata : dataset to predict on, default use train data stored in model \\(\\quad\\) type : default-'prob', which is probability output, 'class' - class output Probability Prediction Probability output is flexble so that researcher can tune the threshold and perform ROC analysis Output probability as multiple columns, each column represent resutls for one class. (for binary prediction, it will be just two columns) pred.dtree.train1 = predict(fit.dtree1, type=&#39;prob&#39;) # default newdata is model train set pred.dtree.test1 = predict(fit.dtree2, newdata=test) # default probability output head(pred.dtree.train) ## 1365 7467 7311 7479 10328 7681 ## 1 0 0 0 0 0 ## Levels: 0 1 Class Prediction (threshold = 0.5) Threshold of 0.5 applied Single column output pred.dtree.train2 = predict(fit.dtree1, type = &quot;class&quot;) # default newdata is model train set pred.dtree.test2 = predict(fit.dtree2, type = &quot;class&quot;, newdata=test) head(pred.dtree.train) ## 1365 7467 7311 7479 10328 7681 ## 1 0 0 0 0 0 ## Levels: 0 1 11.5.6.3 Confusion Table Confusion table below proofs that type=‘class’ is indeed prediction with threshold &gt;0.5. table(test$left, pred.dtree.test1[,2] &gt; 0.5, dnn=c(&quot;Actual&quot;, &quot;Predicted&quot;)) ## Predicted ## Actual FALSE TRUE ## 0 2953 29 ## 1 53 565 table(test$left, pred.dtree.test2, dnn=c(&quot;Actual&quot;, &quot;Predicted&quot;)) ## Predicted ## Actual 0 1 ## 0 2953 29 ## 1 53 565 11.5.6.4 Visualize The Model Visualize with Text fit.dtree ## n= 8400 ## ## node), split, n, loss, yval, (yprob) ## * denotes terminal node ## ## 1) root 8400 1382 0 (0.835476190 0.164523810) ## 2) S&gt;=0.465 6403 410 0 (0.935967515 0.064032485) ## 4) TIC&lt; 4.5 5688 56 0 (0.990154712 0.009845288) ## 8) ANH&lt; 286.5 5681 51 0 (0.991022707 0.008977293) * ## 9) ANH&gt;=286.5 7 2 1 (0.285714286 0.714285714) * ## 5) TIC&gt;=4.5 715 354 0 (0.504895105 0.495104895) ## 10) LPE&lt; 0.805 262 10 0 (0.961832061 0.038167939) * ## 11) LPE&gt;=0.805 453 109 1 (0.240618102 0.759381898) ## 22) ANH&lt; 215.5 71 6 0 (0.915492958 0.084507042) * ## 23) ANH&gt;=215.5 382 44 1 (0.115183246 0.884816754) ## 46) S&lt; 0.715 29 5 0 (0.827586207 0.172413793) * ## 47) S&gt;=0.715 353 20 1 (0.056657224 0.943342776) ## 94) NP&lt; 3.5 10 0 0 (1.000000000 0.000000000) * ## 95) NP&gt;=3.5 343 10 1 (0.029154519 0.970845481) * ## 3) S&lt; 0.465 1997 972 0 (0.513269905 0.486730095) ## 6) NP&gt;=2.5 1264 383 0 (0.696993671 0.303006329) ## 12) S&gt;=0.115 920 39 0 (0.957608696 0.042391304) * ## 13) S&lt; 0.115 344 0 1 (0.000000000 1.000000000) * ## 7) NP&lt; 2.5 733 144 1 (0.196452933 0.803547067) ## 14) LPE&gt;=0.575 92 5 0 (0.945652174 0.054347826) * ## 15) LPE&lt; 0.575 641 57 1 (0.088923557 0.911076443) ## 30) LPE&lt; 0.445 25 0 0 (1.000000000 0.000000000) * ## 31) LPE&gt;=0.445 616 32 1 (0.051948052 0.948051948) ## 62) S&lt; 0.35 18 4 0 (0.777777778 0.222222222) * ## 63) S&gt;=0.35 598 18 1 (0.030100334 0.969899666) ## 126) ANH&lt; 125 8 0 0 (1.000000000 0.000000000) * ## 127) ANH&gt;=125 590 10 1 (0.016949153 0.983050847) ## 254) ANH&gt;=165.5 8 1 0 (0.875000000 0.125000000) * ## 255) ANH&lt; 165.5 582 3 1 (0.005154639 0.994845361) * Visualize with Graph Use rpart::prp to plot rpart model (prp is shortform for ‘plot rpart’). Notice that the graph has the below characteristics: Nodes at the same depth level addes up to 100% probability Leafs nodes are formed at different depth levels All leaf nodes adds up to 100% probability prp(fit.dtree, type = 2, extra = 104, fallen.leaves = TRUE, main=&quot;Decision Tree&quot;) 11.5.6.5 Cross Validation Analysis Yes, rpart has built-in cross validation, calculated when a model is created ! Use rpart::cptable to find out the lowest value of cross validation error (column xerror), and its cross standard deviation (column xstd) Use xerror + xstd as the cutting point for best tree size for optimization (pruning), see plotcp below cptable is stored in the model built with rpart. printcp(fit.dtree) #fit.dtree$cptable # alternative ## ## Classification tree: ## rpart(formula = left ~ ., data = train, method = &quot;class&quot;, control = rpart.control(cp = 0.001)) ## ## Variables actually used in tree construction: ## [1] ANH LPE NP S TIC ## ## Root node error: 1382/8400 = 0.16452 ## ## n= 8400 ## ## CP nsplit rel error xerror xstd ## 1 0.1609986 0 1.000000 1.00000 0.0245874 ## 2 0.0850217 3 0.429088 0.42909 0.0169872 ## 3 0.0593343 5 0.259045 0.26194 0.0134673 ## 4 0.0426918 6 0.199711 0.20260 0.0119045 ## 5 0.0180897 7 0.157019 0.16064 0.0106378 ## 6 0.0137482 8 0.138929 0.14327 0.0100611 ## 7 0.0072359 9 0.125181 0.13025 0.0096034 ## 8 0.0057887 11 0.110709 0.12663 0.0094719 ## 9 0.0043415 12 0.104920 0.11505 0.0090373 ## 10 0.0021708 13 0.100579 0.11216 0.0089251 ## 11 0.0010000 14 0.098408 0.11143 0.0088968 Visualize cptable with rpart::plotcp. size of tree means the number of nodes in the tree, it is the number of splits + 1 Horizontal line is the valued at lowest xerror + its xstd Optimum tree size for pruning is below the horizontal line, use this chart to determine the optimum tree size, and its cp value, prune with this cp value plotcp(fit.dtree) 11.5.6.6 Pruning Tree The decision tree was originally built with some constrains (rpart::control) to limits its tree size The tree size can be further reduced by pruning, with the use the cp value in prunning command Prune the tree size that yields lowest xerror, based on the cp analysis above Pruning creates new, reduced nodes decision tree model For this example, we purposely prune the tree to one size smaller, for the sake of demonstration Bigger tree size (lower complexity parameter, cp) means higher accuracy of the model to training data, hence it always reduce the errors (error in $cptable). However, cross validation error (xerror) may go up and down as the tree size increases To choose pruning cp value, first choose the tree size: From cptable, pruning cp value = sqrt(cpvalue for tree-size N * cpvalue for tree-size N-1) From plotcp, choose a tree size, pruning cp value = corresponding cpvalue at the y-axis Remember, tree size = nsplit + 1 fit.dtree.prune = prune(fit.dtree, cp = 0.005) printcp(fit.dtree.prune) ## ## Classification tree: ## rpart(formula = left ~ ., data = train, method = &quot;class&quot;, control = rpart.control(cp = 0.001)) ## ## Variables actually used in tree construction: ## [1] ANH LPE NP S TIC ## ## Root node error: 1382/8400 = 0.16452 ## ## n= 8400 ## ## CP nsplit rel error xerror xstd ## 1 0.1609986 0 1.00000 1.00000 0.0245874 ## 2 0.0850217 3 0.42909 0.42909 0.0169872 ## 3 0.0593343 5 0.25904 0.26194 0.0134673 ## 4 0.0426918 6 0.19971 0.20260 0.0119045 ## 5 0.0180897 7 0.15702 0.16064 0.0106378 ## 6 0.0137482 8 0.13893 0.14327 0.0100611 ## 7 0.0072359 9 0.12518 0.13025 0.0096034 ## 8 0.0057887 11 0.11071 0.12663 0.0094719 ## 9 0.0050000 12 0.10492 0.11505 0.0090373 plotcp(fit.dtree.prune) 11.5.6.7 Prediction We would like to compare the model performance: a. Original Model with training set b. Pruned Model with training set c. Pruned Model with test set Overall, we can see that the performance dropped slightly after prunning for datset. # original model with training set dtree.pred.train = predict(fit.dtree, newdata=train, type=&quot;class&quot;) eval.binclass(score=dtree.pred.train, label=train$left)[2:5] ## accuracy recall precision specificity ## 1 0.9838095 0.9124457 0.9882445 0.9978626 # original model with test set dtree.pred.test1 = predict(fit.dtree, newdata=test, type=&quot;class&quot;) eval.binclass(score=dtree.pred.test1, label=test$left)[2:5] ## accuracy recall precision specificity ## 1 0.9816667 0.9110032 0.9808362 0.9963112 # pruned model with training set dtree.pred.train.pruned = predict(fit.dtree.prune, data=train, type=&quot;class&quot;) eval.binclass(score=dtree.pred.train.pruned, label=train$left)[2:5] ## accuracy recall precision specificity ## 1 0.9827381 0.9095514 0.9843383 0.9971502 # pruned model with test set dtree.pred.test2 = predict(fit.dtree.prune, newdata=test, type=&quot;class&quot;) eval.binclass(score=dtree.pred.test2, label=test$left)[2:5] ## accuracy recall precision specificity ## 1 0.9802778 0.9110032 0.9723661 0.9946345 "],
["11-6-conditional-inference-tree.html", "11.6 Conditional Inference Tree", " 11.6 Conditional Inference Tree "],
["11-7-random-forest.html", "11.7 Random Forest", " 11.7 Random Forest 11.7.1 The Concept Random forest is a type of Ensemble Method, which enjoy all the ensemble benefits. Random forest model contain large amount of decision trees as oppose to tranditional single decision tree. 11.7.1.1 How Does It Work ? Divides training data into large amount random subsets, each subset will draw samples from training data using bagging method (with replacement). Each subset will contain the same number of observations as training set Create a decision tree for each subset, resulting multiple trees (henced called Forest) Each tree uses different set of random variables for splitting (not all variables are being used in each tree) When doing prediction, all tress will votes and majority wins Random Forest Process Flow 11.7.1.2 The Beliefs Random Forest provides much better accuracy compared to single decision tree. The major belief why this works are: Most of the trees can provide correct prediction for most part of the data The tree are making mistake at different place Hence, major votes for classification result expected to be closer to correct classification 11.7.1.3 Advantages and Disadvantages Advantages A single decision tree suffers from the phenomena of overfitting, as the signle tree model memorize the data Randomforest uses bagging (with replacement) to ‘artificially’ creates different types of dataset, hence reducing the phenomena of memorizing Cross validation is not required, since each subset are already being randomized It is usually more accurate than single tree, but no guarantee Disadvantage Ensemble method such as random forest is not interpretable 11.7.2 Run The Code 11.7.2.1 Determine Class or Probability Outcome randomForest can build both class and probability model, depending on the dependent variable: If the depenedent variable is factor, it will build a model for class prediction If the depenedent variable is number, it will build a model for probability prediction For 0/1 dependent variable, if not factorized, it is treated as number, and therefore for probability prediction. Here we convert all independent variable to factor for class prediction. This step is different from rpart (single decision tree), as rpart does not require this step. rpart can predict both class and probability by just specifying the type=‘class’ or type=‘response’ train$left = as.factor(train$left) test$left = as.factor(test$left) 11.7.2.2 Build The Model library(randomForest) set.seed(1234) fit.forest &lt;- randomForest(left~., data=train) 11.7.2.3 Prediction Prediction syntax is similar to rpart decision tree. predict (object, newdata=NULL, type=c('response','class') \\(\\quad\\) object : model object from lm, glm, randomForest etc \\(\\quad\\) newdata : dataset to predict on, default use train data stored in model \\(\\quad\\) type : default-'prob', which is probability output, 'class' - class output Probability Prediction Probability output is flexble so that researcher can tune the threshold and perform ROC analysis Output probability as multiple columns, each column represent resutls for one class. (for binary prediction, it will be just two columns) pred.rf.train1 = predict(fit.forest, type=&#39;prob&#39;) # default newdata is model train set pred.rf.test1 = predict(fit.forest, type=&#39;prob&#39;, newdata=test) head(pred.rf.train) ## 1365 7467 7311 7479 10328 7681 ## 1 0 0 0 0 0 ## Levels: 0 1 Class Prediction (threshold = 0.5) Threshold of 0.5 applied Single column output pred.rf.train2 = predict(fit.forest, type = &#39;class&#39;) # default newdata is model train set pred.rf.test2 = predict(fit.forest, newdata=test) # default type=&#39;class&#39; as class output head(pred.dtree.train) ## 1365 7467 7311 7479 10328 7681 ## 1 0 0 0 0 0 ## Levels: 0 1 11.7.2.4 Confusion Table Confusion table below proofs that type=‘class’ is indeed prediction with threshold &gt;0.5. #forest.pred &lt;- predict(fit.forest, newdata=test) table(test$left, pred.rf.test1[,2] &gt; 0.5, dnn=c(&quot;Actual&quot;, &quot;Predicted&quot;)) ## Predicted ## Actual FALSE TRUE ## 0 2975 7 ## 1 48 570 table(test$left, pred.rf.test2, dnn=c(&quot;Actual&quot;, &quot;Predicted&quot;)) ## Predicted ## Actual 0 1 ## 0 2975 7 ## 1 48 570 11.7.2.5 Analyize Vairable Importance What Variables Are Important ? randomForest::importance output matrix of all variable used in the randomForest and their contribution in term of decreasing GiniIndex (decreasing impurity and increasing purity) The higher the value, means the variable is more important in contributing to accuracy of the model forest.imp = importance(fit.forest) forest.imp[order(forest.imp, decreasing=T),, drop=FALSE] ## order descendingly ## MeanDecreaseGini ## S 782.815405 ## NP 419.376853 ## TIC 400.909756 ## ANH 358.655346 ## LPE 289.506732 ## Newborn 9.953228 Visualize The Importance randomForest::varImpPlot comes with handy plotting utility to plot the variable importance data. varImpPlot(fit.forest) 11.7.2.6 Frequent Used Variables Random forest models work by building a large collection of trees. As a result, we lose some of the interpretability that comes with CART in terms of seeing how predictions are made and which variables are important However, one of the way to give us insight into which variables are important, other than the decrease of impurity above, we can calculate how frequent a variable is being used for decision making (splitting) randomForest::varUsed returns a vector of frequency of each variable used. However, it doesn’t comes with the variable name, which can be obtained from $forest$xlevels. randomForest::varUsed(x, by.tree=FALSE, count=TRUE) \\(\\quad\\) x : model object of randomForest \\(\\quad\\) by.tree : FALSE, default display per variable appearance, and not per tree \\(\\quad\\) count : TRUE, default aggregate all varibales appearance into frequency, otherwise display individual appearance vu = varUsed(fit.forest, count = TRUE) vusorted = sort(vu, decreasing = FALSE, index.return = TRUE) dotchart(vusorted$x, names(fit.forest$forest$xlevels[vusorted$ix])) "],
["11-8-svm.html", "11.8 SVM", " 11.8 SVM library(e1071) set.seed(1234) fit.svm &lt;- svm(class~., data=train) fit.svm #By default, the svm() function sets gamma to 1 / (number of predictors) and cost to 1. svm.pred &lt;- predict(fit.svm, na.omit(test)) svm.perf &lt;- table(na.omit(test)$class, svm.pred, dnn=c(&quot;Actual&quot;, &quot;Predicted&quot;)) svm.perf set.seed(1234) tuned &lt;- tune.svm(left~., data=train, gamma=10^(-6:1), cost=10^(-10:10)) tuned fit.svm &lt;- svm(left~., data=train, gamma=.01, cost=1) fit.svm svm.pred &lt;- predict(fit.svm, na.omit(test)) svm.perf &lt;- table(na.omit(test)$class, svm.pred, dnn=c(&quot;Actual&quot;, &quot;Predicted&quot;)) svm.perf 11.8.1 Run The Code "],
["11-9-performance-measurement.html", "11.9 Performance Measurement", " 11.9 Performance Measurement There are many performance measurement used for binary classification. Here are the rules of thumb which one to use: Recall: If you don’t mind getting some inaccurate result, as long as you get as much correct ones Precision: If you demand rate of correctness and willing to reject some correct results F1 Score: For a more balanced measurement, taking into consideraton both recall and precision 11.9.1 Confusion Matrix Confusion Matrix and Performance Measurement 11.9.1.1 Accuracy Accuracy answers the question: From the total samples, how many had been correctly predicted by the model ? \\(Accuracy = \\frac{TP+TN}{TP+TN+FP+FN}\\) This measurement is useful when the both classes are balanced (that is, the number of TP and TN cases are almost balanced) In practice, it seems that the best accuracy is usually achieved when the cutpoint is near the Probability(actual TRUE) Accuracy is completely usessless in highly skewed class. For example, with a disease that only affects 1 in a million people a completely bogus screening test that always reports “negative” will be 99.9999% accurate 11.9.1.2 Recall Recall answers the question: Out of all actual positive samples, how many were correctly predicted by classifiers ? \\(Recall = \\frac{TP}{TP+FN}\\) 11.9.1.3 Precision Precison answers the question: Out of all the samples classifier predicted as positive, what fraction were correct ? \\(Precision = \\frac{TP}{TP+FP}\\) 11.9.1.4 F1 Score F1 score is the harmonic mean of Precision and Recall. Intuitively, F1 Score is the weighted average of Precision and Recall. It takes into account all three measures: TP, FP and FN \\(Precision = 2*\\frac{Recall * Precision}{Recall + Precision}\\) F1 is usually more useful than accuracy, especially if you have an unbalanced class distribution 11.9.2 Performance related to Logistic Regression 11.9.2.1 Area Under ROC (AUROC) ROC curve is basically a graph of TPR vs FPR (some refer to as Recall vs (1-Sensitivity), plotted for different thresholds Comparing two different models, the model with higher AUROC is considered to have higher overall Accuracy AUROC (Area Under ROC) measures : AUC of 0.5: means the model is as good as tossing a coin, worthless AUC of 1.0: means for all cutoff points, TPR=1 and FPR=0. Intuitively it means, all samples had been correctly classified into TP (TPR=1) and TN(FPR=0), and there is no FP and FN. Ultiamtely it means Accuracy is 100% AUROC and Thresholds 11.9.2.2 Cutoff Threshold Impact Cutoff threshold direclty influence the value of TP, FP, TN, FN. If cutoff threshold is lowered (lower probability to classify as Postive), the results are: More linient and hence more samples will be classified as Positive More predicted Positives means more TP and FP, hence TPR and FPR increases However, TPR and FPR increases at different rate: If TPR increases faster than FPR -&gt; this is good, as the lowered threshold generated more TP than FP If FPR increases faster then TPR -&gt; this is not good, as the lowered threhsold generated more FP than TP The cutoff with highest TPR/FPR value is the optimal means optimum point whereby It is possible to discover the optimum cutoff by finding the cutoff with highest TPR/FPR Different threshold produces different performance metrics (Accuracy, Recall, Precision and Specificity and F1-score). As an example, picture below shows how threshold influences the ROC curve. Threshold and ROC The only way to estimate the optimum threshold for each of the performance measurement will be to measure them for a wide range of threshold. 11.9.2.3 Cutoff Visualization Selecting a cutoff threshold depends on the objectives of the researcher. To help understanding the how cutoff changes the performance metircs, try visualize them in below graph: 1.Threshold vs Accuracy 2.Threshold vs Recall (TPR) 3.Threshold vs TPR/FPR 4.Threshold vs Precision 5.Threshold vs F1 Score 6.ROC Curve (TPR vs FPR) 11.9.3 Model Evaluation 11.9.3.1 Compare With Baseline In an highly unbalanced dataset (eg. patients that is diagnosed with cancer skewed towards negative). Hence, it is essential to make a lazy baseline comparison with simply classifying every records with negative (cutoff at 1) or positive (cutoff at 0). Put in mind of the dataset used for comparison: Use data from training set as model baseline, when concerning training performance Use dta from test set as model baseline, when concerning test performance Model Accuracy is not better than ‘lazy’ baseline ! In highly bias class, in term of accuracy, the model usually unable to outperform the baseline by good marign Hence accuracy is not a good measurement in such case Model is still useful if the research objective is towards other measurement usch as recall, precision Adjust the threshold to achieve optimum better objectives 11.9.3.2 Combination of Variables Model AIC to measure the usefullness. It is like R-square in linear regression. Use it to compare model with different combinations of variables. Since logistic regression is a actual linear regression of ln(odds), multicolinearity rule apply. 11.9.4 Run The Code 11.9.4.1 General Function This is a custom general function to evaluate all key metrics for Classification. eval.bincalss ( score=NULL, label=NULL, cutoff=NULL) score: predicted value, either probability or binary label: actual value cutoff: threshold (0-1) if score is probability eval.binclass = function(score = NULL, label = NULL, cutoff = NA) { actual = factor(as.logical(as.integer(as.character(label))), levels = c(TRUE, FALSE)) if (is.na(cutoff)) predicted = factor(as.logical(as.integer(as.character(score))), levels = c(TRUE, FALSE)) # binary else predicted = factor(as.logical(score &gt; cutoff), levels = c(TRUE, FALSE)) # regression ct = table(actual, predicted, useNA = &#39;no&#39;, exclude = c(NA)) #confusion table tp = ct[1] fp = ct[2] fn = ct[3] tn = ct[4] accuracy = (ct[1] + ct[4]) / (sum(ct)) recall = ct[1] / (ct[1] + ct[3]) precision = ct[1] / (ct[1] + ct[2]) fpr = ct[2] / (ct[2] + ct[4]) specificity = ct[4] / (ct[2] + ct[4]) tpr_fpr = recall / (1 - specificity) fscore = 2 * (precision * recall / (precision + recall)) data.frame(cutoff = cutoff, accuracy = accuracy, recall = recall, precision = precision, specificity = specificity, fscore = fscore, fpr = fpr, tpr_fpr = tpr_fpr, tp = tp, fp = fp, fn = fn, tn = tn) } eval.binclass(score=pred.logit, label=test$left, cutoff=0.5) ## cutoff accuracy recall precision specificity fscore fpr ## 1 0.5 0.8147222 0.1957929 0.4158076 0.9429913 0.2662266 0.05700872 ## tpr_fpr tp fp fn tn ## 1 3.434437 121 170 497 2812 11.9.4.2 ROCR::Construct the Performance Metric ROCR::predictionis the main function to crate prediction object that contains key data such as thresholds, TP, FP, TN, FP. With these data points, we can plot and calculate multiple performacne and visualization. It takes only two inputs: 1. Score (probability of being POSITIVE) 2. Label (actual POSITVE) rocr.pred = prediction(pred.logit, test$left) rocr.metrics = data.frame( cutoff = rocr.pred@cutoffs[[1]], accuracy = (rocr.pred@tp[[1]] + rocr.pred@tn[[1]]) / (rocr.pred@tp[[1]] + rocr.pred@tn[[1]] + rocr.pred@fp[[1]] + rocr.pred@fn[[1]]), tpr = rocr.pred@tp[[1]] / (rocr.pred@tp[[1]] + rocr.pred@fn[[1]]), fpr = rocr.pred@fp[[1]] / (rocr.pred@fp[[1]] + rocr.pred@tn[[1]]), ppv = rocr.pred@tp[[1]] / (rocr.pred@tp[[1]] + rocr.pred@fp[[1]]) ) rocr.metrics$fscore = 2 * (rocr.metrics$tpr * rocr.metrics$ppv) / (rocr.metrics$tpr + rocr.metrics$ppv) rocr.metrics$tpr_fpr = rocr.metrics$tpr / rocr.metrics$fpr 11.9.4.3 ROCR::List The Performance Table The code below discover the optimum threshold for few metrices. Threhold for the highest estiamtes is summarized. ## discovery the optimal threshold for various metrics rocr.best = rbind( best.accuracy = c(max = max(rocr.metrics$accuracy, na.rm = TRUE), cutoff = rocr.metrics$cutoff[which.max(rocr.metrics$accuracy)]), best.ppv = c(max = max(rocr.metrics$ppv, na.rm = TRUE), cutoff = rocr.metrics$cutoff[which.max(rocr.metrics$ppv)]), best.recall = c(max = max(rocr.metrics$tpr, na.rm = TRUE), cutoff = rocr.metrics$cutoff[which.max(rocr.metrics$tpr)]), best.fscore = c(max = max(rocr.metrics$fscore, na.rm = TRUE), cutoff = rocr.metrics$cutoff[which.max(rocr.metrics$fscore)]), best.tpr_fpr = c(max = max(rocr.metrics$tpr_fpr, na.rm = TRUE), cutoff = rocr.metrics$cutoff[which.max(rocr.metrics$tpr_fpr)]) ) kable(rocr.best) max cutoff best.accuracy 0.8383333 0.2564513 best.ppv 0.5214797 0.2564513 best.recall 1.0000000 0.0073737 best.fscore 0.6031102 0.2490565 best.tpr_fpr 5.2584316 0.2564513 11.9.4.4 ROCR::Visualize The Data and Performance Plot TP, Tn, FP and FN These data are available in the ROCR::prediction object. plot (rocr.pred@cutoffs[[1]], rocr.pred@tp[[1]],xlim=c(0,1), ylim=c(0,12000), col=&#39;green&#39;) lines(rocr.pred@cutoffs[[1]], rocr.pred@tn[[1]], xlim = c(0, 1), ylim = c(0, 12000),col=&#39;red&#39;) lines(rocr.pred@cutoffs[[1]], rocr.pred@fp[[1]], xlim = c(0, 1), ylim = c(0, 12000), col=&#39;blue&#39;) lines(rocr.pred@cutoffs[[1]], rocr.pred@fn[[1]], xlim = c(0, 1), ylim = c(0, 12000),col=&#39;orange&#39;) legend(&quot;top&quot;, inset = .05, cex = 1, title = &quot;Legend&quot;, c(&quot;TP&quot;, &quot;TN&quot;, &quot;FP&quot;,&quot;FN&quot;), horiz = TRUE, lty = c(1, 1), lwd = c(2, 2), col = c(&quot;green&quot;, &quot;red&quot;, &quot;blue&quot;,&#39;orange&#39;), bg = &quot;grey96&quot;) Plot ROC Curve We can build a ROCR::performance object, and plot it ! AUC is calculated with ROCR::performance as well. rocr.perf = performance(rocr.pred, measure = &quot;tpr&quot;, x.measure = &quot;fpr&quot;) rocr.auc = as.numeric(performance(rocr.pred, &quot;auc&quot;)@y.values) plot(rocr.perf, lwd = 3, colorize = TRUE, print.cutoffs.at = seq(0, 1, by = 0.1), text.adj = c(-0.2, 1.7), main = &#39;ROC Curve&#39;) mtext(paste(&#39;auc : &#39;, round(rocr.auc, 5))) abline(0, 1, col = &quot;red&quot;, lty = 2) Plot Accuracy rocr.perf = performance(rocr.pred, measure = &quot;acc&quot;) best.x = rocr.perf@x.values[[1]][which.max(rocr.perf@y.values[[1]])] best.y = max(rocr.perf@y.values[[1]]) plot(rocr.perf, main = &#39;Accuracy vs Cutoff&#39;, xlim = c(0, 1), ylim = c(0, 1)) abline(v = best.x, col = &#39;red&#39;, lty = 2) abline(h = best.y, col = &#39;red&#39;, lty = 2) text(best.x + 0.1, 0.05, round(best.x, 5), col = &quot;red&quot;) text(0.05, best.y + 0.05, round(best.y, 5), col = &quot;red&quot;) Plot Precesion rocr.perf = performance(rocr.pred, measure = &quot;prec&quot;) best.x = rocr.perf@x.values[[1]][which.max(rocr.perf@y.values[[1]])] best.y = max(rocr.perf@y.values[[1]][is.finite(rocr.perf@y.values[[1]])]) plot(rocr.perf, main = &#39;Precision vs Cutoff&#39;, xlim = c(0, 1), ylim = c(0, 1)) abline(v = best.x, col = &#39;red&#39;, lty = 2) abline(h = best.y, col = &#39;red&#39;, lty = 2) text(best.x + 0.1, 0.05, round(best.x, 5), col = &quot;red&quot;) text(0.05, best.y + 0.05, round(best.y, 5), col = &quot;red&quot;) Plot Multiple Metrices Into One Graph As we can see, ROCR::performance is good to build measure and plot for single y-axis measurement. It unfortunately does not support plotting multiple y-axis into one graph. Threfore, we shall build the plot using metrices table constructed earlier (variable rocr.metrics). ### set initial margin of the plot par(mar = c(5, 5, 4, 6)) ## plot graph on left hand side scale plot(rocr.metrics$cutoff, rocr.metrics$tpr, axes = FALSE,, ylab = &#39;&#39;, xlab = &#39;&#39;, col = &#39;green&#39;, main = &#39;Multiple Performances vs Cutoff&#39;, ylim = c(0, 1)) lines(rocr.metrics$cutoff, rocr.metrics$fpr, col = &#39;red&#39;, ylim = c(0, 1)) axis(1, pretty(range(rocr.metrics$cutoff[is.finite(rocr.metrics$cutoff)]), 10)) axis(2, ylim = c(0, 1), col = &quot;black&quot;, las = 1) ## las=1 makes horizontal labels lines(rocr.metrics$fpr, rocr.metrics$tpr, lwd = 3, col = &#39;black&#39;, ylim = c(0, 1), xlim = c(0, 1)) lines(rocr.metrics$cutoff, rocr.metrics$accuracy, col = &#39;purple&#39;, ylim = c(0, 1), xlim = c(0, 1)) lines(rocr.metrics$cutoff, rocr.metrics$ppv, col = &#39;cyan&#39;, ylim = c(0, 1), xlim = c(0, 1)) lines(rocr.metrics$cutoff, rocr.metrics$fscore, col = &#39;pink&#39;, ylim = c(0, 1), xlim = c(0, 1)) abline(0, 1, lty = 2, col = &#39;black&#39;) ## plot graph on right hand side scale par(new = TRUE) plot(rocr.metrics$cutoff, rocr.metrics$tpr_fpr, axes = FALSE, ylab = &#39;&#39;, xlab = &#39;&#39;, col = &#39;orange&#39;) axis(4, col = &quot;orange&quot;, las = 1) ## las=1 makes horizontal labels best = rocr.metrics$cutoff[which.max(rocr.metrics$tpr_fpr[is.finite(rocr.metrics$tpr_fpr)])] abline(v = best, col = &#39;red&#39;, lty = 2) text(best + 0.075, 0.05, round(best, 5), col = &quot;red&quot;) ## axis labels mtext(&quot;cutoff&quot;, side = 1, col = &quot;black&quot;, line = 2.5) mtext(&quot;tpr, fpr, accuracy, precision, f1-scaore&quot;, side = 2, col = &quot;black&quot;, line = 2.5) mtext(&quot;tpr/fpr&quot;, side = 4, col = &quot;black&quot;, line = 2.5) ## legend legend(&quot;topright&quot;, inset = .05, cex = 1, title = &quot;Legend&quot;, c(&quot;TPR&quot;, &quot;FPR&quot;, &quot;TPR/FPR&quot;, &quot;ROC&quot;, &quot;Accuracy&quot;, &quot;Precision&quot;, &quot;F1-Score&quot;), horiz = FALSE, lty = c(1, 1), lwd = c(2, 2), col = c(&quot;green&quot;, &quot;red&quot;, &quot;orange&quot;, &#39;black&#39;, &#39;purple&#39;, &#39;cyan&#39;, &#39;pink&#39;), bg = &quot;grey96&quot;) -->"]
]
