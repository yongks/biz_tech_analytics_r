[
["8-clustering-analysis.html", "Chapter 8 Clustering Analysis", " Chapter 8 Clustering Analysis Cluster analysis is a data-reduction technique designed to uncover subgroups of observations within a dataset It reduce a large number of observations to a much smaller number of clusters or types A cluster is defined as a group of observations that are more similar to each other than they are to the observations in other groups "],
["8-1-packages.html", "8.1 Packages", " 8.1 Packages Below summarizes all packages, their functions and purposes used in this Clustering Analysis chapter. Package Function Purpose 1 Base - R dist Calculate distance between data points with methods: euclidean, maximum, cenberra, minkowski, manhattan scale Scale data (minus mean, div by SD) hclust Build hirerchical cluster model (no cutting) kmeans Build k-means cluster model 2 factoextra fviz_nbclust Optimum number of cluster (K) visual analysis, methods: wss, silhoutte hcut Build hirerchical cluster model (with cutting) fviz_dend Visualize h-cluster model in dendrogram graph as ggplot object fviz_cluster Visualize data points with cluster grouping as ggplot object 3 NbClust NbClust 30 indices to analyze optimal number of cluster, K 4 flexclust randIndex Agreement measure for two cluster results "],
["8-2-application.html", "8.2 Application", " 8.2 Application Business : researchers use cluster analysis for customer segmentation. Customers are arranged into clusters based on the similarity of their demographics and buying behaviours. Marketing campaings are then tailored to appeal to the groups. Psychological: researchers cluster data on the symptoms and demographics of depressed patients, seeking to uncover subtypes of depression, with the hope of finding more effective targeted treatments and a better understanding of the disorder. Medical: researchers use cluster analysis to help catalog gene-expression patterns obtained from DNA microarray data. This can help them to understand normal growth and development and the underlying causes of many human diseases Information Retrieval: The world wide web consists of billions of Web pages, and the results of a query to a search engine can return thousands of pages. Clustering can be used to group these search results into a small number of clusters, each of which captures a particular aspect of the query. For example, a query of “movie” might return Web pages grouped into categories such as reviews, trailers, starts and theaters. Each category (Cluster) can be bnorken into subcategories (sub-clusters), producing a hierachical structure that further assists a user’s exploration of the query results. "],
["8-3-sample-data.html", "8.3 Sample Data", " 8.3 Sample Data Sample data used in this chapter emulate two dimensional data points with three groups with clear grouping when visualize. set.seed(1234) my.df = data.frame( id = paste(&#39;ID_&#39;, 1:15, sep = &#39;&#39;), grp = c(rep(&#39;G1&#39;, 5), rep(&#39;G2&#39;, 5), rep(&#39;G3&#39;, 5)), value1 = c( round(rnorm(5, mean = 10, sd = 3)), round(rnorm(5, mean = 10, sd = 3)), round(rnorm(5, mean = 30, sd = 3))), value2 = c( round(rnorm(5, mean = 10, sd = 3)), round(rnorm(5, mean = 20, sd = 3)), round(rnorm(5, mean = 20, sd = 3))), stringsAsFactors = F ) rownames(my.df) = my.df[,1] str(my.df) ## &#39;data.frame&#39;: 15 obs. of 4 variables: ## $ id : chr &quot;ID_1&quot; &quot;ID_2&quot; &quot;ID_3&quot; &quot;ID_4&quot; ... ## $ grp : chr &quot;G1&quot; &quot;G1&quot; &quot;G1&quot; &quot;G1&quot; ... ## $ value1: num 6 11 13 3 11 12 8 8 8 7 ... ## $ value2: num 10 8 7 7 17 20 19 19 21 18 ... plot(my.df$value1, my.df$value2) "],
["8-4-general-steps.html", "8.4 General Steps", " 8.4 General Steps Choose appropriate attributes This is the most important steps. Choose attributes that that actions can be taken upon A sohisticated cluster analysis can’t compensate for a poor choice of variables Scale Data When NOT to scale If you have attributes with a well-defined meaning. Say, latitude and longitude, then you should not scale your data, because this will cause distortion When To Scale If you have mixed numerical data, where each attribute is something entirely different (say, shoe size and weight), has different units attached (lb, tons, m, kg …) then these values aren’t really comparable anyway; z-standardizing them is a best-practise to give equal weight to them If variables vary in range, then the variable with the largest value will have the greatest impact on result. This is undesirable Therefore data must be scaled so that they can be compared fairly Methods of Scaling Popular scaling methods are: Normalize to mean=0 and sd=1 Divide by Max Minus min, divide by Min-Max range Screen for Outliers Outliers can distort results. Screen to remove them Calculate Data Point Distances Popular measure of distance between two data point is Euclidean distance Others are Manhattan, Canberra, Asymmetric Binary, Maximum and Minkowski also available Chosoe a Clustering Alrorithm, and Inter-Custer Distance Method Try few Clustering Solutions Decide the best clustering algorithm, cluster distance method and number of cluster, K Use NbClus as a tool to guide choosing K (number of cluster) Visualize the result Visualization can help you determine the meaning and usefulness of the cluster solution Hierarchical clustering are usually presented as a dendrogram Partitioning results are typically visualized using a bivariate cluster plot Intepret the Cluster Once a cluster solution has been obtained, you must interpret (and possibly name) the clusters What do the observations in a cluster have in common? How do they differ from the observations in other clusters? This step is typically accomplished by obtaining summary statistics for each variable by cluster For continuous data, the mean or median for each variable within each cluster is calculated. For mixed data (data that contain categorical variables), the summary statistics will also include modes or category distributions Validate Result Validating the cluster solution involves asking the question: “Are these groupings in some sense real, and not a manifestation of unique aspects of this dataset or statistical technique?” If a different cluster method or different sample is employed, would the same clusters be obtained? If actual grouping data is known, run randIndex to measure the degree of agreement The fpc, clv, and clValid packages each contain functions for evaluating the stability of a clustering solution (not discussed here) "],
["8-5-distance-algorithm.html", "8.5 Distance Algorithm", " 8.5 Distance Algorithm The choice of an appropriate metric will influence the shape of the clusters, as some elements may be close to one another according to one distance and farther away according to another. For example, in a two dimensional data, the distance between the point (1,1) and the origin (0,0) can be 2 under Manhattan distance, \\(\\sqrt{2}\\) under Euclidean distance, or 1 under Maximum distance. dist is used to measure distance for all numeric elements in dataframe or matrix. Supplying non-numeric columns for dist will incur warning. dist( x, method = ) default method = 'euclidean' \\(\\quad\\) method = 'euclidean', &quot;maximum&quot;, &quot;manhattan&quot;, &quot;canberra&quot;, &quot;binary&quot; or &quot;minkowski&quot; 8.5.1 Euclidean Distance \\[Euclidean-d(p,q) = \\sqrt{\\sum_{i=1}^n (p_i-q_i)^2} \\quad,n = dimension\\] The Euclidean distance is a distance measure between two points or or vectors in a two- or multidimensional (Euclidean) space based on Pythagoras’ theorem The distance is calculated by taking the square root of the sum of the squared pair-wise distances of every dimension Below command measures distance for numeric columns of all data points in my.df, using euclidean algorithmn. data.scaled = scale(my.df[,3:4]) # Z-Score scaling d.euclidean = dist( data.scaled ) # Euclidean distance round (d.euclidean,1) ## ID_1 ID_2 ID_3 ID_4 ID_5 ID_6 ID_7 ID_8 ID_9 ID_10 ID_11 ID_12 ID_13 ## ID_2 0.6 ## ID_3 0.9 0.3 ## ID_4 0.6 0.8 1.0 ## ID_5 1.4 1.7 1.9 2.1 ## ID_6 2.0 2.3 2.5 2.6 0.6 ## ID_7 1.7 2.1 2.4 2.4 0.5 0.4 ## ID_8 1.7 2.1 2.4 2.4 0.5 0.4 0.0 ## ID_9 2.1 2.5 2.7 2.7 0.8 0.4 0.4 0.4 ## ID_10 1.5 2.0 2.2 2.1 0.4 0.6 0.2 0.2 0.6 ## ID_11 2.5 2.3 2.3 3.0 1.7 1.8 2.1 2.1 2.2 2.1 ## ID_12 3.1 3.1 3.2 3.7 1.8 1.5 1.9 1.9 1.8 2.1 1.2 ## ID_13 2.5 2.4 2.4 3.1 1.6 1.6 1.9 1.9 2.1 2.0 0.2 1.0 ## ID_14 3.0 2.9 3.0 3.6 1.9 1.7 2.1 2.1 2.1 2.2 0.8 0.5 0.6 ## ID_15 2.9 2.7 2.7 3.5 2.1 2.1 2.4 2.4 2.5 2.5 0.4 1.1 0.5 ## ID_14 ## ID_2 ## ID_3 ## ID_4 ## ID_5 ## ID_6 ## ID_7 ## ID_8 ## ID_9 ## ID_10 ## ID_11 ## ID_12 ## ID_13 ## ID_14 ## ID_15 0.6 8.5.2 Manhattan Distance \\[Manhattan - d(p,q) = \\sum_{i=1}^n |p_i-q_i| \\quad,n = dimension\\] The Manhattan distance (sometimes also called Taxicab distance) metric is related to the Euclidean distance But instead of calculating the shortest diagonal path (“beeline”) between two points, it calculates the distance based on gridlines Below command measures distance for numeric columns of all data points in my.df, using manhattan algorithm. data.scaled = scale(my.df[,3:4]) # Z-Score scaling d.manhattan = dist( data.scaled, method=&#39;manhattan&#39;) round (d.manhattan, 1) ## ID_1 ID_2 ID_3 ID_4 ID_5 ID_6 ID_7 ID_8 ID_9 ID_10 ID_11 ID_12 ID_13 ## ID_2 0.9 ## ID_3 1.2 0.4 ## ID_4 0.9 1.0 1.0 ## ID_5 1.8 1.7 2.1 2.7 ## ID_6 2.5 2.4 2.6 3.4 0.7 ## ID_7 1.9 2.4 2.8 2.8 0.7 0.6 ## ID_8 1.9 2.4 2.8 2.8 0.7 0.6 0.0 ## ID_9 2.3 2.8 3.2 3.2 1.1 0.6 0.4 0.4 ## ID_10 1.6 2.3 2.7 2.5 0.6 0.9 0.3 0.3 0.7 ## ID_11 3.3 3.3 3.3 4.2 1.9 2.4 2.6 2.6 3.0 2.5 ## ID_12 4.3 4.2 4.2 5.2 2.5 1.8 2.4 2.4 2.0 2.7 1.3 ## ID_13 3.4 3.4 3.4 4.3 1.6 2.1 2.3 2.3 2.7 2.2 0.3 1.1 ## ID_14 4.2 4.1 4.1 5.1 2.4 1.7 2.3 2.3 2.3 2.6 0.9 0.7 0.8 ## ID_15 3.9 3.8 3.8 4.8 2.1 2.6 2.8 2.8 3.2 2.7 0.6 1.5 0.5 ## ID_14 ## ID_2 ## ID_3 ## ID_4 ## ID_5 ## ID_6 ## ID_7 ## ID_8 ## ID_9 ## ID_10 ## ID_11 ## ID_12 ## ID_13 ## ID_14 ## ID_15 0.9 8.5.3 Maximum Distance \\[d(x,y)= sup|x_j - y_j|, 1≤ j ≤ d\\] 8.5.4 Canberra Distance \\[\\sum_{j=1}^{d}|x_j - y_j|) / (|x_j|+|y_j|)\\] 8.5.5 Minkowski Distance The Minkowski distance is a generalized form of the Euclidean distance (if m=2) and the Manhattan distance (if m=1). \\[\\left(\\sum_{i=1}^n |p_i-q_i|^p\\right)^{1/m}\\] "],
["8-6-optimum-number-of-clusters-k.html", "8.6 Optimum Number of Clusters (K)", " 8.6 Optimum Number of Clusters (K) There are three (3) popular methods for determining the optimal number of clusters. Elbow Method Applicable for partioning clustering, such as k-means Average Silhoutte Method Gap Statistics (not discussed here) There is no guarantee that they will agree with each other. In fact, they probably won’t. However, use this as a guidine and test few highest criteria score to determinee final number of cluster. 8.6.1 Elbow Method 8.6.1.1 Elbow Concept The objective of partitioning clustering (such as K-Mean) is to define clusters such that the total intra-cluster variation (known as total within-cluster variation or total within-cluster sum of square, wss) is minimized. 8.6.1.2 Elbow Algorithm Run K-mean clustering algorithm for K=1 to n For each K, calculate the within-cluster-sum-of-square (wss) Plot the curve of wss against the number of clusters K The location of bend (knee) in the plot is generally considered as the indicator of the appropriate number of clusters When the WSS value stop decreasing significantly (at the knee), then the number of clusters probably had reached its optimum. Although this approach is heuristic, it still provide a good guideline for K selection. 8.6.1.3 Elbow Codes (for K-mean) - Do It Yourself! The method presented here does not require any external library ! However, it requires writing a funciton to calculate WSS and plot the results. Define the The Algorithmn # Algorithmn: Compute k-means and plot wss for k=2 to k=15 wssplot = function(data, nc=15, seed=1234){ wss &lt;- (nrow(data)-1)*sum(apply(data,2,var)) for (i in 2:nc) { set.seed(seed) wss[i] &lt;- sum(kmeans(data, centers=i)$withinss) } plot(1:nc, wss, type=&quot;b&quot;, xlab=&quot;Number of Clusters (K)&quot;, ylab=&quot;Total Within Groups Sum of Squares&quot;) wss } Run The Code If number of observations &lt;=nc(default 15), specify smaller nc. wssplot(data.scaled, nc=8) ## [1] 28.000000 12.865178 2.467904 2.191659 2.038777 0.785887 1.396947 ## [8] 1.030988 abline(v=3, lty=2) # mark the optimum K after facts The wssplot above indicates that there is a distinct drop in the within-groups sum of squares when moving from 1 to 3 clusters. After three clusters, this decrease drops off, suggestign that a three-cluster solution may be a good fit to the data. 8.6.1.4 Elbow Codes - using factoextra::fviz_nbclust,hcut factoextra combined functions to calculate ‘silhoutte’ and output ggplot object For k-mean wss analysis, kmeans helper function from base-R is required For pam wss analysis, cluster:pam helper function is required For h-cluster wss analysis, hcut helper function by its own library is used. Somehow base-R hclust is not supproted library(factoextra) library(cluster) fviz_nbclust(data.scaled, kmeans, method = &quot;wss&quot;) + labs(subtitle=&#39;kmeans&#39;) fviz_nbclust(data.scaled, pam, method = &quot;wss&quot;) + labs(subtitle=&#39;pam&#39;) fviz_nbclust(data.scaled, hcut, method = &quot;wss&quot;) + labs(subtitle=&#39;hcut&#39;) + geom_vline(xintercept = 3, linetype = 2) 8.6.2 Average Silhoutte Method 8.6.2.1 Average Silhoutte Concept Average silhouette method computes the average silhouette of observations for different values of k. The optimal number of clusters k is the one that maximize the average silhouette over a range of possible values for k (Kaufman and Rousseeuw [1990]). Silhouette analysis can be used to study the separation distance between the resulting clusters. The silhouette plot displays a measure of how close each point in one cluster is to points in the neighboring clusters and thus provides a way to assess parameters like number of clusters visually. This measure has a range of [-1, 1]. Silhouette coefficients (as these values are referred to as) near +1 indicate that the sample is far away from the neighboring clusters. A value of 0 indicates that the sample is on or very close to the decision boundary between two neighboring clusters and negative values indicate that those samples might have been assigned to the wrong cluster. 8.6.2.2 Average Silhoutte Algorithm Compute clustering algorithm (e.g., k-means clustering) for different values of k For each k, calculate the average silhouette of observations (avg.sil) Plot the curve of avg.sil according to the number of clusters k The location of the maximum is considered as the appropriate number of clusters 8.6.2.3 Average Silhoutte Code - factoextra:fviz_nbclust Example code below shows silhoute analysis for kmeans, pam and h-cluster: factoextra combined functions to calculate ‘silhoutte’ and output ggplot object For k-mean silhoutte analysis, kmeans helper function from base-R is required For pam silhoutte analysis, cluster:pam helper function is required For h-cluster silhoutte analysis, hcut helper function by its own library is used. Somehow base-R hclust is not supproted library(factoextra) library(cluster) fviz_nbclust(data.scaled, kmeans, method = &quot;silhouette&quot;) + labs(subtitle=&#39;kmeans&#39;) fviz_nbclust(data.scaled, pam, method = &quot;silhouette&quot;) + labs(subtitle=&#39;pam&#39;) fviz_nbclust(data.scaled, hcut, method = &quot;silhouette&quot;) + labs(subtitle=&#39;hcut&#39;) 8.6.3 NbClust Package (with 30 Indices) NbClust package offers numerous 26 indices for determining the best number of clusters in a cluster analysis. library(&#39;NbClust&#39;) Multiple indices are computed simultaneously - a clear advantage Paramter index='all' will utilize all indices to evaluate the optimum number of clusters Nbclust returns a list that contains all evaluation statistic based on the indices used Results of the evaluation is stored in Best.nc vector Using table and barplot is best way to visualize the result of best K Supported Indices are kl, ch, hartigan, ccc, scott, marriot, trcovw, tracew, friedman, rubin, cindex, db, silhouette, duda, pseudot2, beale, ratkowsky, ball, ptbiserial, gap, frey, mcclain, gamma, gplus, tau, dunn, hubert, sdindex, dindex, sdbw ‘all’ (all indices except GAP, Gamma, Gplus and Tau) ‘alllong’ (all indices with Gap, Gamma, Gplus and Tau included) NbClust( data=, diss=NULL, distance='euclidean', min.nc=2, max.nc=15, method=NULL, index='all', alphaBeale=0.1) \\(\\quad\\) data = matrix or dataframe \\(\\quad\\) diss = dissimilarity matrix, if not NULL, then distance should be NULL \\(\\quad\\) distance = &quot;euclidean&quot;, &quot;maximum&quot;, &quot;manhattan&quot;, &quot;canberra&quot;, &quot;binary&quot;, &quot;minkowski&quot; or &quot;NULL&quot; \\(\\quad\\) min.nc = minimum number of clusters \\(\\quad\\) max.nc = maximum number of clusters \\(\\quad\\) method = &quot;ward.D&quot;, &quot;ward.D2&quot;, &quot;single&quot;, &quot;complete&quot;, &quot;average&quot;, &quot;mcquitty&quot;, &quot;median&quot;, &quot;centroid&quot;, &quot;kmeans&quot; \\(\\quad\\) index = 'all' to use all indices for evaluation NbClust output an object with below values: Best.nc : Best number of clusters proposed by each index and the corresponding index value Best.partition: vector of cluster group for every observation 8.6.3.1 Run The Code Run NbClust for average (h-clustering) and kmeans method. nbc.hclust = NbClust(data.scaled, distance=&quot;euclidean&quot;, min.nc=2, max.nc=8, method=&quot;average&quot;) nbc.kmeans = NbClust(data.scaled, min.nc=2, max.nc=8, method=&quot;kmeans&quot;) 8.6.3.2 Visualize The Result Visualize using Base-R As output Best.nc[1,] shows, majority indices favor three (3) clusters. table( nbc.hclust$Best.n[1,] ) ## ## 0 1 2 3 5 6 7 8 ## 2 1 2 14 2 1 2 2 barplot( table(nbc.hclust$Best.n[1,] ), xlab=&quot;Numer of Clusters&quot;, ylab=&quot;Number of Criteria&quot;, main=&quot;Number of Clusters Chosen by 26 Criteria\\nh-cluster&quot;) table( nbc.kmeans$Best.nc[1,] ) ## ## 0 1 2 3 5 7 8 ## 2 1 2 15 1 1 4 barplot( table(nbc.kmeans$Best.nc[1,] ), xlab=&quot;Numer of Clusters&quot;, ylab=&quot;Number of Criteria&quot;, main=&quot;Number of Clusters Chosen by 26 Criteria\\nkmeans&quot;) Visualize using factoextra::fviz_nbclust() Single function fviz_nbclust() from factoextra library will use value in NbClust object to visualize the optimal cluster number. fviz_nbclus output ggplot object, hence can be easily customized. library(&#39;factoextra&#39;) fviz_nbclust(nbc.hclust) + labs(subtitle=&#39;H-Cluster&#39;) fviz_nbclust(nbc.kmeans) + labs(subtitle=&#39;K-Means&#39;) "],
["8-7-clustering-algorithm-compared.html", "8.7 Clustering Algorithm Compared", " 8.7 Clustering Algorithm Compared Description h-cluster k-means 1 Computation Time Fast. Linear to number of observation Slow: Quadradric to number of observation 2 Initial K needed No Yes 3 Fine Tuning Experiment with different method of Linkage Experiment with different K centroids 4 Perform Well in Hierachical Nature Data Set Spherical Data Points 5 Perform Bad in Large data sets U-Shape, Outliers 6 Unique Advantages Good for hirechical discovery 7 R Library Base R, factoextra Base R "],
["8-8-hierarchical-clustering.html", "8.8 Hierarchical Clustering", " 8.8 Hierarchical Clustering Hierarchical clustering is a widely used data analysis tool The idea is to build a binary tree of the data that successively merges similar groups of points Number of clusters (K) is required as import It is an unsupervised learning 8.8.1 Clustering Algorithm This is how Hierarchical Clustering works: 1. Initially, put each data point in its own cluster 2. Calucate the distances between each cluster and all other clusters (inter-cluster distance) 3. Combine the two clusters with the smallest distance - This reduce cluster number by one 4. Repeat step (2) and (3) until all clusters have been merged into single cluster 8.8.2 Inter Cluster Distance Method Once distance for all data points has been measured, decide which of the five (5) methods below to measure distance between clusters: Single Linkage: Shortest distance among all data points betweentwo clusters Complete Linkage (common): Longest distance among all data points between two clusters Average Linkage (common): Average distance of all points between two clusters Centroid: Find the centroid of each cluster and calculate the distance between centroids between two clusters Please note that the Inter Cluster Distance Method above uses Distance Algorithmn such as ‘euclidean’, “maximum”, “manhattan”, “canberra”, “binary” or “minkowski” to calculate actual distance value. 8.8.3 Run The Code Generally, step (A), (B) and (C) are followed for H-clustering analysis. A. Before running H-Clustering Fiter Data (choose only numeric columns) Scale Data (if required) Calculate distance, using B. Performing H-Clustering Build H-Cluster model, require input of inter-cluster distance method Derive cluster by **cutting* into K number of clusters C. Visualize Display frequency, number of observations for each cluster group Plot dendrogram, superimpose cluster group into the plot 8.8.3.1 Using Base-R Utilities hclust (d, method ='complete') \\(\\quad\\) d = distance calculated using dist() \\(\\quad\\) method = 'single', 'complete', 'average', 'centroid' as cluster distance method Filter, Scale, Calculate Distance data.scaled = scale(my.df[,3:4]) # scale data d.euclidean = dist( data.scaled ) # calculate distance Build H-Cluster Model, Cutting into clusters fit.average = hclust (d.euclidean, method=&#39;average&#39;) # build the model clusters = cutree (fit.average, k=3) # derive the clusters clusters ## ID_1 ID_2 ID_3 ID_4 ID_5 ID_6 ID_7 ID_8 ID_9 ID_10 ID_11 ID_12 ## 1 1 1 1 2 2 2 2 2 2 3 3 ## ID_13 ID_14 ID_15 ## 3 3 3 Display frequency table, Visualize with dendogram, superimpose cluster group barplot( table(clusters), xlab=&#39;Cluster Group Number&#39;, ylab=&#39;No. of Observations&#39; ) plot( fit.average, main=&#39;HCluster, Average-Linkage\\n3 Clusters Solution&#39; ) rect.hclust( fit.average, k=3, border = 2:5 ) 8.8.3.2 Using factoextra Package factoextra provides single function hcut to scale, calcuate distance and cutting into cluster groups. Which is handy. library(&quot;factoextra&quot;) hcut(x, k = 2, hc_method = &quot;ward.D2&quot;, hc_metric = &quot;euclidean&quot;, stand = FALSE, graph = FALSE) \\(\\quad\\) x = matrix or dataframe \\(\\quad\\) k = number of clusters to cut \\(\\quad\\) hc_method = inter-cluster distance method: ward.D, ward.D2, single, complete, average \\(\\quad\\) hc_metric = distance calc method: euclidean, manhattan, maximum, canberra, binary, &quot;minkowski \\(\\quad\\) stand = TRUE: scale x with z-score, FALSE: not to scale x hcut output below useful values (not all displayed): \\(\\quad\\) data = original data (if stand=FALSE), scaled data (if stand=TRUE) \\(\\quad\\) nbclust = number of clusters \\(\\quad\\) cluster = cluster group number assigned for each observation \\(\\quad\\) size = frequency vector, number of observations for each cluster \\(\\quad\\) method = inter-cluster distance method applied \\(\\quad\\) dist.method = distance method applied Filter, Scale, Calculate Distance, Build H-Cluster Model, Cutting into Clusters fit.hc = hcut(my.df[,3:4], k=3, hc_method=&#39;average&#39;, hc_metric=&#39;euclidean&#39;, stand = TRUE) Display frequency table, Visualize with dendogram, superimpose cluster group barplot( table(fit.hc$cluster), xlab=&#39;Cluster Group Number&#39;, ylab=&#39;No. of Observations&#39; ) fviz_dend(fit.hc, rect = TRUE, rect_border = &#39;red&#39;, cex = 0.5, lower_rect = -0.5, horiz = T) "],
["8-9-k-mean-clustering.html", "8.9 K-Mean Clustering", " 8.9 K-Mean Clustering K-mean is the most common partitioning clustering algorithm Partitioning means data points need to be initally partioned into few clusters to start the process with The other partitioning clustering method is Medoids 8.9.1 Clustering Algorithm Define K number of centroids (data points) Cluster Assignment Each observation is assigned to the nearest centroid, using euclidean distance Update Centroids After all observations had been assigned to the centroids, a new centroids is calculated Repeat step (2) - (3) until convergence Convergence means none of the observations changed cluster membership 8.9.2 Run The Code Generally, step (A), (B) and (C) are followed for H-clustering analysis. A. Before running H-Clustering Fiter Data (choose only numeric columns) Scale Data (if required) B. Performing H-Clustering Build K-Means Cluster model, require input Number of initial centers (clusters K) K-Means Algorithmn Number of tries to seed random centers, before choosing the best model C. Visualize Display frequency, number of observations for each cluster group Plot graph, superimpose cluster group into the plot 8.9.2.1 kmeans( x, centers, nstart=1, algorithmn='Hartiga-Wong' ) \\(\\quad\\) x = matrix or dataframe \\(\\quad\\) centers = number of centroids \\(\\quad\\) nstart = how many times to randomly try seeding centroids \\(\\quad\\) algorithm = &quot;Hartigan-Wong&quot;-default, &quot;Lloyd&quot;, &quot;Forgy&quot;, &quot;MacQueen&quot; nstart=25 is a good number to use. kmeans output below useful values: cluster : cluster number for all observations centers : Centre values (of each dimensions) for each cluster withinss : Total Sum of Squares Within For Each Cluster size : Number of observations for each cluster number fit.kmeans = kmeans(data.scaled, 3, nstart = 25) fit.kmeans$cluster ## ID_1 ID_2 ID_3 ID_4 ID_5 ID_6 ID_7 ID_8 ID_9 ID_10 ID_11 ID_12 ## 3 3 3 3 1 1 1 1 1 1 2 2 ## ID_13 ID_14 ID_15 ## 2 2 2 fit.kmeans$withinss ## [1] 0.5512567 1.1226046 0.7940430 fit.kmeans$size ## [1] 6 5 4 8.9.3 Visualizing K-Mean Cluster library(factoextra) fviz_cluster( fit.kmeans, data = data.scaled, geom = &quot;point&quot;, stand = FALSE, ellipse.type = &quot;norm&quot;) "],
["8-10-agreement-with-actual-group-data.html", "8.10 Agreement With Actual Group Data", " 8.10 Agreement With Actual Group Data The adjusted Rand index provides a measure of the agreement between two sets of grouping data, adjusted for chance. It ranges from -1 (no agreement) to 1 (perfect agreement). library(&#39;flexclust&#39;) 8.10.1 Compare Cluster with Actual Data If we know the actual grouping data, we can then run this index analysis against the cluster grouping. Construct a table of cluster groups and actual groups. Then run randIndex on it to reveal the agreement measure. # cluster data generated using cutree() table(my.df$grp, clusters) ## clusters ## 1 2 3 ## G1 4 1 0 ## G2 0 5 0 ## G3 0 0 5 randIndex( table(my.df$grp, clusters) ) ## ARI ## 0.7920792 8.10.2 Compare Two Clusters We can also compare cluster data from two different clusters, eg. clusters using different algorithm. # cluster data generated using kmeans and factoextra::hcut() table(fit.kmeans$cluster, fit.hc$cluster) ## ## 1 2 3 ## 1 0 6 0 ## 2 0 0 5 ## 3 4 0 0 randIndex( table(fit.kmeans$cluster, fit.hc$cluster) ) ## ARI ## 1 It this case, both models give similar clustering result. -->"]
]
