[
["11-classification.html", "Chapter 11 Classification ", " Chapter 11 Classification "],
["11-1-introduction.html", "11.1 Introduction", " 11.1 Introduction "],
["11-2-application.html", "11.2 Application", " 11.2 Application "],
["11-3-comparing-algorithm.html", "11.3 Comparing Algorithm", " 11.3 Comparing Algorithm "],
["11-4-performance-measurement.html", "11.4 Performance Measurement", " 11.4 Performance Measurement There are many performance measurement used for binary classification. Here are the rules of thumb which one to use: Recall: If you don’t mind getting some inaccurate result, as long as you get as much correct ones Precision: If you demand rate of correctness and willing to reject some correct results F1 Score: For a more balanced measurement, taking into consideraton both recall and precision 11.4.1 Confusion Matrix Confusion Matrix and Performance Measurement 11.4.1.1 Accuracy Accuracy answers the question: From the total samples, how many had been correctly predicted by the model ? \\(Accuracy = \\frac{TP+TN}{TP+TN+FP+FN}\\) This measurement is useful when the both classes are balanced (that is, the number of TP and TN cases are almost balanced) In practice, it seems that the best accuracy is usually achieved when the cutpoint is near the Probability(actual TRUE) 11.4.1.2 Recall Recall answers the question: Out of all actual positive samples, how many were correctly predicted by classifiers ? \\(Recall = \\frac{TP}{TP+FN}\\) 11.4.1.3 Precision Precison answers the question: Out of all the samples classifier predicted as positive, what fraction were correct ? \\(Precision = \\frac{TP}{TP+FN}\\) 11.4.1.4 F1 Score F1 score is the harmonic mean of Precision and Recall. Intuitively, F1 Score is the weighted average of Precision and Recall. It takes into account all three measures: TP, FP and FN \\(Precision = 2*\\frac{Recall * Precision}{Recall + Precision}\\) F1 is usually more useful than accuracy, especially if you have an unbalanced class distribution 11.4.1.5 Area Under ROC (AUROC) ROC curve is basically a graph of TPR vs FPR (some refer to as Recall vs (1-Sensitivity), plotted for different thresholds Comparing two different models, the model with higher AUROC is considered to have higher overall Accuracy AUROC (Area Under ROC) measures : AUC of 0.5: means the model is as good as tossing a coin, worthless AUC of 1.0: means for all cutoff points, TPR=1 and FPR=0. Intuitively it means, all samples had been correctly classified into TP (TPR=0) and TN(FPR=0), and there is no FP and FN. Ultiamtely it means Accuracy is 100% AUROC and Thresholds 11.4.2 Cutoff Threshold and Performance 11.4.2.1 Impact of Cutoff Threshold to Performance Cutoff threshold direclty influence the value of TP, FP, TN, FN. If cutoff threshold is lowered (lower probability to classify as Postive), the results are: More linient and hence more samples will be classified as Positive More predicted Positives means more TP and FP, hence TPR and FPR increases However, TPR and FPR increases at different rate: If TPR increases faster than FPR -&gt; this is good, as the lowered threshold generated more TP than FP If FPR increases faster then TPR -&gt; this is not good, as the lowered threhsold generated more FP than TP Different threshold produces different performance metrics (Accuracy, Recall, Precision and Specificity and F1-score). As an example, picture below shows how threshold influences the ROC curve. Threshold and ROC The only way to estimate the optimum threshold for each of the performance measurement will be to measure them for a wide range of threshold. 11.4.2.2 Cutoff and Performance Visualization Cutoff and Performance is best visualized using the graphs below: Threshold vs Accuracy Threshold vs Recall / TPR Threshold vs Precision Threshold vs Specificity / TNR Threshold vs TPR / FPR ROC Curve (TPR vs FPR) 11.4.2.3 Cutoff Inbalance Class 11.4.2.4 Maximize F1 Score 11.4.3 Run The Code 11.4.3.1 Sample Data We shall simulate the results of binary prediction and its true value: predicted = sample(0:1, 33, replace=T) actual = sample(0:1, 33, replace=T) 11.4.3.2 The Confusion Table Building the confusion table is the first step to calculate the rest of the metrics. t = table(actual, predicted) # 2-dim frequency table t ## predicted ## actual 0 1 ## 0 13 5 ## 1 5 10 11.4.3.3 Performance Ratio Accuracy p1 = prop.table(t) # for accuracy calc addmargins(p1) ## predicted ## actual 0 1 Sum ## 0 0.3939394 0.1515152 0.5454545 ## 1 0.1515152 0.3030303 0.4545455 ## Sum 0.5454545 0.4545455 1.0000000 Recall, Miss Rate, Fallout, Specificity p2 = prop.table(t, margin = 1) # tpr, fpr, fnr, tnr calc addmargins(p2,2) ## predicted ## actual 0 1 Sum ## 0 0.7222222 0.2777778 1.0000000 ## 1 0.3333333 0.6666667 1.0000000 PPV, FDR, FOR, NPV p3 = prop.table(t, margin = 2) # ppv, fdr, for, npv addmargins(p3,2) ## predicted ## actual 0 1 Sum ## 0 0.7222222 0.3333333 1.0555556 ## 1 0.2777778 0.6666667 0.9444444 11.4.3.4 Put All Codes Together eval_binclass = function ( predicted, actual) { t = table(actual, predicted) # accuracy p1 = prop.table(t) # for accuracy calc p2 = prop.table(t, margin = 1) # tpr, fpr, fnr, tnr calc p3 = prop.table(t, margin = 2) # ppv, fdr, for, npv f1 = 2*(p3[1]*p2[1]/(p3[1]+p2[1])) result = c(accuracy=p1[1]+p1[4],tpr=p2[1],fpr=p2[2],fnr=p2[3],tnr=p2[4], ppv=p3[1], fdr=p3[2], fo_r=p3[3], npv=p3[4], f1=f1) } eval_binclass( predicted, actual ) 11.4.4 ROC Curve "],
["11-5-logistic-regression.html", "11.5 Logistic Regression", " 11.5 Logistic Regression 11.5.1 The Concept Logistic Regression is a actually a classification algorithm. It is used to predict: Binary outcome (1=Yes/Sucess, 0=No/Failure), given a set of independent variables. Multinomial outcome (more than two categories) - however, reference category for comparison must be specified, otehrwise, must run multiple regressions with different refence categories Logistic Regression as a special case of linear regression where: The outcome variable is categorical Ln of odds as dependent variable Linear regression cannot be used for classification because: Binary data does not have a normal distribution, which is a condition for many types of regressions Predicted values can go beyond 0 and 1, which violates the definition of probability Probabilities are often not linear 11.5.2 Assumptions Since logistic regression is related to linear combination of IVs, it share some common asssumptions regarding IVs and error terms: Dependent variable must be 1/0 type eg. ‘sucess/failure’, ‘male/female’, ‘yes/no’. Must not be ordinal and continous Observations must be independent Like OLS, Linearity between logit with all independent variables Like OLS, NO multicollinearity - if found, create interaction term, or drop one of the IVs Like OLS, error terms are assumed uncorrelated Although logit is a linear relation with independent variables, logistic regression (which use MLE) is different from OLS Linear Regression as below, due to the fact that DV is categorical and not continuuous: Can handle categorical independent variables Does not assume normality of DV and IVs: becauae \\(p\\) follow Bernoulli distribution Does not assume linearity between DV and IVs: because DV is categorical Does not assume homoscedasticity Does not assume normal errors 11.5.3 Equations The goal of logistic regression is to estimate \\(p\\) (the probability of ‘Success’) for a linear combination of the independent variables This is done by ‘linking’ the linear combination of independent variables to Bernoulli probability distribution (with domain from 0 to 1), to predict the probability of success The link function is called logit, which is the natural log of odds ratio. It is a linear function against independent variables: \\(logit(p) = ln(odds) = ln\\bigg(\\frac{p}{1-p}\\bigg) = \\beta_0 + \\beta_1 x_1 + ... + \\beta_n x_n\\) Derive Odd ratio by anti-log. It measures the ‘strength’ of IV in affecting the outcome, p: \\(odds = \\frac{p}{1-p} = e^{\\beta_0 + \\beta_1 x_1 + ... + \\beta_n x_n}\\) \\(p\\) can be further derived as below sigmoid function. \\(p\\) is non-linear against independent varibales : \\(p = \\frac{1}{1+e^{-\\beta_0 + \\beta_1x_1 + ... + \\beta_nx_n}}\\) 11.5.4 Sample Data 11.5.5 Run The Code glm (formula, family=gaussian(link=identity), data) \\(\\quad\\) formula : example y ~ x1 + x2 + x3 \\(\\quad\\) family : binomial, gaussian, poisson, quasi etc $\\quad$link : logit-default for binomial(), identity-default for gaussian, log-default for poisson` 11.5.5.1 Binomial Binomial Example glm (y ~ x1 + x2 + x3 , family=binomial(logit), data=my.df) -->"]
]
