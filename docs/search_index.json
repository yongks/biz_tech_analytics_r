[
["11-classification.html", "Chapter 11 Classification ", " Chapter 11 Classification "],
["11-1-introduction.html", "11.1 Introduction", " 11.1 Introduction "],
["11-2-application.html", "11.2 Application", " 11.2 Application "],
["11-3-comparing-algorithm.html", "11.3 Comparing Algorithm", " 11.3 Comparing Algorithm "],
["11-4-performance-measurement.html", "11.4 Performance Measurement", " 11.4 Performance Measurement There are many performance measurement used for binary classification. Here are the rules of thumb which one to use: Recall: If you don’t mind getting some inaccurate result, as long as you get as much correct ones Precision: If you demand rate of correctness and willing to reject some correct results F1 Score: For a more balanced measurement, taking into consideraton both recall and precision 11.4.1 Confusion Matrix Confusion Matrix and Performance Measurement 11.4.1.1 Accuracy Accuracy answers the question: From the total samples, how many had been correctly predicted by the model ? \\(Accuracy = \\frac{TP+TN}{TP+TN+FP+FN}\\) This measurement is useful when the both classes are balanced (that is, the number of TP and TN cases are almost balanced) In practice, it seems that the best accuracy is usually achieved when the cutpoint is near the Probability(actual TRUE) Accuracy is completely usessless in highly skewed class. For example, with a disease that only affects 1 in a million people a completely bogus screening test that always reports “negative” will be 99.9999% accurate 11.4.1.2 Recall Recall answers the question: Out of all actual positive samples, how many were correctly predicted by classifiers ? \\(Recall = \\frac{TP}{TP+FN}\\) 11.4.1.3 Precision Precison answers the question: Out of all the samples classifier predicted as positive, what fraction were correct ? \\(Precision = \\frac{TP}{TP+FP}\\) 11.4.1.4 F1 Score F1 score is the harmonic mean of Precision and Recall. Intuitively, F1 Score is the weighted average of Precision and Recall. It takes into account all three measures: TP, FP and FN \\(Precision = 2*\\frac{Recall * Precision}{Recall + Precision}\\) F1 is usually more useful than accuracy, especially if you have an unbalanced class distribution 11.4.1.5 Area Under ROC (AUROC) ROC curve is basically a graph of TPR vs FPR (some refer to as Recall vs (1-Sensitivity), plotted for different thresholds Comparing two different models, the model with higher AUROC is considered to have higher overall Accuracy AUROC (Area Under ROC) measures : AUC of 0.5: means the model is as good as tossing a coin, worthless AUC of 1.0: means for all cutoff points, TPR=1 and FPR=0. Intuitively it means, all samples had been correctly classified into TP (TPR=1) and TN(FPR=0), and there is no FP and FN. Ultiamtely it means Accuracy is 100% AUROC and Thresholds 11.4.2 Cutoff Threshold Optimization 11.4.2.1 Understanding Cutoff Impacts Cutoff threshold direclty influence the value of TP, FP, TN, FN. If cutoff threshold is lowered (lower probability to classify as Postive), the results are: More linient and hence more samples will be classified as Positive More predicted Positives means more TP and FP, hence TPR and FPR increases However, TPR and FPR increases at different rate: If TPR increases faster than FPR -&gt; this is good, as the lowered threshold generated more TP than FP If FPR increases faster then TPR -&gt; this is not good, as the lowered threhsold generated more FP than TP The cutoff with highest TPR/FPR value is the optimal means optimum point whereby It is possible to discover the optimum cutoff by finding the cutoff with highest TPR/FPR Different threshold produces different performance metrics (Accuracy, Recall, Precision and Specificity and F1-score). As an example, picture below shows how threshold influences the ROC curve. Threshold and ROC The only way to estimate the optimum threshold for each of the performance measurement will be to measure them for a wide range of threshold. 11.4.2.2 Cutoff Impact Visualization Selecting a cutoff threshold depends on the objectives of the researcher. To help understanding the how cutoff changes the performance metircs, try visualize them in below graph: 1.Threshold vs Accuracy 2.Threshold vs Recall (TPR) 3.Threshold vs TPR/FPR 4.Threshold vs Precision 5.Threshold vs F1 Score 6.ROC Curve (TPR vs FPR) 11.4.3 Model Evaluation 11.4.3.1 Compare With Baseline In an highly unbalanced dataset (eg. patients that is diagnosed with cancer skewed towards negative). Hence, it is essential to make a lazy baseline comparison with simply classifying every records with negative (cutoff at 1) or positive (cutoff at 0). Put in mind of the dataset used for comparison: Use data from training set as model baseline, when concerning training performance Use dta from test set as model baseline, when concerning test performance Model Accuracy is not better than ‘lazy’ baseline ! In highly bias class, in term of accuracy, the model usually unable to outperform the baseline by good marign Hence accuracy is not a good measurement in such case Model is still useful if the research objective is towards other measurement usch as recall, precision Adjust the threshold to achieve optimum better objectives 11.4.3.2 Combination of Variables Model AIC to measure the usefullness. It is like R-square in linear regression. Use it to compare model with different combinations of variables. Since logistic regression is a actual linear regression of ln(odds), multicolinearity rule apply. 11.4.4 Run The Code 11.4.4.1 Library ROCR is a great and fast library for measuring calssification performance. I library(ROCR) 11.4.4.2 Import Data and Train The Model ### Load The Data train = read.csv(&#39;./datasets/hr.train.csv&#39;) test = read.csv(&#39;./datasets/hr.test.csv&#39;) str(train.data) ## &#39;data.frame&#39;: 12000 obs. of 7 variables: ## $ S : num 0.38 0.8 0.11 0.72 0.37 0.41 0.1 0.92 0.89 0.42 ... ## $ LPE : num 0.53 0.86 0.88 0.87 0.52 0.5 0.77 0.85 1 0.53 ... ## $ NP : int 2 5 7 5 2 2 6 5 5 2 ... ## $ ANH : int 157 262 272 223 159 153 247 259 224 142 ... ## $ TIC : int 3 6 4 5 3 3 4 5 5 3 ... ## $ Newborn: int 0 0 0 0 0 0 0 0 0 0 ... ## $ left : int 1 1 1 1 1 1 1 1 1 1 ... ## Trian The Model Using Logistic Regression fit = glm(left ~ ., family = binomial, data = train) pred = predict(fit, type = &#39;response&#39;, newdata = test) 11.4.4.3 Construct the Performance Metric ROCR::predictionis the main function to crate prediction object that contains key data such as thresholds, TP, FP, TN, FP. With these data points, we can plot and calculate multiple performacne and visualization. It takes only two inputs: 1. Score (probability of being POSITIVE) 2. Label (actual POSITVE) rocr.pred = prediction(pred, test$left) rocr.metrics = data.frame( cutoff = rocr.pred@cutoffs[[1]], accuracy = (rocr.pred@tp[[1]] + rocr.pred@tn[[1]]) / (rocr.pred@tp[[1]] + rocr.pred@tn[[1]] + rocr.pred@fp[[1]] + rocr.pred@fn[[1]]), tpr = rocr.pred@tp[[1]] / (rocr.pred@tp[[1]] + rocr.pred@fn[[1]]), fpr = rocr.pred@fp[[1]] / (rocr.pred@fp[[1]] + rocr.pred@tn[[1]]), ppv = rocr.pred@tp[[1]] / (rocr.pred@tp[[1]] + rocr.pred@fp[[1]]) ) rocr.metrics$fscore = 2 * (rocr.metrics$tpr * rocr.metrics$ppv) / (rocr.metrics$tpr + rocr.metrics$ppv) rocr.metrics$tpr_fpr = rocr.metrics$tpr / rocr.metrics$fpr 11.4.4.4 List The Performance Table The code below discover the optimum threshold for few metrices. Threhold for the highest estiamtes is summarized. ## discovery the optimal threshold for various metrics rocr.best = rbind( best.accuracy = c(max = max(rocr.metrics$accuracy, na.rm = TRUE), cutoff = rocr.metrics$cutoff[which.max(rocr.metrics$accuracy)]), best.ppv = c(max = max(rocr.metrics$ppv, na.rm = TRUE), cutoff = rocr.metrics$cutoff[which.max(rocr.metrics$ppv)]), best.recall = c(max = max(rocr.metrics$tpr, na.rm = TRUE), cutoff = rocr.metrics$cutoff[which.max(rocr.metrics$tpr)]), best.fscore = c(max = max(rocr.metrics$fscore, na.rm = TRUE), cutoff = rocr.metrics$cutoff[which.max(rocr.metrics$fscore)]), best.tpr_fpr = c(max = max(rocr.metrics$tpr_fpr, na.rm = TRUE), cutoff = rocr.metrics$cutoff[which.max(rocr.metrics$tpr_fpr)]) ) kable(rocr.best) max cutoff best.accuracy 0.8400000 0.2563290 best.ppv 0.5143781 0.2563290 best.recall 1.0000000 0.0083197 best.fscore 0.6009448 0.2467452 best.tpr_fpr 5.2960770 0.2563290 11.4.4.5 Visualize The Data and Performance Plot TP, Tn, FP and FN These data are available in the ROCR::prediction object. plot (rocr.pred@cutoffs[[1]], rocr.pred@tp[[1]],xlim=c(0,1), ylim=c(0,12000), col=&#39;green&#39;) lines(rocr.pred@cutoffs[[1]], rocr.pred@tn[[1]], xlim = c(0, 1), ylim = c(0, 12000),col=&#39;red&#39;) lines(rocr.pred@cutoffs[[1]], rocr.pred@fp[[1]], xlim = c(0, 1), ylim = c(0, 12000), col=&#39;blue&#39;) lines(rocr.pred@cutoffs[[1]], rocr.pred@fn[[1]], xlim = c(0, 1), ylim = c(0, 12000),col=&#39;orange&#39;) legend(&quot;top&quot;, inset = .05, cex = 1, title = &quot;Legend&quot;, c(&quot;TP&quot;, &quot;TN&quot;, &quot;FP&quot;,&quot;FN&quot;), horiz = TRUE, lty = c(1, 1), lwd = c(2, 2), col = c(&quot;green&quot;, &quot;red&quot;, &quot;blue&quot;,&#39;orange&#39;), bg = &quot;grey96&quot;) Plot ROC Curve We can build a ROCR::performance object, and plot it ! AUC is calculated with ROCR::performance as well. rocr.perf = performance(rocr.pred, measure = &quot;tpr&quot;, x.measure = &quot;fpr&quot;) rocr.auc = as.numeric(performance(rocr.pred, &quot;auc&quot;)@y.values) plot(rocr.perf, lwd = 3, colorize = TRUE, print.cutoffs.at = seq(0, 1, by = 0.1), text.adj = c(-0.2, 1.7), main = &#39;ROC Curve&#39; ) mtext(paste(&#39;auc : &#39;, round(rocr.auc, 5))) abline(0, 1, col = &quot;red&quot;, lty = 2) Plot Accuracy rocr.perf = performance(rocr.pred, measure = &quot;acc&quot;) best.x = rocr.perf@x.values[[1]][which.max(rocr.perf@y.values[[1]])] best.y = max(rocr.perf@y.values[[1]]) plot(rocr.perf, main = &#39;Accuracy vs Cutoff&#39;, xlim = c(0, 1), ylim = c(0, 1)) abline(v = best.x, col = &#39;red&#39;, lty = 2) abline(h = best.y, col = &#39;red&#39;, lty = 2) text(best.x + 0.1, 0.05, round(best.x, 5), col = &quot;red&quot;) text(0.05, best.y + 0.05, round(best.y, 5), col = &quot;red&quot;) Plot Precesion rocr.perf = performance(rocr.pred, measure = &quot;prec&quot;) best.x = rocr.perf@x.values[[1]][which.max(rocr.perf@y.values[[1]])] best.y = max(rocr.perf@y.values[[1]][is.finite(rocr.perf@y.values[[1]])]) plot(rocr.perf, main = &#39;Precision vs Cutoff&#39;, xlim = c(0, 1), ylim = c(0, 1)) abline(v = best.x, col = &#39;red&#39;, lty = 2) abline(h = best.y, col = &#39;red&#39;, lty = 2) text(best.x + 0.1, 0.05, round(best.x, 5), col = &quot;red&quot;) text(0.05, best.y + 0.05, round(best.y, 5), col = &quot;red&quot;) Plot Multiple Metrices Into One Graph As we can see, ROCR::performance is good to build measure and build plot for single y-axis measurement. It unfortunately does not support polotting multiple y-axis into one graph. Threfore, we shall build the plot using metrices table constructed earlier (variable rocr.metrics). ### set initial margin of the plot par(mar = c(5, 5, 4, 6)) ## plot graph on left hand side scale plot(rocr.metrics$cutoff, rocr.metrics$tpr, axes = FALSE,, ylab = &#39;&#39;, xlab = &#39;&#39;, col = &#39;green&#39;, main = &#39;Multiple Performances vs Cutoff&#39;, ylim = c(0, 1)) lines(rocr.metrics$cutoff, rocr.metrics$fpr, col = &#39;red&#39;, ylim = c(0, 1)) axis(1, pretty(range(rocr.metrics$cutoff[is.finite(rocr.metrics$cutoff)]), 10)) axis(2, ylim = c(0, 1), col = &quot;black&quot;, las = 1) ## las=1 makes horizontal labels lines(rocr.metrics$fpr, rocr.metrics$tpr, lwd = 3, col = &#39;black&#39;, ylim = c(0, 1), xlim = c(0, 1)) lines(rocr.metrics$cutoff, rocr.metrics$accuracy, col = &#39;purple&#39;, ylim = c(0, 1), xlim = c(0, 1)) lines(rocr.metrics$cutoff, rocr.metrics$ppv, col = &#39;cyan&#39;, ylim = c(0, 1), xlim = c(0, 1)) lines(rocr.metrics$cutoff, rocr.metrics$fscore, col = &#39;pink&#39;, ylim = c(0, 1), xlim = c(0, 1)) abline(0, 1, lty = 2, col = &#39;black&#39;) ## plot graph on right hand side scale par(new = TRUE) plot(rocr.metrics$cutoff, rocr.metrics$tpr_fpr, axes = FALSE, ylab = &#39;&#39;, xlab = &#39;&#39;, col = &#39;orange&#39;) axis(4, col = &quot;orange&quot;, las = 1) ## las=1 makes horizontal labels best = rocr.metrics$cutoff[which.max(rocr.metrics$tpr_fpr[is.finite(rocr.metrics$tpr_fpr)])] abline(v = best, col = &#39;red&#39;, lty = 2) text(best + 0.075, 0.05, round(best, 5), col = &quot;red&quot;) ## axis labels mtext(&quot;cutoff&quot;, side = 1, col = &quot;black&quot;, line = 2.5) mtext(&quot;tpr, fpr, accuracy, precision, f1-scaore&quot;, side = 2, col = &quot;black&quot;, line = 2.5) mtext(&quot;tpr/fpr&quot;, side = 4, col = &quot;black&quot;, line = 2.5) ## legend legend(&quot;topright&quot;, inset = .05, cex = 1, title = &quot;Legend&quot;, c(&quot;TPR&quot;, &quot;FPR&quot;, &quot;TPR/FPR&quot;, &quot;ROC&quot;, &quot;Accuracy&quot;, &quot;Precision&quot;, &quot;F1-Score&quot;), horiz = FALSE, lty = c(1, 1), lwd = c(2, 2), col = c(&quot;green&quot;, &quot;red&quot;, &quot;orange&quot;, &#39;black&#39;, &#39;purple&#39;, &#39;cyan&#39;, &#39;pink&#39;), bg = &quot;grey96&quot;) "],
["11-5-logistic-regression.html", "11.5 Logistic Regression", " 11.5 Logistic Regression 11.5.1 The Concept Logistic Regression is a actually a classification algorithm. It is used to predict: Binary outcome (1=Yes/Sucess, 0=No/Failure), given a set of independent variables. Multinomial outcome (more than two categories) - however, reference category for comparison must be specified, otehrwise, must run multiple regressions with different refence categories Logistic Regression as a special case of linear regression where: The outcome variable is categorical Ln of odds as dependent variable Linear regression cannot be used for classification because: Binary data does not have a normal distribution, which is a condition for many types of regressions Predicted values can go beyond 0 and 1, which violates the definition of probability Probabilities are often not linear 11.5.2 Assumptions Since logistic regression is related to linear combination of IVs, it share some common asssumptions regarding IVs and error terms: Dependent variable must be 1/0 type eg. ‘sucess/failure’, ‘male/female’, ‘yes/no’. Must not be ordinal and continous Observations must be independent Like OLS, Linearity between logit with all independent variables Like OLS, NO multicollinearity - if found, create interaction term, or drop one of the IVs Like OLS, error terms are assumed uncorrelated Although logit is a linear relation with independent variables, logistic regression (which use MLE) is different from OLS Linear Regression as below, due to the fact that DV is categorical and not continuuous: Can handle categorical independent variables Does not assume normality of DV and IVs: becauae \\(p\\) follow Bernoulli distribution Does not assume linearity between DV and IVs: because DV is categorical Does not assume homoscedasticity Does not assume normal errors 11.5.3 Equations The goal of logistic regression is to estimate \\(p\\) (the probability of ‘Success’) for a linear combination of the independent variables This is done by ‘linking’ the linear combination of independent variables to Bernoulli probability distribution (with domain from 0 to 1), to predict the probability of success The link function is called logit, which is the natural log of odds ratio. It is a linear function against independent variables: \\(logit(p) = ln(odds) = ln\\bigg(\\frac{p}{1-p}\\bigg) = \\beta_0 + \\beta_1 x_1 + ... + \\beta_n x_n\\) Derive Odd ratio by anti-log. It measures the ‘strength’ of IV in affecting the outcome, p: \\(odds = \\frac{p}{1-p} = e^{\\beta_0 + \\beta_1 x_1 + ... + \\beta_n x_n}\\) \\(p\\) can be further derived as below sigmoid function. \\(p\\) is non-linear against independent varibales : \\(p = \\frac{1}{1+e^{-\\beta_0 + \\beta_1x_1 + ... + \\beta_nx_n}}\\) The logistic graph below shows P(Y=1) vs \\(\\beta_0 + \\beta_1 x_1 + ... + \\beta_n x_n\\). **Intuitively, larger coefficients and independence varibles values increases the chance of being classified as Y=1*8 Compare the graph below to understand why logistic regression is better than linear regression in binary classification \\(\\quad\\) 11.5.3.1 High School Log Formula Some of the high scholl maths are useful for calculating the logistic scores. Remember them by heart. What is ls log \\(y = 10^a\\) \\(log(y) = a\\) \\(y = e^a\\) \\(ln(y) = a\\) Ln is log base e \\(ln(a) = log_e(a)\\) Basic properties of log \\(ln(1) = log(1) = 0\\) \\(ln(0) = log(0) = Inf\\) Log of division becomes substration of log \\(ln\\bigg(\\frac{a}{b}\\bigg) = ln(a) - ln(b)\\) Exponential of summation becomes multiplication of exponential \\(e^{a+b} = e^a * e^b\\) 11.5.4 Sample Data 11.5.5 Run The Code glm (formula, family=gaussian(link=identity), data) \\(\\quad\\) formula : example y ~ x1 + x2 + x3 \\(\\quad\\) family : binomial, gaussian, poisson, quasi etc $\\quad$link : logit-default for binomial(), identity-default for gaussian, log-default for poisson` 11.5.5.1 Binomial Binomial Example glm (y ~ x1 + x2 + x3 , family=binomial(logit), data=my.df) -->"]
]
